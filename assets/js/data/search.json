[ { "title": "Exploring Azure Container Apps (ACA) and the Missing YAML Functionality", "url": "/ACA-missing-yaml/", "categories": "", "tags": "FinOps, DevOps, Azure", "date": "2024-04-09 08:30:12 +0000", "snippet": "IntroductionHave you ventured into the world of Azure Container Apps (ACA)? If not, let‚Äôs dive in! I recently spent some time exploring ACA, and while it offers several benefits, there‚Äôs one crucial feature that‚Äôs conspicuously absent: the ability to use YAML configurations from Kubernetes (K8s). In this post, I‚Äôll discuss ACA, its advantages, and why the inclusion of YAML support would be a game-changer.What Is Azure Container Apps (ACA)?Azure Container Apps simplifies container orchestration by managing the details for you. Whether you‚Äôre a seasoned developer or just starting out, ACA allows you to focus on your application logic without worrying about the underlying infrastructure. Here are some key points: Container Agnostic: ACA supports any Linux-based x86-64 container image, regardless of the runtime, programming language, or development stack you choose. No Required Base Image: Unlike some other platforms, ACA doesn‚Äôt impose restrictions on base images. You have the flexibility to use the container image that best suits your needs.The Missing Piece: YAML from K8sWhile ACA streamlines container management, the absence of YAML support from K8s is a notable limitation. Let‚Äôs explore why this matters: Configuration Consistency: YAML files are the lingua franca of Kubernetes. By allowing ACA to consume YAML configurations, we‚Äôd achieve consistency across our deployments. Developers familiar with K8s could seamlessly transition to ACA without rewriting their configurations. Infrastructure as Code (IaC): YAML files serve as declarative IaC. They define the desired state of our applications and infrastructure. Integrating ACA with existing YAML files would enhance our IaC practices. Easier Migration: Many organizations already use Kubernetes. Enabling ACA to read K8s YAML would simplify migration. We could gradually move workloads to ACA while leveraging existing configurations.The Low-Hanging Fruit: Adding YAML SupportHere‚Äôs the exciting part: Implementing YAML support in ACA would be a relatively straightforward enhancement. While not every K8s YAML line would be directly compatible, we could adapt and remove any incompatible sections. The benefits would far outweigh the effort: Cost Savings: ACA‚Äôs pay-as-you-go pricing model makes it an economical choice. By leveraging existing YAML configurations, we‚Äôd save both time and money. Testing Agility: With YAML support, testing ACA for current solutions becomes a breeze. We can validate our configurations, ensure compatibility, and confidently deploy applications.ConclusionAzure Container Apps holds immense promise, but let‚Äôs not overlook the power of YAML. By bridging the gap between ACA and K8s configurations, we unlock new possibilities. So, Microsoft, consider this our friendly nudge: Add YAML support to ACA, and watch it flourish! üåüHappy containerizing! üöÄüê≥Cover from: ACA vs AKS: Which Azure Service Is Better for Running Containers?" }, { "title": "Quo Vadis, Open-source?", "url": "/quo-vadis-open-source/", "categories": "", "tags": "open source", "date": "2024-04-09 08:30:12 +0000", "snippet": "Reflecting on the past year, I‚Äôve become increasingly convinced that the open-source model faces challenges from various angles. Here are my favorite moments from last months:üëâ Moq - the dotnet open source library for mockingThe owner added a SponsorLink that was sending a hash of developer email to a cloud service. Why? Because the owner wanted to test how they could get funds for their work. While some might consider this approach unconventional, it‚Äôs clear that the owner‚Äôs motivation was to earn money from their work. We can argue about the method, but the goal is clear: we don‚Äôt want to work for free.üëâ TerraformThere was a huge controversy around Terraform when they changed their license. Although some view this as a broken promise to the Terraform community, the reaction was to create a fork and ‚Äústart over.‚Äù Whether this is good or bad depends on individual perspectives, but it‚Äôs likely that companies will now think twice before investing in such open-source projects. As a result, the trust in open source is decreasing.üëâ RedisRedis also underwent a licensing change. While opinions on whether this change is good or bad may vary, the first sentence in many news articles is quite (un)expected. For instance, TechCrunch reported: ‚ÄúThe Linux Foundation last week announced that it will host Valkey, a fork of the Redis in-memory data store. Valkey is backed by Amazon Web Services, Google Cloud, Oracle, Ericsson, and Snap.‚Äù Essentially, some major companies decided to create a fork and work on it, ensuring they continue to profit from Redis without losing their investment. Did you expect that or not?üëâ XZ backdoorThis incident in my opinion is a masterpiece in providing a backdoor within open source. Over three years of effort within the open-source community led to the insertion of a backdoor code into a library used by the Open SSH server. Kudos to Microsoft employee Andres Freund, who detected this change. Interestingly, it wasn‚Äôt during a code review; he stumbled upon it while investigating performance regression in Debian/Postgres. It makes you wonder how many such backdoors exist in open source, still undetected.To trust or not to trust?At the end of the day, do you still believe that the current model is working? And more importantly, do you have any ideas on how to address this on a global scale? Ensuring that creators receive fair compensation for their work while maintaining user trust in the software they produce is crucial. While trust remains intact for now, a few missteps could irreparably damage it. Once that trust is broken, there may be no way to fully restore it." }, { "title": "6 steps to pimp my terminal", "url": "/geting-secrets-from-github-actions/", "categories": "", "tags": "GitHub, GitHub Actions, DevOps", "date": "2023-01-18 08:30:12 +0000", "snippet": "How to get secrets from GitHub ActionsWorking with secrets can be hard. Especially when you forget their values, and you would like to retrieve them. In my case I didn‚Äôt store Azure service principal for test environment anywhere. Moreover I wanted to move them from project level into organization level. Unfortunately reading them from GitHub is not possible, but the actions have them. What have left? Of course some hacking.Of course running:steps: - run: | echo '$'won‚Äôt work because GitHub is smart enough to replace any secret with ***What alternatives we have? I see three:Uploading somewhereWe can upload it to external endpoint - too complicated, but for sure it will work. I think about it but I didn‚Äôt try, because other methods worked üòÖUse base64That‚Äôs the most typical trick. Instead of echo a secret, we can try to echo bas64 of this secret. The job is quite obvious but let‚Äôs write it down: steps: - run: | echo '$' | base64To be fair the result was quite strange. I received partially good file, but with omitted clientSecret, which I was looking for. So the output was:ICJj***CiAg***CiAg***CiAgImFjdGl2ZURpcmVjdG9yeUVuZHBvaW50VXJsIjogImh0dHBzOi8vbG9naW4ubWljcm9zb2Z0b25saW5lLmNvbSIsCiAgInJlc291cmNlTWFuYWdlckVuZHBvaW50VXJsIjogImh0dHBzOi8vbWFuYWdlbWVudC5henVyZS5jb20vIiwKICAiYWN0aXZlRGlyZWN0b3J5R3JhcGhSZXNvdXJjZUlkIjogImh0dHBzOi8vZ3JhcGgud2luZG93cy5uZXQvIiwKICAic3FsTWFuYWdlbWVudEVuZHBvaW50VXJsIjogImh0dHBzOi8vbWFuYWdlbWVudC5jb3JlLndpbmRvd3MubmV0Ojg0NDMvIiwKICAiZ2FsbGVyeUVuZHBvaW50VXJsIjogImh0dHBzOi8vZ2FsbGVyeS5henVyZS5jb20vIiwKICAibWFuYWdlbWVudEVuZHBvaW50VXJsIjogImh0dHBzOi8vbWFuYWdlbWVudC5jb3JlLndpbmRvd3MubmV0LyIKfQo=Which decoded to:{ \"c \"activeDirectoryEndpointUrl\": \"https://login.microsoftonline.com\", \"resourceManagerEndpointUrl\": \"https://management.azure.com/\", \"activeDirectoryGraphResourceId\": \"https://graph.windows.net/\", \"sqlManagementEndpointUrl\": \"https://management.core.windows.net:8443/\", \"galleryEndpointUrl\": \"https://gallery.azure.com/\", \"managementEndpointUrl\": \"https://management.core.windows.net/\"}Good work GitHub! Anyway let‚Äôs try something else.Use artifactsLast try! We can write it to the file and upload it as an artifact. It shocked me, but it works like a charm steps: - run: | echo '$' &gt; file.txt - uses: actions/upload-artifact@v3 with: name: file.txt path: file.txtThe uploaded file had all the data.Only remember to delete the workflow run after test üòÄThat‚Äôs all folksp.s Cover image by Mika Baumeister on Unsplash" }, { "title": "6 steps to pimp my terminal", "url": "/few-steps-to-pimp-my-terminal/", "categories": "", "tags": "windows, terminal, powershell, pwsh", "date": "2021-06-02 09:15:12 +0000", "snippet": "ToolsMy current setup is mixed of tools from following list: oh-my-posh + powerlevel10k_rainbow (with mods) theme + custom font posh-git Set-PSReadlineKeyHandler for ‚Äúbash style‚Äù search + autocomplete PSKubectlCompletion with PSKubeContext for kubernetes Register-ArgumentCompleter for dotnet MagicTooltips which is responsible for displaying tooltips for kubectl and azIf you just want to copy-paste, scroll down to the end and copy what you need from my $PROFILE file üòÖStep 1 - standard + small hacksThe posh-git and oh-my-posh modules are quite popular, but will intruce them shortly: posh-git displays info about git status oh-my-posh allows to displays a lot of segments (like: time, battery, last exit code, kubernetes, aws, az, ‚Ä¶.)There are numerous themes for oh-my-posh. I‚Äôm using powerlevel10k rainbow, but there are missing icons even on the oh-my-posh website. You‚Äôll need a unique typeface to get this appearance. Oh-my-posh recommends Meslo LGM NF, but I went with Delugia Nerd Font Complete. Is is a Cascadia Code ‚ÄúExtended‚Äù, which includes Nerd Fonts and powerline glyphs.You must ‚Äúforce it‚Äù in Windows Terminal JSON after installation (to access settings, press CTRL+, then the gear icon on the left): \"profiles\": { \"defaults\": { \"fontFace\": \"Delugia Nerd Font Complete\" },The final outcome is awesome:Finally, remember to include your $PROFILE file (the best approach is to modify it using code $PROFILE if you have VS Code installed):# posh gitImport-Module-With-Measure posh-git# oh-my-poshImport-Module-With-Measure oh-my-poshSet-PoshPrompt -Theme powerlevel10k_rainbowStep 2: bash style history search and autocompleteDo you get tired of typing CTRL+R to look up commands in the history? Is it easier to just use arrows to find the last invocation of kubectl get, for example? I was one of them. Simply add the following to your profile:# up&amp;down arrow for history searchSet-PSReadLineKeyHandler -Key UpArrow -Function HistorySearchBackwardSet-PSReadLineKeyHandler -Key DownArrow -Function HistorySearchForwardAdd one more if you‚Äôd like to view all autocomplete options after pressing TAB:# menu complete using TAB instead of CTRL+SPACESet-PSReadlineKeyHandler -Chord Tab -Function MenuCompleteStill not convinced? It‚Äôs time for a little demonstration:Yummy üòãStep 3: AutocompleteToday I‚Äôm only using dotnet and kubernetes autocomplete. I hope that soon I could use also az.Kubernetes config I described in my last post,Anyway, the dotnet command is easy:# dotnet completionRegister-ArgumentCompleter -Native -CommandName dotnet -ScriptBlock { param($commandName, $wordToComplete, $cursorPosition) dotnet complete --position $cursorPosition \"$wordToComplete\" | ForEach-Object { [System.Management.Automation.CompletionResult]::new($_, $_, 'ParameterValue', $_) }}What about Az CLI? The proof of concept is easy, but it isn‚Äôt a top priority for Azure team üò•, just check GitHub.The gist is the only thing I‚Äôve found, but I‚Äôm not using it.If you need more stuff for autocomplete, just search, implement or rewrite bash to PowerShell. No shorthands, sorry üò•Step 4: Magic tooltipsThat‚Äôs great for displaying dynamic segments. What does this imply? When I type kubectl&lt;SPACE&gt;, for example, I want to see information about Kubernetes context, but I don‚Äôt need it most of the time. The same goes for az¬†and aws.The oh-my-posh author guides me to a MagicTooltips module. I‚Äôll probably have to submit a PR there to include namespace as well.Anyway, I choose to set it up as follows:$global:MagicTooltipsSettings = @{ VerticalOffset = 0 HorizontalOffset = 4 HorizontalAlignment = \"Right\" Providers= @{ Kubernetes = @{ Commands = \"kubectl,helm,kubens,kubectx,k9s,k\" } }}Import-Module MagicTooltipsHowever, to make your theme seem good, you‚Äôll probably need to set HorizontalOffset and VerticalOffset.My powerlevel 10k setup looks like this (the cluster name is under the clock):Step 5: find slow modules/segmentsI noticed performance issues after installing various extensions. To repair it, I chose to do the following: remove unused segments in powerlevel theme measure Import-ModuleStep 6: having ‚Äúsudo‚Äù modeIsn‚Äôt it awesome to use sudo in a terminal?I used two functions to implement it:function fcuk { $cmd = (Get-History ((Get-History).Count))[0].CommandLine Write-Host \"Running $cmd in $PWD\" start-process pwsh -verb runas -WorkingDirectory $PWD -ArgumentList \"-NoExit -Command pushd $PWD; Write-host 'cmd to run: $cmd'; $cmd\"}function sudo { if ($first -eq '!!') { fcuk; } else { $file=$args[0]; [string]$arguments = \"\"; if ($args.Count -gt 1) { $c = $args.Count - 1; [string]$arguments = $args[1..$c] } Write-Host \"file = $file args = $arguments\"; start-process $file -verb runas -WorkingDirectory $PWD -ArgumentList $arguments; }}My $PROFILE fileFinally, here‚Äôs what my $PROFILE file looks like:# helperfunction Import-Module-With-Measure { param ($ModuleName) $import = Measure-Command { Import-Module $ModuleName } Write-Host \"$ModuleName import $($import.TotalMilliseconds) ms\"}# posh gitImport-Module-With-Measure posh-git# oh-my-poshImport-Module-With-Measure oh-my-poshSet-PoshPrompt -Theme powerlevel10k_rainbow# menu complete using TAB instead of CTRL+SPACESet-PSReadlineKeyHandler -Chord Tab -Function MenuComplete# up&amp;down arrow for history searchSet-PSReadLineKeyHandler -Key UpArrow -Function HistorySearchBackwardSet-PSReadLineKeyHandler -Key DownArrow -Function HistorySearchForward# kubectl + stuff Import-Module-With-Measure PSKubectlCompletionSet-Alias k -Value kubectlRegister-KubectlCompletionImport-Module-With-Measure PSKubeContextSet-Alias kubens -Value Select-KubeNamespaceSet-Alias kubectx -Value Select-KubeContextRegister-PSKubeContextComplete# magic tooltip config$global:MagicTooltipsSettings = @{ VerticalOffset = 0 HorizontalOffset = 4 HorizontalAlignment = \"Right\" Providers= @{ Kubernetes = @{ Commands = \"kubectl,helm,kubens,kubectx,k9s,k\" } }}Import-Module MagicTooltips# dotnet completionRegister-ArgumentCompleter -Native -CommandName dotnet -ScriptBlock { param($commandName, $wordToComplete, $cursorPosition) dotnet complete --position $cursorPosition \"$wordToComplete\" | ForEach-Object { [System.Management.Automation.CompletionResult]::new($_, $_, 'ParameterValue', $_) }}# SUDO functionsfunction fcuk { $cmd = (Get-History ((Get-History).Count))[0].CommandLine Write-Host \"Running $cmd in $PWD\" start-process pwsh -verb runas -WorkingDirectory $PWD -ArgumentList \"-NoExit -Command pushd $PWD; Write-host 'cmd to run: $cmd'; $cmd\"}function sudo { if ($first -eq '!!') { fcuk; } else { $file=$args[0]; [string]$arguments = \"\"; if ($args.Count -gt 1) { $c = $args.Count - 1; [string]$arguments = $args[1..$c] } Write-Host \"file = $file args = $arguments\"; start-process $file -verb runas -WorkingDirectory $PWD -ArgumentList $arguments; }}What comes next?You can also find some interesting configurations at: Terminal-Icons module - check screenshot Pimp my terminal by Dominik Jeske - check shortcutsMaybe you have something useful in your setup? If yes please share!" }, { "title": "Make PowerShell with k8s great again", "url": "/pskube-context/", "categories": "", "tags": "powershell, k8s", "date": "2021-05-31 13:38:51 +0000", "snippet": "So, like me, you‚Äôre using Windows. WSL isn‚Äôt always reliable (e.g. VPN problems), yet you‚Äôre working with Kubernetes and require autocomplete tools. I‚Äôm the same way :)Kubectl autocompleteWith bash or zsh, having autocomplete is straightforward. Almost only one command is required to complete the task:source &lt;(kubectl completion bash)However, this will obviously not work in PowerShellüòÖ. Especially when you look what and how many lines the kubectl completion bash command produces and how many lines it generates:kubectl completion bash | wc -l14143 is the correct answer! To implement autocomplete, more than 14k lines of code were written!How to use it on Windows? Is it possible to move it? Yes, and thankfully, someone else did the legwork for you: https://github.com/mziyabo/PSKubectlCompletion/blob/master/PSKubectlCompletion.psm1To use autocomplete we will need the PSKubectlCompletion module. Install it with Install-Module PSKubectlCompletion and include with:Import-Module PSKubectlCompletionSet-Alias k -Value kubectlRegister-KubectlCompletionAt this point the basics are working quite well. But how about other tools? Like ‚Ä¶kubectx and kubensIf you never use the original ones these are two simple commands to switch your current Kubernetes context (kubectx) and namespace (kubens). In PowerShell world they should be named rather like Select-KubeContext and Select-KubeNamespace, but I will you aliases instead üòúAnd for above someone else did the work for you once more. It‚Äôs me this time: https://github.com/ptrstpp950/PSKubeContextHow it works? Just let me present a small YouTube:Great, isn‚Äôt it? I‚Äôm so glad it works that I made and published a module called PSKubeContextTo utilize it, install it with Install-Module PSKubeContext, and simply add the following to your profile:Import-Module PSKubeContextSet-Alias kubens -Value Select-KubeNamespaceSet-Alias kubectx -Value Select-KubeContextRegister-PSKubeContextCompleteWhat‚Äôs inside: Select-KubeContext or if you prefer alias kubectx Select-KubeNamespace or kubens 100% autocomplete using üôàüôâüôä A menu in command line to select the context/namespace if something goes wrong üòÖAnd that‚Äôs it. Your PowerShell is great again and works well with Kubernetes CLI." }, { "title": "Improve the performance of git on WSL2", "url": "/git-wsl2/", "categories": "", "tags": "windows, wsl2, git", "date": "2021-05-19 06:10:00 +0000", "snippet": "I‚Äôve returned to Windows after a year on the Mac. I built up pwsh and added a lot of tools, but I still need some Linux/MacOS ones, such as envsubst. Of course, WSL2 comes to the rescue, but you‚Äôll have to checkout the repo twice for Linux and Visual Studio. It is ridiculous.I set up oh-my-posh with powerline on WSL2, which is a fantastic combination. However, there is a ‚Äúbut‚Äù: git status is quite slow. What is the reason for this? Just read 4197How can this be resolved? Simply replace the git command with the git function, as shown below:function git() { if [[ $(pwd -P) = /mnt/* ]]; then git.exe \"$@\" else command git \"$@\" fi}And that‚Äôs all üôàüôâüôä" }, { "title": "Obsidian backup with OneDrive", "url": "/obsidian-backup-with-onedrive/", "categories": "", "tags": "tools", "date": "2021-05-05 17:20:51 +0000", "snippet": "I decided to try Obsidian as my new work notes tools. Looks really good, but I note there internal company stuff, so store it in the cloud is not an option.Fortunately my company uses OneDrive, so making a backup is piece of cake:# launch CMD bellow command doesn't work in pwshmklink /j \"[path to root of OneDrive folder]\\obsidian-notes\" \"[Path to obsidian notes folder]So easy and so useful üòÄ" }, { "title": "Reduce OMS logs for AKS", "url": "/reduce-oms-logs-for-aks/", "categories": "", "tags": "azure, aks", "date": "2020-05-08 06:57:45 +0000", "snippet": "What‚Äôs going on?Every week my team is trying to check our bill for Azure. This week OMS won the battle. We have a small AKS cluster, and logs are one of the most expensive part of it.Usage| where TimeGenerated &gt; startofday(ago(7d))| where IsBillable == true| summarize TotalVolumeGB = sum(Quantity) / 1000 by bin(TimeGenerated, 1d), Solution| render barchartThe result is following:What does it mean? In our case we have: a lot of Log Managment a lot of Container InsightsLet‚Äôs step in one more level so the query is following.Usage| where TimeGenerated &gt; startofday(ago(7d))| where IsBillable == true| summarize TotalVolumeGB = sum(Quantity) / 1000 by bin(TimeGenerated, 1d), DataType, Solution| render barchartAnd the result looks like below.Who is guilty? Container Log Container Inventory PerfContainer logThis category is easy. It is logs from stdout and stderr. If you want to find which namespaces are generating too much logs you can use:let startTime = ago(1h);let containerLogs = ContainerLog| where TimeGenerated &gt; startTime| where _IsBillable == true| summarize BillableDataMBytes = sum(_BilledSize)/ (1000. * 1000.) by LogEntrySource, ContainerID;let kpi = KubePodInventory| where TimeGenerated &gt; startTime| distinct ContainerID, Namespace, ControllerName;containerLogs| join kpi on $left.ContainerID == $right.ContainerID| extend sourceNamespace = strcat(LogEntrySource,\"/\", Namespace)| summarize MB=sum(BillableDataMBytes) by sourceNamespace| render piechartAnd for pods (bewear for used regex):let startTime = ago(1h);let containerLogs = ContainerLog| where TimeGenerated &gt; startTime| where _IsBillable == true| summarize BillableDataMBytes = sum(_BilledSize)/ (1000. * 1000.) by LogEntrySource, ContainerID;let kpi = KubePodInventory| where TimeGenerated &gt; startTime| distinct ContainerID, Namespace, ControllerName;containerLogs| join kpi on $left.ContainerID == $right.ContainerID| extend sourceNamespace = strcat(LogEntrySource,\"/\", Namespace, \"/\", extract(\"(.*)-.*\", 1,ControllerName))| summarize MB=sum(BillableDataMBytes) by sourceNamespace| render piechartIn my case the biggest problem was with Ingress, but to be fair I don‚Äôt need this logs at all. Why? Because my apps already flush everything to application insights.Before fixing, let‚Äôs see what else is in the logs!Container InventoryThe second part was Container Inventory. As you look through it, every minute every container logs state + environment variables. What are the size of it last week? Let‚Äôs check it out!ContainerInventory| where TimeGenerated &gt; ago(7h)| summarize size = sum(string_size(EnvironmentVar)) / (1000. * 1000.)In my case it was 2GB in last week. Yes 2GB of env variables. By the way, the secretes are there too!Perf - last but not leastFirst the query to checkout what in it.Perf| where TimeGenerated &gt; ago(10m)| summarize count(), BilledSizeKb=bin(sum(_BilledSize)/1024.0, 1) by Computer, bin(TimeGenerated, 1m)| order by TimeGenerated ascEvery instance logs a lot and if we drill in there are a lot of important things in it. Maybe somehow I can reduce it by half?Check OMS AgentLet‚Äôs see what‚Äôs going on in OMS agent. The easiest way is to get YAML with autocomplete:kubectl get po -n kube-system omsagent-rs-&lt;TAB&gt;&lt;TAB&gt; -o yamlSo we can find out that it optionaly loads config map: - configMap: defaultMode: 420 name: container-azm-ms-agentconfig optional: trueQuick search and probably you will find the official docs and the default fileAs you can see we can remove stdout and stderr easy, the same with env variables. Just take a look. I removed comments to make it clear. [log_collection_settings] [log_collection_settings.stdout] enabled = false exclude_namespaces = [\"kube-system\"] [log_collection_settings.stderr] enabled = false exclude_namespaces = [\"kube-system\"] [log_collection_settings.env_var] enabled = falseAfter few hours we can verify results. The first query for Container Inventory is bellow.ContainerInventory| where TimeGenerated &gt; ago(24h)| summarize size = sum(string_size(EnvironmentVar)) / (1000. * 1000.) by bin(TimeGenerated, 1h)| order by TimeGenerated asc Results are just awesome. Before the size was around 22.0, now it is 0.17, so it is 100th smaller.The same is with prometheus data. We can just change interval with following settings. [prometheus_data_collection_settings.cluster] interval = \"1m\"And [prometheus_data_collection_settings.node] interval = \"1m\"I‚Äôm not using OMS scraping for Prometheus, so it won‚Äôt help in my case at all.Perf - tableFor perf table we have to wait. You can find following code:# Kubernetes perf&lt;source&gt;type kubeperftag oms.api.KubePerfrun_interval 60slog_level debug&lt;/source&gt;But this doesn‚Äôt work. And we have to wait until AKS team gives us possibility to tune settings.Sum upI saved a lot of data, just checkout following diagram:" }, { "title": "Azure DevOps CLI to the rescue", "url": "/azure-devops-cli-to-the-rescue/", "categories": "", "tags": "azure-devops", "date": "2020-04-22 14:10:53 +0000", "snippet": "I need to migrate a lot of repositories from one Azure DevOps to another. Migration! Don‚Äôt ask why, please. Let‚Äôs assume it was much more simple than other options.I was thinking that I will have to do it manually, but I found Azure DevOps CLI. Hurray!Setupaz devops and az repos are inside az do the easiest way is to log in using az login and forget about other options ;)Then we can set up default organization and project using:az devops configure --defaults organization=https://dev.azure.com/MyOrg project=MyProjectFind all repositoriesThe first query was quite easy. I need to find all repositories in the old organization: az repos list --organization https://dev.azure.com/MyOldOrg --project OldProject -o tsv --query \"[].name\"CopyFrom the above list choose all needed projects and declare them as an array in bash:export repoArray=( repo1 repo2 repo3)Now we can copy using create and import.But before you start coping generate PAT token for GIT for your old Azure DevOps. Just go to https://dev.azure.com/MyOldOrg/_usersSettings/tokensfor REPO_NAME in \"${repoArray[@]}\"do az repos create --name $REPO_NAME --project \"NewProject\" az repos import create \\ --git-source-url \"https://dev.azure.com/MyOldOrg/_git/$REPO_NAME\" \\ --repository $REPO_NAME \\ --project \"NewProject\" \\ --requires-authorization \\ --user-name piotr.stappdoneCreate build pipelineNow the funny part. If you are using YAML files for a build we can create build definition using:az pipelines create --name $REPO_NAME \\ --yaml-path 'build.yml' \\ --repository-type tfsgit \\ --branch develop \\ --project \"NewProject\" \\ --repository $REPO_NAME" }, { "title": "Restart, start and stop all Azure Web Apps in resource group using PowerShell", "url": "/restart-start-and-stop-all-azure-web-apps-in-resource-group-using-powershell/", "categories": "", "tags": "azure, devops", "date": "2019-03-19 15:30:19 +0000", "snippet": "This post is short but useful. We need just two lines (we can combine it to one):$allSites = Get-AzureRmWebApp -ResourceGroupName resource-group-name@($allSites).GetEnumerator() | Restart-AzureRmWebAppThe trick is to use GetEnumerator() because above commandlet returns System.Collections.Generic.List instead of array of individual objects into pipeline.This also works with Stop-AzureRmWebApp and Start-AzureRmWebApp.Simple, fast and useful. What we need more :)" }, { "title": "Running bash script in Azure DevOps on Windows machine", "url": "/running-bash-script-in-azure-devops-on-windows-machine/", "categories": "", "tags": "vsts, azure-devops", "date": "2019-01-18 20:29:50 +0000", "snippet": "I know that title sound ridiculous, but you know in IT sometimes strange things just happen. In my case was migrating agents from Windows to Linux, and I want to have compatibility between them.You can ask if the Windows agent contains a bash? Of course, it contains. Even if you won‚Äôt install Windows Subsystem for Linux. So where is bash? In Git folder. To be precise in C:\\Program Files (x86)\\Git\\usr\\bin. If you add this to the path you have working Bash task on your Windows agent.One more tip. If you are passing paths into Arguments of your build escape them with ' so for example instead ofPackage FileVersion=1.0.0.$(Build.BuildId) Configuration=$(BuildConfiguration) Output=$(build.artifactstagingdirectory)putPackage FileVersion=1.0.0.$(Build.BuildId) Configuration=$(BuildConfiguration) Output='$(build.artifactstagingdirectory)'Second tip. Remember about git checkout and CR-LF versus LF. If you have a problem add .gitattributes file with the following contentbuild.sh text eol=lfThat was a quite shot recipe.The end!p.s. Photo by Sai Kiran Anagani on Unsplash" }, { "title": "Using SOAP security in dotnet core", "url": "/using-soap-security-in-dotnet-core/", "categories": "", "tags": "dotnet", "date": "2019-01-18 19:26:55 +0000", "snippet": "During last week, I was migrating some projects to the dotnet core. One of the most irritating parts of such migration is lack of full WebService and WCF support in dotnet core. For example when you google for SOAP security headers you end in GitHub issue WS Security in .NetCore 2.0 #2605. But seriously is it impossible?Code firstFirst we need to add a custom message header like belowvar url = configuration.Url;var wsClient = new ServiceWebClient( new BasicHttpBinding(BasicHttpSecurityMode.Transport), new EndpointAddress(url));using (new OperationContextScope(wsClient.InnerChannel)){ OperationContext.Current.OutgoingMessageHeaders.Add( new SecurityHeader(username, password, DateTime.Now)); return wsClient.SomeMethod(requestObject);}And below a definition of SecurityHeader public class SecurityHeader : MessageHeader { private readonly string _username; private readonly string _nonce; private readonly string _created; private readonly string _password; public SecurityHeader(string username, string password, DateTime created) { _username = username; _nonce = CalculateNonce(); _created = created.ToString(\"yyyy-MM-ddTHH:mm:ss.fffZ\"); _password = password; } [XmlRoot(ElementName = \"Password\", Namespace = \"http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-wssecurity-secext-1.0.xsd\")] public class Password { [XmlAttribute(AttributeName = \"Type\")] public string Type { get; set; } [XmlText] public string Text { get; set; } } [XmlRoot(ElementName = \"Nonce\", Namespace = \"http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-wssecurity-secext-1.0.xsd\")] public class Nonce { [XmlAttribute(AttributeName = \"EncodingType\")] public string EncodingType { get; set; } [XmlText] public string Text { get; set; } } [XmlRoot(ElementName = \"UsernameToken\", Namespace = \"http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-wssecurity-secext-1.0.xsd\")] public class UsernameToken { [XmlElement(ElementName = \"Username\", Namespace = \"http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-wssecurity-secext-1.0.xsd\")] public string Username { get; set; } [XmlElement(ElementName = \"Password\", Namespace = \"http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-wssecurity-secext-1.0.xsd\")] public Password Password { get; set; } [XmlElement(ElementName = \"Nonce\", Namespace = \"http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-wssecurity-secext-1.0.xsd\")] public Nonce Nonce { get; set; } [XmlElement(ElementName = \"Created\", Namespace = \"http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-wssecurity-utility-1.0.xsd\")] public string Created { get; set; } [XmlAttribute(AttributeName = \"Id\", Namespace = \"http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-wssecurity-utility-1.0.xsd\")] public string Id { get; set; } } public override string Name { get; } = \"Security\"; public override string Namespace { get; } = \"http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-wssecurity-secext-1.0.xsd\"; protected override void OnWriteHeaderContents(XmlDictionaryWriter writer, MessageVersion messageVersion) { var serializer = new XmlSerializer(typeof(UsernameToken)); var pass = CreateHashedPassword(_nonce, _created, _password); serializer.Serialize(writer, new UsernameToken { Username = _username, Password = new Password { Text = pass, Type = \"http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-username-token-profile-1.0#PasswordDigest\" }, Nonce = new Nonce { Text = _nonce, EncodingType = \"http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-soap-message-security-1.0#Base64Binary\" }, Created = _created }); } private static string CalculateNonce() { //Allocate a buffer var byteArray = new byte[32]; //Generate a cryptographically random set of bytes using (var rnd = RandomNumberGenerator.Create()) { rnd.GetBytes(byteArray); } //Base64 encode and then return return Convert.ToBase64String(byteArray); } private static string CreateHashedPassword(string nonceStr, string created, string password) { var nonce = Convert.FromBase64String(nonceStr); var createdBytes = Encoding.UTF8.GetBytes(created); var passwordBytes = Encoding.UTF8.GetBytes(password); var combined = new byte[createdBytes.Length + nonce.Length + passwordBytes.Length]; Buffer.BlockCopy(nonce, 0, combined, 0, nonce.Length); Buffer.BlockCopy(createdBytes, 0, combined, nonce.Length, createdBytes.Length); Buffer.BlockCopy(passwordBytes, 0, combined, nonce.Length + createdBytes.Length, passwordBytes.Length); return Convert.ToBase64String(SHA1.Create().ComputeHash(combined)); } }The result of above is nice security header:&lt;soapenv:Header&gt; &lt;wsse:Security xmlns:wsse=\"http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-wssecurity-secext-1.0.xsd\" xmlns:wsu=\"http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-wssecurity-utility-1.0.xsd\"&gt; &lt;wsse:UsernameToken wsu:Id=\"UsernameToken-F5BF7F4291E607828515478102461514\"&gt; &lt;wsse:Username&gt;login&lt;/wsse:Username&gt; &lt;wsse:Password Type=\"http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-username-token-profile-1.0#PasswordDigest\"&gt;mD64dXigKfYcMTGpiNaiwCwT63o=&lt;/wsse:Password&gt; &lt;wsse:Nonce EncodingType=\"http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-soap-message-security-1.0#Base64Binary\"&gt;tjJ6GXQBdrOuvujnoKuhvg==&lt;/wsse:Nonce&gt; &lt;wsu:Created&gt;2019-01-18T11:17:26.151Z&lt;/wsu:Created&gt; &lt;/wsse:UsernameToken&gt; &lt;/wsse:Security&gt;&lt;/soapenv:Header&gt;The end!p.s. Photo by Sid Ramirez on Unsplash" }, { "title": "3 simple tips for handling environment variables in JavaScript", "url": "/environment-variables-in-javascript/", "categories": "", "tags": "javascript, devops", "date": "2019-01-03 19:18:00 +0000", "snippet": "So you have a SPA application and you are proud of it. And suddenly a random developer comes and say: ‚ÄúIn a test environment, we need a bit different URL/variable/routine/stupid thing/‚Ä¶‚ÄùIn big companies, the way to solve above is a piece of cake. You go to the architect and (s)he gives you a solution. Everybody implements it and are happy ever after.Wait! No! Step back!!!! The real life doesn‚Äôt look like this. But the problem still exists. We need variables in our SPA applications. No matter in is it Angular, React or Vue. Bellow a few ideas on how to solve this problem.Solution 1 - No config at allTo implement ‚Äúno config at all‚Äù you need to create a multiple if statements based for example on URL. So when the application is on localhost variable becomes X, when it is on test becomes Y, and so on. Example code can look like:let envName = \"\";if(document.URL.startsWith(\"http://localhost\")){ envName = \"localhost\"} else if(document.URL.startsWith(\"https://test\")){ envName = \"test\"} else if(document.URL.startsWith(\"https://www\")){ envName = \"prod\"} else { console.warn(\"NOOOOOOO!!!!!\")}Pros It is working, isn‚Äôt it?Cons It isn‚Äôt testable at all until you deploy to the proper environment You cannot create a new env fast Every mistake in the config function requires deploying to every environmentSolution 2 - Build using env filesThis is the most common solution in JavaScript world. For example, in Angular, you have following files:‚îú‚îÄ‚îÄ environment.ts‚îú‚îÄ‚îÄ environment.dev.ts‚îú‚îÄ‚îÄ environment.uat.ts‚îú‚îÄ‚îÄ environment.prod.tsWith entries .angular-cli.json like:\"environmentSource\": \"environments/environment.ts\",\"environments\": { \"dev\": \"environments/environment.ts\", \"prod\": \"environments/environment.prod.ts\"}More in the official documentation. In other frameworks it is similar. For example in React projects we use .env files (example project). In Vue.js is almost same (docs)Pros It looks better Adding new environment looks easy Testable is better because we can test all files separately and check the contentsCons Still building once for all environments is not possible, unless we run npm build in deploy scripts, which can have side effects (like packages changes)Solution 3 - default + overrideDeclare your variables in one file (like above in environment.ts) and in your index.html include a separate javascript file with overrides, like &lt;script src=\"env.js\"&gt;&lt;/script&gt;. If it won‚Äôt exist or be empty your code will use defaults.Pros It looks better Adding a new environment is very easy - just deploy new env.js file It is testableCons You have to talk with backend developers or even worse with DevOps guy to create such file ;)The endDo you use something else? Maybe I am wrong, and don‚Äôt know a better solution. If yes please let me know!" }, { "title": "3 simple steps to install minikube on Azure VM", "url": "/3-simple-steps-to-install-minikube-on-azure-vm/", "categories": "", "tags": "k8s, azure, tutorial", "date": "2018-11-17 09:25:52 +0000", "snippet": "Step 0 - whyIf you want to play with K8S MiniKube is the best option. Still, you don‚Äôt believe that K8S will survive the next few years? And you don‚Äôt want to install ‚Äúnew crap‚Äù on your PC? Using VM in a cloud is a good option :)Step 1 - setup machineFirst of all, we need a shiny machine with nested virtualization. Not all of them will be able to handle nested virtualization. As I remember you need a machine with v3 suffix, for example, Standard D2s v3 will do the job.To be sure that nested virtualization is available, run:lscpu | grep -i virtualYou should see something like:Virtualization: VT-xVirtualization type: fullIf the output is empty, choose another one. The operating system I suggest Ubuntu. And after install run:sudo apt-get updatejust to make your machine updated to latest patches.Step 2 - install VirtualboxFor the MiniKube we need a hypervisor to run Kubernetess. The easiest way is to install VirtualBox. First, we need to add repository key:wget -q https://www.virtualbox.org/download/oracle_vbox_2016.asc -O- | sudo apt-key add -Then install it with:sudo apt install virtualboxStep 3 - install MiniKubeTo check latest version go to: https://github.com/kubernetes/minikube/releasesThen run (replace v.0.30.0 with the latest version):curl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.30.0/minikube-linux-amd64chmod +x minikubesudo mv minikube /usr/local/bin/Now you are ready to run MiniKube:sudo minikube start --vm-driver=virtualboxThis will take a while. Moreover, the above command suggests installing kubectl, which is the main tool for managing K8S. So do as it suggests with the command (copy it from output):curl -Lo kubectl https://storage.googleapis.com/kubernetes-release/release/v1.10.0/bin/linux/amd64/kubectl &amp;&amp; chmod +x kubectl &amp;&amp; sudo cp kubectl /usr/local/bin/ &amp;&amp; rm kubectlThere is also a ‚Äúfull‚Äù instruction on official site, but above command is good enough.To check if everything is fine:sudo kubectl run hello-world --image=k8s.gcr.io/echoserver:1.4 --port=8080sudo kubectl expose deployment hello-world --type=NodePortNow wait a bit (30 seconds) and run:curl $(sudo minikube service hello-world --url)You should see ‚Äúecho‚Äù server page, if not repeat after a while.The endNow you can play safety with MiniKube. Have fun!unsplash-logoPhoto by Maximilian Weisbecker" }, { "title": "Ridiculous bug in Azure Cosmos DB", "url": "/ridiculous-bug-in-azure-cosmos-db/", "categories": "", "tags": "azure", "date": "2018-11-05 18:43:00 +0000", "snippet": "UPDATE Read to the end, it is a problem with code and indexes, but it isn‚Äôt obvious. Basically, it is my fault and my lack of understanding what am I doing. And there is a happy end :)I didn‚Äôt write a post for half a year, I have the flu, I‚Äôm tired. But a few seconds ago I found a stupid, ridiculous stupid bug in Azure Cosmos DB. And I have to share it with the world.Imagine you have the following JSON (some fields skipped for simplicity):{ \"id\": \"token\", \"User\": { \"UserToken\": \"token\", \"Email\": \"email@email.com\" }, \"_ts\": 1521728825}And you write a following query using LINQ:await _dbClient.Where&lt;UserDocument&gt;(_collectionUri,feedOptions, d =&gt; d.User.UserToken == searchString || d.User.Email.Contains(searchString)) .OrderByDescending(d =&gt; d.Timestamp) .AsDocumentQuery().ToListAsync())When you run it with searchString==\"token\" it will return an EMPTY list. Let‚Äôs modify the LINQ a bit:await _dbClient.Where&lt;UserDocument&gt;(_collectionUri,feedOptions, d =&gt; d.User.UserToken == searchString//Bellow line is in comment /*|| d.User.Email.Contains(searchString)*/) .OrderByDescending(d =&gt; d.Timestamp) .AsDocumentQuery().ToListAsync())Magically it started working. What the hell is going on????Fast inspectionI‚Äôm found this issue on Github and copy it to my code. It didn‚Äôt help at all, but I can easily debug SQL statement. In the first version is equal to:SELECT * FROM root WHERE (root[\"User\"][\"UserToken\"] = \"token\")ORDER BY root[\"_ts\"] DESC For the second one:SELECT * FROM root WHERE ((root[\"User\"][\"UserToken\"] = \"token\") OR CONTAINS(root[\"User\"][\"Email\"], \"token\")) ORDER BY root[\"_ts\"] DESC If you run them in portal, first one works, second one doesn‚Äôt. But simple change in the second SQL to:SELECT * FROM root WHERE ((root[\"User\"][\"UserToken\"] = \"token\") OR CONTAINS(root[\"user\"][\"email\"], \"token\")) ORDER BY root[\"_ts\"] DESC makes it working. Can you see the difference? BTW changing all to lower (cammel case) won‚Äôt help!!!! That‚Äôs is why soultion from the GitHub issue didn‚Äôt help me.I‚Äôm stupid or somethingNext part. The second query is working because root[\"user\"][\"email\"] just not exist in JSON, so the doc is just query in ‚Äústrange‚Äù way. But it is more strange. It doesn‚Äôt work only for old documents. If I update a document it starts working WTF? My colleague suggests removing ORDER BY root[\"_ts\"] DESC and it is working. Magic!!!!Forrest Gump said that ‚ÄúStupid is as stupid does‚Äù, so am I stupid or something? Please write in the comment what I did wrong.Whole world to the rescueThanks to my friends and their friends and all the ‚ÄúRabbit‚Äôs Friends-and-Relations‚Äù this post was read by Samer Boshra (I suspect that he is this Samer Boshra who works as Principal Software Development Engineer at Microsoft). His full explanation is in the first comment and the most important information for me is: What‚Äôs happening here is the query gets preempted, due to either approaching the timeout or exceeding the RU budget, before encountering a single match. In such a case, the query will return a continuation token that you should be passed in to the next request to resume execution from where it stopped. Eventually, you should get all matching documents if you followed all the continuations. As for why adding/removing ORDER BY makes a difference - this is basically because the scan order changes and it would result in encountering matching documents in a different order.I decided to check what is going on. It was easy I just opened the query stats tab. And he is right. My basic RTU was 400, but the query needed:My query needed more than 2300 RTU. So it was killed, by Azure Cosmos DB engine. I can only complain to 3 things: I don‚Äôt get the exception or continuation token in my .NET code (the real origin of the problem) I don‚Äôt have such information in the portal (error: too much RTU or something like this) I don‚Äôt know databases good enough :)But to sum up. The fix is on the way. I‚Äôm happy. The sun is shining and my flu is almost over.‚Ä¶ and they lived happily ever after!!! THE END!" }, { "title": "Generate a time-based one-time password (TOTP) in Postman", "url": "/generate-totp-in-postman/", "categories": "", "tags": "javascript, tips", "date": "2018-03-15 08:44:49 +0000", "snippet": "I have a service with a one-time password, which I‚Äôm testing using the Postman app. Everything is fine except I have to use Google Authenticator/Microsoft Authenticator/Auth before every request which is quite annoying. But calculating time-based one-time password (TOTP) is a well-known algorithm. Moreover, the Postman can run JavaScript code before and after each request.Calculate TOTP in JavaScriptIf you Google for a second it is possible. For example, I found a very simple algorithm onhttps://www.thepolyglotdeveloper.com/2014/10/generate-time-based-one-time-passwords-javascript/, but it is a bit outdated because the jsSHA library changed a bit. The second try was http://jsfiddle.net/russau/ch8PK/, much better except JQuery. The only way to use it was to include full jsSHA in Postman Pre-request Script tab:Full solutionThe full code is quite long, 2314 lines, so I publish it at gist: https://gist.github.com/ptrstpp950/42660823675f6bf2f2d2f1503663553aPossibility + algorithm = working solution" }, { "title": "How to test azure function on localhost from the Internet?", "url": "/how-to-test-azure-function-on-localhost-from-the-internet/", "categories": "", "tags": "azure", "date": "2017-08-16 14:52:22 +0000", "snippet": "What a stupid question, isn‚Äôt it? You can answer like I would use my public IP and setup proxy on IIS to access it. Isn‚Äôt good enough?As I (and probably you) know life is not so simple there are firewalls, NATs and more security tools which destroy developers life;)Secure tunnels to localhostTo make life easier we can use some kind of tunnels to localhost. For example ngrok. If you never used it before a short list of features below: HTTP(S) tunnels TCP tunnels TLS tunnels custom domain and custom prefix support in paid plansSetup with Azure functionThe setup is piece of cake. Download it from the official website. Run .\\ngrok authtoken aaa to generate a config file. Its location will be printed as output. In 99% cases it is C:\\Users\\[my user name]\\.ngrok2\\ngrok.yml.Now open it in your favourite editor and replace content with following:tunnels: azurefunction: proto: http addr: 7071 host_header: localhostYou have just configured running ngrok with protocol http and https on port 7071, with rewrite host header to localhost on the template called azurefunction.Now last two steps. Run your Azure function on localhost from Visual Studio with F5 and in PowerShell (or cmd) type:.\\ngrok.exe start azurefunctionAfter a while, in the output you will see your current Internet domain for tests. It will apper in Session Status column. Moreover, you can access ngrok portal on http://127.0.0.1:4040 like belowHave fun!p.s. Of cource you can also debug your Azure function deployed to Azure. But in my opinion, this way is faster." }, { "title": "Cheapest FTP on Azure", "url": "/cheapest-ftp-on-azure/", "categories": "", "tags": "azure", "date": "2017-08-09 10:32:00 +0000", "snippet": "Azure has a storage, to be precise a cheap storage, but there isn‚Äôt an FTP as a service. Still today, some companies want to integrate using FTP instead of anything else. So what we can do?Virtual MachineIf you search, the most results will suggest using VM on which you can install something to have an FTP server. For example, you can install IIS with FTP module or Ubuntu with vsftpd.Whatever you choose this solution is bad because you will have to pay for the VM at least ~30EUR/month for the cheapest one.Website with backendThe second option which I found is install something on the Azure App Service and use blob as a storage. Example working stuff is nodeftpd with azure-storage-fs (from StackOverflow). This solution is much, much cheaper especially that you can use Free plan of Azure App Service.Going one step moreWhat if you need only to upload a file once? Or they are small enough? And you don‚Äôt need multiple logins, but SSL is a must.I had exactly such situation, so I decided to cheat a bit. I setup an Azure App Service and download deployment credentials. Because App Service support deployments with FTP and FTPS, all my requirements were met. Isn‚Äôt it sweet?Free app plan supports only 1GB, but you can easily scale it up and down for a moment when you need.In the end of the day, I have a working FTPS server for exactly zero. But maybe you have a better idea. If yes please share below?" }, { "title": "VSTS and install templates - The Good, the Bad and the Ugly", "url": "/vsts-and-install-templates-the-good-the-bad-and-the-ugly/", "categories": "", "tags": "vsts, azure", "date": "2017-08-02 19:29:00 +0000", "snippet": "If you read my blog post about deploy &amp; warm-up application in Azure, you probably remember below definition. If not you can do it now ;)It is simple, clean and easy to understand. Almost perfect. Only one question appears in my mind: how to reuse it in different environments? Let‚Äôs try to find options.CloneThe first option is very easy. Just let‚Äôs clone the deploy definition. The VSTS has even a special button for itJust clone it as many times as you need.The GoodYou have an exact copy of your environment. Even secrets are copied, so it is a perfect clone.The BadUnfortunately, there is a list of disadvantages: If you want to add a new task, you have to add it to all copied environments Deploy will be to the wrong environment for the first time with quite a big probability. (Personal note: It is because all variables are copied, and you will forget to change all places. In my case three times in a row) After few a clone&amp;change, every environment will have different deploy definition. Can you imagine that code on every environment is different? You cannot clone to another build definitionThe uglyIt is a copy&amp;paste method and you know it is ugly!Save as a templateJust one option below clone is ‚ÄúSave as a template‚Äù. I still don‚Äôt know why it is called template, because after applying it you have a simple build definition. There isn‚Äôt a relation between a build definition and the template. Such strange name probably comes from Visual Studio. When we create a new project, we choose a project template, which creates a proper MSBuild build definition.The GoodIf you are error-prof it is a great way to create a template. Or just to start. Moreover, such template can be applied to different build definition. Especially that: Any secret variable in the environment won‚Äôt be saved as part of the template.The BadLet‚Äôs check the list from cloning: If you want to add a new task, you have to add it to all copied environments Deploy will be to the wrong environment for the first time with quite a big probability. After few a clone&amp;change, every environment will have different deploy definition. Can you imagine that code on every environment is different? You cannot clone to another build definitionOnly one point was crossed out. It is sad. The only advantage to above list is that secrets aren‚Äôt copied.The uglyI don‚Äôt understand the name and how functionality differs from the clone option. Especially that deploy process should be same for every environment to make it repeatable and testable.The task groupTask group is exactly what I understand as a build template. It has same tasks but allows to use different variables. For example, Azure App Service name can be pass to the task group. Creating it a bit complicated. We mark selected tasks (I suggest all) and click Create task group.The result of above operation is one task. Its type is Task group: [the name you entered]. But there aren‚Äôt any variables. To add it we need to modify the just created group.Then in every place, you would like to have a variable, you need to replace a value with $(YOUR_NAME) variable. For sure you have to consider: Azure App Service name Resource group Your deployment process stuff. In my case, it is warm-up URL.An example is in the images below. Before change the task definition isAnd afterAfter you change all your needs in the main screen you can preview and change the description of all variables in your task group.Now in build definition, we need to re-add a task group because variables won‚Äôt show (at the time of writing). Fill them and create a new release. After success, we can reuse this task group in other build definition. Important note: if you deploy anything to Azure, don‚Äôt forget to make Azure subscription as a template variable.The goodIt is a reusable build definition template. We can even put task group in the task group in the task group, ‚Ä¶., in the task group. In my opinion, it is a good way for managing build definition, and make them as similar as it is possible. Moreover, if we need a change between environments, we can create an optional task.The badAll points from the bad part of previous ways can be crossed out. But there isn‚Äôt a way to version a task group (or I didn‚Äôt find it).The uglyIt isn‚Äôt easy to find. And manage it can be frustrating. But it solves a problem, which is just great.The endI don‚Äôt know anything more, but maybe You know something and can share with me. If yes please contact me or write a comment below." }, { "title": "Readable request headers in Application Insights", "url": "/readable-request-headers-in-application-insights/", "categories": "", "tags": "azure", "date": "2017-07-27 08:42:11 +0000", "snippet": "Querying logs is a must have. But to create simple queries, logs must be readable also for humans. Sometimes using default serialisation is not a way to go, and because we are good developers we have to fix it.AssumptionsIn the begging few assumptions: I‚Äôm using LibOwin. To read why check out my previous posts about Owin middleware in NET Standard for Application Insights part 1 and part 2 I‚Äôm trying to write my code to support classic dotnet and dotnet Core, usually using NET Standard :) I‚Äôm using the SeriLog library. Why? Let‚Äôs say because I like it. A more serious reason it is because I log objects, and with included sink, it is easily parsable by Azure stuff. If you don‚Äôt agree or know better logging lib for this purpose, just let me know in comments/email/twitter/facebook/[put something here]The problemIn Owin, or maybe I should write in LibOwin, request and response headers have type IHeaderDictionary which inherits IDictionary&lt;string, string[]&gt;. Above inheritance and a pull request no 14 from 15 June 2016, produce the following result in Analytics queriesSuch representation is ugly and difficult to query. While searching is still simple, a listing is much more complicated. For example, try list all user-agent in last 10 minutes.Why header value is an array of strings?To be precise that was my first question. After a second I found somewhere in my mind that server can send for example more than one accept header. But I need something more to know what‚Äôs going on. Especially that most useful headers in 99% cases are single values, like a User-Agent header mentioned before. In the official documentation, which in this case is RFC 2616 - HTTP/1.1 - 4.2 Message Headers, I found: Multiple message-header fields with the same field-name MAY be present in a message if and only if the entire field-value for that header field is defined as a comma-separated list [i.e., #(values)]. It MUST be possible to combine the multiple header fields into one ‚Äúfield-name: field-value‚Äù pair, without changing the semantics of the message, by appending each subsequent field-value to the first, each separated by a comma. The order in which header fields with the same field-name are received is therefore significant to the interpretation of the combined field value, and thus a proxy MUST NOT change the order of these field values when a message is forwarded.Also on StackOverflow, I found the main problem. It is a Cookie spec. The main problem is in Set-Cookie header, which contains a comma in value.So when I know the reason, and the problem I can apply a simple solutionConvert headers to a dictionaryA simple test showed me that a Dictionary&lt;string,string&gt; serialise as we can expect: key and value pair. So I created a simple extension method which solved my problem.internal static IDictionary&lt;string, string&gt; ConvertHeadersToDictionary(this IHeaderDictionary headers){ var result = new Dictionary&lt;string, string&gt;(); foreach (var header in headers) { if (header.Value.Length == 1) { result.Add(header.Key, header.Value.First()); } else { for (var i = 0; i &lt; header.Value.Length; i++) { result.Add($\"{header.Key}.{i}\", header.Value[i]); } } } return result;}The result below. It is beatiful, isn‚Äôt it?The query is also simple. For example, if we want to find all requests which accept en-US language we can write below querytraces| where customDimensions[\"Headers.Accept-Language\"] contains \"en-US\" If you have a better idea, please let me know. All ideas welcome :)" }, { "title": "Debug production safely using Application Insights", "url": "/debug-production-safety-using-application-insights/", "categories": "", "tags": "azure, vs2017", "date": "2017-06-21 08:44:00 +0000", "snippet": "Debugging production is bad. Even access to production is bad. But debugging production is evil. I even cannot describe how evil I think it is. But if you ask most developers (including myself), they will say that this can be useful. Up to last week, I thought it cannot be done in safety way, but a new feature in Application Insights called Snapshot collector.What does it mean?Requirements Application Insights must be turned on for your application (obvious) Microsoft.ApplicationInsights.SnapshotCollector must be installed and configure (default should do the job) in your application PDB files can be useful, to preview errors.Deploy and testNow you are ready to create a deploy and generate an exception. Not every exception is dumped and it is a default threshold set to 5.When you generate enough exceptions there will be a new button in the Azure portal in Application Insights failures sections. To get there go to Your Application Insights instance -&gt; Failures -&gt; Total of Server exceptions by Problem Id -&gt; Select interesting exception and you will see the exception detailsIf you have PDB on your server you can preview lines. Moreover, you can deselect the Show Just My Code option to see full stack.Now we are ready to click the Open Debug Snapshot button.Using portalIn the portal, there is more than you can expect and it is for free. We can preview call stack with details and local variables. Fair enough to diagnose problems. Of course, some variables cannot be evaluated. My exception happened in below code:private IEnumerable&lt;string&gt; SetupUmbracoRoles(ClaimsIdentity user){ var email = user.FindFirst(ClaimTypes.Name).Value; var userService = ApplicationContext.Current.Services.UserService; var umbraUser = userService.GetByEmail(email); var userGroups = user.FindAll(\"groups\"); var rolesList = userGroups as IList&lt;Claim&gt; ?? userGroups.ToList(); var admin = rolesList.Any(r =&gt; r.Value == AzureADGroupNames.CorpoBlogAdmin); if (!admin &amp;&amp; !rolesList.Any(r =&gt; r.Value == AzureADGroupNames.CorpoBlogEditors)) { if (umbraUser != null) { umbraUser.AllowedSections.ToList().ForEach(s =&gt; umbraUser.RemoveAllowedSection(s)); umbraUser.UserType = userService.GetUserTypeByAlias(UmbracoRoleNames.Translator); userService.Save(umbraUser); } telemetry.TrackEvent(\"unauthorized acces\", new Dictionary&lt;string, string&gt;() { { \"user\", email } }); throw new HttpException(403, \"You do not have any roles configured for the application\"); } //Rest of the code}For above the Debug Snapshot view looks like followingAs you can see for some variables it cannot obtain the value. The reason is simple: Cannot obtain value of the local variable or argument because it is not available at this instruction pointer, possibly because it has been optimized away.Except that it is really useful, especially that you can navigate through the whole stack.Open snapshot in Visual StudioThat‚Äôs sound interesting, but there are two additional requirements: Visual Studio 2017 Enterprise installed, but it can be one in the whole company Visual Studio Snapshot Debugger extension installedNow we are ready to download a snapshot. In my case, it is around 175MB. And such snapshot is stored in the application insights.I suggest opening the dump inside the project you are debugging. Then click proper debug option and how funMy exception looks like following:To be precise the only advantage of downloading the snapshot into Visual Studio is that you can download additional PDB files, which aren‚Äôt available in the Azure portal.Common problemsIf you have more problems, please let me know I will add them here: Exception count doesn‚Äôt exceed the threshold. There is a default value set to 5, which means that at least you have to generate more than 5 exceptions to get the dump. In my case, it was the seventh try, don‚Äôt ask me why. You haven‚Äôt an Application Insights Snapshot Debugger role. If you are a subscription master/owner/admin/god the proper window will appear and you can add it instantly. Otherwise, you will see:Is possible to debug production safety?Using the snapshot debugger with its limitations, the answer is yes. Futhermore I will have to check more limits, configuration, etc to make it more powerful. But I‚Äôm sure that I will use it." }, { "title": "Caring is sharing (vol.1) - OAuth", "url": "/caring-is-sharing-vol-1/", "categories": "", "tags": "caring-is-sharing", "date": "2017-06-09 08:10:00 +0000", "snippet": "Last week I was mainly interested in deeply understanding OAuth, JWT and how to implement it in .NET (Core). https://stormpath.com/blog/authentication-asp-net-core - simple list of basic tools you can use in .NET. Not perfect, but a good point to start. http://nordicapis.com/how-to-handle-batch-processing-with-oauth-2-0/ - great explanation how we can use OAuth in batch processing. I‚Äôm not 100% sure that it is a perfect solution, but this opens my mind to new ideas. http://kevinchalet.com/2016/07/13/creating-your-own-openid-connect-server-with-asos-creating-your-own-authorization-provider/ - this not a single post, but a long series. Strongly recommend if you want to understand how OAuth is working. MsBuild day 1 video from ~53:00 about how to switch user authentication in ASP.NET Core. If you know Polish you can read Marek Grabarz posts about OAuth in Azure B2C - https://marekgrabarz.pl/tag/oauth2-0/ Another post series (thanks got to Tomasz Onyszko) about Open id connect. The most interesting article: OpenID Connect Session Management using an Angular application and IdentityServer4What else? Universities finally realize that Java is a bad introductory programming language - do you remember your first programming lesson? I do and for writing simple ‚ÄúHello World‚Äù I had to use many strange words like public static void Main. Isn‚Äôt this too complicated for a beginner?" }, { "title": "Run Angular cli, React cli, Aurelia or Vue cli build in VSTS", "url": "/run-angular-cli-react-cli-aurelia-or-vue-cli-build-in-vsts/", "categories": "", "tags": "vsts", "date": "2017-06-07 07:35:30 +0000", "snippet": "Modern JavaScript frameworks come with dedicated command line interface (CLI). It applies for: Angular -&gt; Angular CLI React -&gt; react cli - not offical Vue.js -&gt; vue cli Aurelia -&gt; aurelia cli and much moreIt is great for developers because their life is easier. The only question is how to run such build in VSTS without multiple hacks.Before we startThe article is based on VSTS, but as I know it can be applied the same way in TeamCity, Jenkins and other CI tools.Command line taskVSTS supports npm, gulp and grunt task. Simple drag&amp;drop. But for example, Angular has it own CLI. The suggested build command isng bulid --env=prodTo run the custom command in VSTS we can create command line task for Windows agent or batch script if you are on Linux. The example command is:$(Build.SourcesDirectory)\\node_modules\\.bin\\ng.cmdAbove assume that in package.json we will have saved proper CLI as dev dependency.According to above, the task will have below definition.Of course on Linux agent change slash to backslash. If you are confused about slash and backslash, it is easy to remember with Slash pictures ;)First success. The above script is working. Thu build logs are ugly but who cares. But can we do it better?Npm scriptInstead of running command line, we can put a build command into package.json file. Moreover, most nowadays templates contain it already. For example, after we create a project from Angular template we have got{ \"name\": \"Your name here!\", \"version\": \"0.0.0\", \"license\": \"MIT\", \"scripts\": { \"ng\": \"ng\", \"start\": \"ng serve\", \"build\": \"ng build\", \"test\": \"ng test\", \"lint\": \"ng lint\", \"e2e\": \"ng e2e\" }, //unimportant rest of the fileTo be precise we can run build using:npm run buildWhich is equal to above ng build. Of course, we can add options like in the original command. To use custom arguments when executing scripts, but you need to prepend them with --. For example, to build production environment we run:npm run build -- --env=prodWhich is exactly same as the original command ng bulid --env=prod. The npm command looks, in my opinion, more ugly but it is more CI friendly.Using it has an unexpected bonus. We can forget about slash and backslash configuration, and run the same build definition on Windows and Linux agent.Npm taskIn the VSTS we can choose npm task and apply arguments like below.To sum up. If we decide to wrap framework command line interface with npm scripts, the build process is a piece of cake and logs looks much more human-friendly.If you have a better idea or have some doubts, please let me know." }, { "title": "How to deploy & warm-up an application in Azure using VSTS", "url": "/how-to-deploy-and-warmup-an-app-in-azure/", "categories": "", "tags": "vsts", "date": "2017-06-06 07:13:00 +0000", "snippet": "Disclaimer This article was also published on my company blog. You can read it in Polish and in English on FinAi blog.This article is based on Umbraco site deployment, but I‚Äôm sure that it will fit in most common ASP.NET projects. If you will have any problems, just let me know.Create a build definitionFirst, we need to create a build definition. In most common cases just choose ASP.NET (Preview) template and it will work like a charm. Of course, as always some error will appear. To fix it on your machine check out your project into the clean directory and run:msbuild.exe YourProject.sln /p:DeployOnBuild=true /p:WebPublishMethod=Package /p:PackageAsSingleFile=true /p:SkipInvalidConfigurations=true /p:PackageLocation=\"c:\\tmp\\\"In my experience the most common problems are: not existing files included in csproj (problem with .gitignore) references to NuGet packages from other projectsBe patient, cross your fingers, and in after a while, it will work. Success!Make it installThat should be simple, isn‚Äôt it? Few steps maybe more. First, we need to create release definition. In most cases, your choice is the Azure App Service Deployment. We can add more tasks later.In the next step, you can mark ‚ÄúContinuous deployment‚Äù option, so every build will create a release and try to deploy it. For dev environment, it should be checked. After pressing ‚ÄúCreate‚Äù button new task shows on the list. Magic!Fill ‚ÄúAzure subscription‚Äù and ‚ÄúApp Service name‚Äù and you are ready for the first deploy. I suggest renaming ‚ÄúEnvironment 1‚Äù on left to something meaningful like ‚Äúdev‚Äù, ‚Äúuat‚Äù or ‚Äúperf‚Äù. Press ‚Äú+Release‚Äù button and run your first release. In my case, it is working like a charm, but when I try to redeploy I got following error:Error Code: ERROR_FILE_IN_USE More Information: Web Deploy cannot modify the file 'XYZ.dll' on the destination because it is locked by an external process. In order to allow the publish operation to succeed, you may need to either restart your application to release the lock, or use the AppOffline rule handler for .Net applications on your next publish attempt. Learn more at http://go.microsoft.com/fwlink/?LinkId=221672#ERROR_FILE_IN_USE.To fix it we need to inspect and change ‚ÄúAdditional Deployment Options‚Äù section in Deploy Azure App Service task. A simple fix is to check ‚ÄúTake App Offline‚Äù like below:Ready? Steady? Wait for a second!! There are a few additional options to consider: Remove additional files at destination - if you don‚Äôt mark it after a while you can have a garbage on your server. If you use before an HTML5 clipboard helper with fallback to Adobe Flash (like ZeroCliboard) you can create a security hole on your server (e.g XSS on old ZeroClipboard). But if you write any files on the server you will delete them. Of course writing files on Azure App Service in not the best way to create software, but it happens. The better option is using a Blob Storage for your files, and Application Insights for logs. But if you really must to save some files and keep them on the server, check the next option Exclude files from the App_Data folder - the purpose of App_data folder is simple: Contains application data files including .mdf database files, XML files, and other data store files. (from official documentation). So this is the place when ASP.NET app should write files if it really must. If you remove this folder you can lose some data.In my opinion, both options should be checked.Warm-upThe saddest thing about classic ASP.NET websites is startup time. No matter how big is your application I‚Äôm sure that you thing that startup can be faster. Of course, you can optimize some operations. For example, to reduce MVC views compilation time, you can use Razor Generator, for dependency injection you can try faster IoC container (simple benchmark), and so on. But still it can be faster for the first user, isn‚Äôt it? The simple way to cheat is to warm up the site ourselves. We can use a range of tasks from VSTS to create and run even cloud web performance test. But there isn‚Äôt a simple task to just invoke a bunch of URLs. But we can create a PowerShell task to do the job:param( [Parameter(Mandatory)] [string] $rootUrl)# Make sure that website is alivefor($tryIndex=0; $tryIndex -le 10; $tryIndex++){ try{ $time = Measure-Command{Invoke-WebRequest $rootUrl -UseBasicParsing -ErrorAction Stop -ErrorVariable siteIsNotAlive} Write-Host \"Site is running\" Write-Host \"wget $rootUrl in $($time.TotalSeconds)\" break; } catch{ Write-Host \"Sleep + repeat\" Start-Sleep -s 1 }}# url suffixes$suffixes = @(\"/\", \"/url_1\", \"url_2\")$suffixes | ForEach-Object{ $url = $rootUrl+$_; $time = Measure-Command{Invoke-WebRequest $url -UseBasicParsing} Write-Host \"wget $url in $($time.TotalSeconds)\"}Probably you ask yourself why I included the part after comment: Make sure that website is alive. It is because azure app service doesn‚Äôt start instant. When the site is still offline, it returns an HTTP error code. You can put above script in VSTS directly or commit it to your repository. I believe that second option is better because deployment scripts are code as anything else. So they should be in the repository. But after a commit, we cannot select it, because it is inside MSDeploy package. To fix it we need to modify the csproj file to copy above script in the same location as the MSDeploy package. The change in csproj for MSBuild is:&lt;Target Name=\"CopyDeployScriptToPackageFolder\" AfterTargets=\"Package\"&gt; &lt;Copy SourceFiles=\"$(MSBuildProjectDirectory)\\deployment\\warm-up.ps1\" DestinationFolder=\"$(PackageLocation)\" /&gt;&lt;/Target&gt;Now new task in VSTS is very simple:DowntimeIf you still here you probably see the problem: the site downtime is long. Or to be precise is too long for [put your important reason here]. There is a way to fix this in Azure. We need to use application slots. Such solution isn‚Äôt cheap because you will have to use Standard or Premium plan before we can add a slot. After migration adding a new slot is a piece of cake. Just click Add Slot button in Deployment slots section in App Service tile in the Azure portal.The slot is created, so we can change the Deploy Azure App Service task. Click Deploy to slot checkbox and select created slot. Moreover, we have to change warm-up script argument to slot address, because our slot has different URL.But how to swap slots? Again add a new task to VSTS workflow. This time an ‚ÄúAzure App Service Manage‚Äù task. In the time of writing it has a PREVIEW label, but for me, it works perfectly. In the end, we can also stop the slot. After deployment, it contains the previous version of our service. It will be easy to rollback, just swap slots again, to restore it. One more thing, we have to make make sure that our slot is running before running the warm-up script. The easiest way is by adding start slot task. In the end, the full release definition is:There is only one big disadvantage of above setup. We cannot use the App_data folder anymore because it doesn‚Äôt sync between slots. To save data we need to use other Azure stuff like blobs, external logging, SQL database, etc.p.s. If you are interested more in VSTS, you can check my next article in this topic: VSTS and install templates - The Good, the Bad and the Ugly" }, { "title": "Owin middleware in .NET Standard for Application Insights - part 2", "url": "/owin-middleware-in-net-standard-for-application-insights-part-2/", "categories": "", "tags": "dotnet", "date": "2017-05-30 07:14:54 +0000", "snippet": "Some things were done, much more to do. If you didn‚Äôt read part 1, look at it before reading this one.Simple trackingThe basic telemetry tracking method is a piece of cake. The example code belowpublic async Task Invoke(IDictionary&lt;string, object&gt; owin){ var context = new OwinContext(owin); var requestStart = DateTimeOffset.Now; var sw = Stopwatch.StartNew(); try { await _next.Invoke(owin).ConfigureAwait(false); sw.Stop(); var name = context.Request.Path.HasValue ? context.Request.Path.Value : \"unknown\"; var requestTelemetry = new RequestTelemetry( name, requestStart, sw.Elapsed, context.Response.StatusCode.ToString(), _isRequestSuccessfulFunc(owin)) {Url = context.Request.Uri}; requestTelemetry.Properties.Add(\"HttpMethod\", context.Request.Method); _client.TrackRequest(requestTelemetry); } catch (Exception exception) { _client.TrackException(exception); sw.Stop(); }}One explanation to above code. There is a requestTelemetry.HttpMethod but it is marked as obsolete, so I decided to pass it as custom properties. So instead I haverequestTelemetry.Properties.Add(\"HttpMethod\", context.Request.Method);Passing custom propertiesIn some cases, we want to pass some custom to telemetry. For example URL referrer or error HTTP context or some headers. Whatever you like. To make it flexible as much as possible I decided to use an Action, which will modify a properties dictionary. Of course, the default action should be empty, so the constructor changed a bitpublic HttpRequestTelemetryTrackingMiddleware( AppFunc next, Func&lt;IDictionary&lt;string, object&gt;, bool&gt; isSuccessRequestFunc = null, Action&lt;IDictionary&lt;string, string&gt;&gt; telemetryPropertiesSetter = null, TelemetryConfiguration configuration = null){ _next = next; _isRequestSuccessfulFunc = isSuccessRequestFunc ?? (owin =&gt; new OwinContext(owin).Response.StatusCode &lt; 400); _client = configuration != null ? new TelemetryClient(configuration) : new TelemetryClient(); _telemetryPropertiesSetter = telemetryPropertiesSetter ?? (dict =&gt; { });}Moreover, you can notice one more change, from the previous article. I decided to remove OwinContext from above method because I hide LibOwin. Creating an OwinContext in dotNET Framework or dotNET Core is very easy and in both can be written as new OwinContext(owin). When I introduced a second Owin implementation, it just makes a rubbish instead of clean code.Example telemetry setterIn my case, I decided to track HTTP referer and some of the request headers. The example code is:Action&lt;IDictionary&lt;string, object&gt;, IDictionary&lt;string, string&gt;&gt; telemetrySetter = (owin, properties) =&gt;{ var ctx = new OwinContext(owin var dictionary = ctx.Request.Headers.Keys.Where(key =&gt; key != \"UMB_UCONTEXT\") .ToDictionary(key =&gt; key, key =&gt; ctx.Request.Headers[key] if (ctx.Request.Headers.ContainsKey(\"Referer\")) { properties.Add(\"urlReferrer\", ctx.Request.Headers[\"Referer\"]); properties.Add(\"requestHeader\", JsonConvert.SerializeObject(dictionary)app.Use&lt;HttpRequestTelemetryTrackingMiddleware&gt;(isSuccessfulRequest, telemetrySetter);To check what can be useful I suggest looking at Migrating HTTP handlers and modules to ASP.NET Core middleware article. It isn‚Äôt exactly about Owin properties, but almost perfectly describes possible properties.GetLastServerErrror or HttpContext?If you code in ASP.NET, you probably saw GetServerError before. It is the last chance to get know what had happened. It is also accessible by HttpContext.Current.Error. It is an example of properties, which we cannot access in Owin because Owin‚Äôs pipeline works in a bit different way. I was thinking about accessing HttpContext in _telemetryPropertiesSetter action, but there is one problem:await _next.Invoke(owin).ConfigureAwait(false);According to the accepted answer on Best practice to call ConfigureAwait for all server-side code question on StackOverflow, such method will lose invocation context. The only difference ConfigureAwait makes in ASP.NET is whether that thread enters the request context when resuming the method.Such information makes me think about above code, but I‚Äôm not sure if I should change it to ConfigureAwait(true);. Maybe should I just handle errors using for example IHttpModule instead? What do you think?" }, { "title": "Owin middleware in .NET Standard for Application Insights", "url": "/owin-middleware-in-net-standard-for-application-insights/", "categories": "", "tags": "dotnet, azure", "date": "2017-05-22 20:16:01 +0000", "snippet": "I‚Äôm sure that you heard about .NET Standard. To simplify the definition just one quote from the offical GitHub repo FAQ .NET Standard is a specification that represents a set of APIs that all .NET platforms have to implement. This unifies the .NET platforms and prevents future fragmentation.BTW in case you don‚Äôt know just read the full article Introducing .NET Standard.On the other hand, Owin is (from owin.org): OWIN defines a standard interface between .NET web servers and web applications. The goal of the OWIN interface is to decouple server and application, encourage the development of simple modules for .NET web development, and, by being an open standard, stimulate the open source ecosystem of .NET web development tools.Both tools should work together in a perfect way. In my case, I tried to find a way to use Application Insights in the OWIN based applications. I have some in .NET Framework and in .NET Core.Application Insights OWIN extensionsThere is an open source project Application Insights OWIN extensions. Sounds perfect, isn‚Äôt it? But unfortunately, it isn‚Äôt. As we check it on I can have .NET Core, it has some problems. Mostly, because of its reference to Microsoft.Owin package. There is a known replacement for .NET Core called Microsoft.AspNetCore.Owin.MigrationI migrated a class to uses the Microsoft.AspNetCore.Owin package:public class RequestTrackingMiddleware{ private readonly RequestDelegate _next; public RequestTrackingMiddleware(RequestDelegate next) { _next = next; } public async Task Invoke(HttpContext context) { var sw = Stopwatch.StartNew(); await _next(context); sw.Stop(); Console.WriteLine(sw); }But this approach is stupid. It is because I cannot use above class in standard .NET Framework. I ended with a question on Stack Overflow. In the middle my colleague Marcin send me an article about How to write OWIN middleware in 5 different steps. It is exactly the same as David Fowl posted in the answer. The third step solves my problem. To simplify the answer we just need to know that all Owin wrappers are made on the top of IDictionary&lt;string, object&gt; environment class. And Invoke method must return the Task. Are you ready? Check the next paragraph for the working code.Creating portable codeThe first working example which I created is very simple.using AppFunc = Func&lt;IDictionary&lt;string, object&gt;, Task&gt;;public class OwinMiddleware{ private readonly AppFunc _next; public OwinMiddleware(AppFunc next) { _next = next; } public async Task Invoke(IDictionary&lt;string, object&gt; environment) { var sw = Stopwatch.StartNew(); await _next.Invoke(environment); sw.Stop(); } }But such code isn‚Äôt easy to use because you need to know how to access specific key on the dictionary. To find out what are correct keys probably the easiest way is to use the debugger like below. The wrappers are better, because they give us access using know properties like Request or Response`Owin wrapperSo we need an Owin wrapper, without dependencies. And there is exactly such. It is a LibOwin. But installing using the newest NuGet won‚Äôt work. The package will be installed, but there won‚Äôt be a file App_Packages\\LibOwin.X.Y\\LibOwin.cs as it should. This bug probably is caused by newest NuGet.Anyway, I decided to add manually from the official repo. Hurray, now we have an IntelliSense.But life is life and there are two compile errors. One is easy to fix just install System.Security.Claims NuGet package. For the second one, we need a deeper search. The answer is System.Globalization.Extensions package.Above changes allow creating a quite simple method.public async Task Invoke(IDictionary&lt;string, object&gt; environment){ var context = new OwinContext(environment); await _next.Invoke(environment); _log.Info(\"Resonse staus is: \"+context.Response.StatusCode);But how to pass _log variable? It is not very compilcated.Additional argumentsBecause I would like to track all requests and send them Application Insights I created following constructorpublic HttpRequestTelemetryTrackingMiddleware( AppFunc next, TelemetryConfiguration configuration = null){ _next = next; _client = configuration != null ? new TelemetryClient(configuration) : new TelemetryClient();}And use it like below in classic .NET Frameworkpublic override void Configuration(Owin.IAppBuilder app){ app.Use&lt;HttpRequestTelemetryTrackingMiddleware&gt;();}Unfortunatley, without a reason, this cause an error: The class 'HttpRequestTelemetryTrackingMiddleware' does not have a constructor taking 1 arguments.I spent some time why this error happens, but I still don‚Äôt know the reason. If you do, please leave a comment or send me an info. Anyway, I added the second constructor to save my time. After above changes and adding a Microsoft.ApplicationInsights package, the Invoke function is:public async Task Invoke(IDictionary&lt;string, object&gt; environment){ var context = new OwinContext(environment); var requestStart = DateTimeOffset.Now; var sw = Stopwatch.StartNew(); try { await _next.Invoke(environment); sw.Stop(); var path = context.Request.Path.HasValue ? context.Request.Path.Value : \"unknown\"; var requestTelemetry = new RequestTelemetry( context.Request.Method + \" \" + path, requestStart, sw.Elapsed, context.Response.StatusCode.ToString(), _isRequestSuccessfulFunc(environment)); _client.TrackRequest(requestTelemetry); } catch (Exception exception) { _client.TrackException(exception); sw.Stop(); }}If you decide to copy&amp;paste above code, you will notice that it won‚Äôt compile, because _isRequestSuccessfulFunc is missing. The main aime of it is to detect that request is correct.When HTTP request is a correct one?A simple answer is when its value is 200. But that‚Äôs not true. For example, there are more statuses in the range between 200 and 300, like 204 No Content. They are correct too.The next try is all statuses lower than 400, should be good. But this is untrue too. For example, we don‚Äôt want to have temporary redirects (status 302) but status 304 Not Modified is perfect, cause it indicates that the resource has not been modified and the client can use his local cache.Moreover, you want to accept even some 404 Not Found statuses for some requests. For example, in my case, my deployment script makes a fake request to make sure that my 404 page is working correctly.So the final answer is: it is complicated. To make a decision we need more information and it depends on your current project. If you need to check HTTP response statuses, you can check List of HTTP status codesBut the default function can be very simple. I just need to have a control on it. Below the default implementation in the constructor. Notice that I introduced one more parameter.public HttpRequestTelemetryTrackingMiddleware( AppFunc next, Func&lt;IDictionary&lt;string, object&gt;, bool&gt; isSuccessRequestFunc = null, TelemetryConfiguration configuration = null) { _next = next; _isRequestSuccessfulFunc = isSuccessRequestFunc ?? (owinCtx =&gt; new OwinContext(owinCtx).Response.StatusCode &lt; 400); _client = configuration != null ? new TelemetryClient(configuration) : new TelemetryClient()}This extension is as much flexible as it is possible, but it will be difficult to use. Because in each implementation we will be missing Owin context helpers. To make it simpler we can make OwinContext from LibOwin public and pass it as an argument instead of IDictionary. To use it we need to define compilation constant LIBOWIN_PUBLIC in the build tag of our project.After above the constructor simplifies a bit.public HttpRequestTelemetryTrackingMiddleware( AppFunc next, Func&lt;IOwinContext, bool&gt; isSuccessRequestFunc = null, TelemetryConfiguration configuration = null){ _next = next; _isRequestSuccessfulFunc = isSuccessRequestFunc ?? (owinCtx =&gt; owinCtx.Response.StatusCode &lt; 400) _client = configuration != null ? new TelemetryClient(configuration) : new TelemetryClient()}What‚Äôs coming next?Above code only measure a time of HTTP requests, but I need more. I would like to have control on user id, operation id and maybe a session id. Next time I will share my progress how we can track more. So are you ready for the part 2? It is already available: Owin middleware in .NET Standard for Application Insights part 2" }, { "title": "Search and clean redundant JS and CSS on your page", "url": "/chrome-coverage/", "categories": "", "tags": "web", "date": "2017-04-18 06:56:00 +0000", "snippet": "Creating a web page is easy. But making it fast it isn‚Äôt a piece of cake. First steps are ‚Äúeasy‚Äù. You optimize the server-side response time, images sizes and weight, minification, and bundling. And then you probably see the following error in PageSpeed Insights: Eliminate render-blocking CSS in above-the-fold contentWhat? How can I find which CSS is redundant?Chrome to the rescueIn the latest Chrome (the minimum version is 59 according to the release notes), there is a new tool called Coverage. To run it just launch developers tools and press three dots on the lower left:The rest is ‚Äúeasy‚Äù. Run your browser in incognito mode to skip scripts and CSS from extensions, press record, visit your site and review results. For example, on my blog most lines of the foundation.css file are redundant and I can delete it:For the first time in my life, I can easy check which part of CSS are used on my website. I hope in near future we can use this functionality for example with selenium on CI. That‚Äôs really great." }, { "title": "Working on IISExpress with custom domain and SSL on 443", "url": "/working-on-iisexpress-with-custom-domain-and-ssl-on-443/", "categories": "", "tags": "visual-studio", "date": "2017-03-26 17:31:00 +0000", "snippet": "Working with IISExpress in extremely useful. But running in on custom domain and SSL on a default port (443) is a bit tricky.Custom domainIf you are running Visual Studio as administrator, you can put https://mystuff.local and it will work like a charm. But without an admin privilege, it simply doesn‚Äôt. To fix it we need to configure the netsh tool. The command is quite simple:netsh http add urlacl url=https://mystuff.local:{port}/ user={login}Where {port} is a port number which you would like to use. In my case, I will unblock 80 and 443. To obtain {login} just run whoami in the PowerShell or CMD prompt.The second step is to add proper binding in the application.host config file under the .vs folder in your project. It will look more or less like below:&lt;site name=\"WebApp(1)\" id=\"3\"&gt; &lt;application path=\"/\" applicationPool=\"Clr4IntegratedAppPool\"&gt; &lt;virtualDirectory path=\"/\" physicalPath=\"C:\\projects\\MyWebApp\\WebApp\" /&gt; &lt;/application&gt; &lt;bindings&gt; &lt;binding protocol=\"http\" bindingInformation=\"*:80:mystuff.local\" /&gt; &lt;binding protocol=\"https\" bindingInformation=\"*:443:mystuff.local\" /&gt; &lt;/bindings&gt;&lt;/site&gt;Now in the Visual Studio, we can configure our domain for HTTP protocol. The F5 button should work. For HTTPS on 443 we still have a problem even with admin privilege. For ports like 443xx it is working like a charm. Why?HTTPSFirst of all, let‚Äôs find out why 443xx ports work. The netsh is again our best friend. Let‚Äôs run:netsh http show sslcertOMG! A lot of lines! But don‚Äôt worry, there are almost same. The only difference is the port number. That is the reason why HTTPS works on IISExpress any port like 443xx. Bellow the last one:IP:port : 0.0.0.0:44399 Certificate Hash : {HASH YOU ARE LOOKING FOR} Application ID : {GUID YOUR ARE LOOKING FOR} [rest of the entry]Now using above run in the elevated cmd promptnetsh http add sslcert ipport=0.0.0.0:443 certhash=HASH_FROM_ABOVE appid={GUID_FROM_ABOVE}In the PowerShell prompt you will recive an error that parameter is incorent because of { and } chars so the correnct command is:netsh http add sslcert ipport=0.0.0.0:443 certhash=HASH_FROM_ABOVE appid=\"{GUID_FROM_ABOVE}\"And that‚Äôs it. Now you can run IISExpress safety on https://mystuff.local/. Happy coding." }, { "title": "Using letter Pi(œÄ) in JavaScript", "url": "/using-letter-pi-in-javascript/", "categories": "", "tags": "javascript", "date": "2017-03-09 18:13:37 +0000", "snippet": "Today I was doing some math in JavaScript. I needed a simple calculation from degrees to radians. Of course, I didn‚Äôt remember the formula. I copied it from Rapid Tables but I forgot to replace œÄ char. Unexpectedly I received a normal error:Uncaught ReferenceError: œÄ is not definedWhat? Is Pi sign a valid variable in JavaScript?DocumentationThe official ECMA Script documentation is quite complicated. But we can simplify this to (by Mathias Bynen):An identifier must start with $, _, or any character in the Unicode categories ‚ÄúUppercase letter (Lu)‚Äù, ‚ÄúLowercase letter (Ll)‚Äù, ‚ÄúTitlecase letter (Lt)‚Äù, ‚ÄúModifier letter (Lm)‚Äù, ‚ÄúOther letter (Lo)‚Äù, or ‚ÄúLetter number (Nl)‚Äù.This allows us to write following code from ES5var œÄ = Math.PI;Isn‚Äôt this cool?" }, { "title": "Discover what is inside webpack bundle", "url": "/discover-what-is-inside-webpack-bundle/", "categories": "", "tags": "javascript", "date": "2017-03-07 20:08:18 +0000", "snippet": "You start a new project using magic tools like yo, a built-in CLI (e.g. aurelia-cli, angular-cli, react-cli, vue-cli, omg-new-framework-cli) or just a template from GitHub. Most of them have a webpack inside because this is a fancy module bundler nowadays. But what exactly is inside of built bundle?Running plain webpack toolBefore we start optimization we need to run plain webpack. In most templates, it is hidden behind grunt, gulp or native command. Running it manually is simple: ./node_modules/.bin/webpackbut I‚Äôm sure that you will end with an error like:No configuration file found and no output filename configured via CLI option.A configuration file could be named 'webpack.config.js' in the current directory.Use --help to display the CLI options.So instead we should run (replace ./conf/webpack-dist.conf.js with proper path)./node_modules/.bin/webpack --config ./conf/webpack-dist.conf.jsThis will give a nice output with all files listed, but it isn‚Äôt readable enough.Make it more readableLike always we need another NPM. For example a Webpack Bundle Size Analyzer. After install we can run:./node_modules/.bin/webpack --config ./conf/webpack-dist.conf.js --json | webpack-bundle-size-analyzer.cmdAnd receive a simple plain result where &lt;self&gt; is your codexxx: 169.66 KB (44.7%)xxx-validate: 81.14 KB (21.4%)xxx-router: 54.8 KB (14.5%)xxx-resource: 34.22 KB (9.03%)style-loader: 6.74 KB (1.78%)css-loader: 1.42 KB (0.375%)xxx-loader: 1.12 KB (0.295%)webpack: 509 B (0.131%)&lt;self&gt;: 29.56 KB (7.80%)" }, { "title": "Search through archives using C#", "url": "/search-through-archives/", "categories": "", "tags": "code", "date": "2017-02-27 18:40:00 +0000", "snippet": "I had a small problem, I need to grep IIS logs and search for few lines. There was only one problem - the size of logs. A few gigabytes of compressed data with 15% compress ratio. The interesting stuff for me happened between 9:00 and 10:00 AM, so everything after 10:00 AM I can just skip.Using a log aggregator.The best option for storing and searching logs is log aggregator. But IIS logs are heavy and searching them is rather a last resort. If I had them on ELK, Splunk or even Hadoop grep will be easy.The first try.I used simple Select-string command combine with 7-zip for unraring my logs and delete them after the search. Why delete? Uncompress data were too large for my hard disk. Everything was good until I didn‚Äôt found a mistake in my search after operation finished.Let‚Äôs make some code.The most obvious optimization is to stop searching logs which happened after 10:00 AM. The Select-string doesn‚Äôt have such option and I don‚Äôt have to unpack everything. Therefore, I created a simple program.After a search, I decide to use a NuGet package SharpCompress. It has a simple API, and it is powerful. The first version looks like following:using System;using System.Diagnostics;using System.IO;using SharpCompress.Readers;namespace FastIISExtractor{ class Program { static void Main(string[] args) { var stopwatch = new Stopwatch(); stopwatch.Start(); const string maxTime = \"2017-02-02 10:00\"; const string searchValue = \"phrase to find\"; const string searchDirectory = @\"d:\\somewhere\\\"; const string searchPattern = \"*.rar\"; const string resultFile = @\"d:\\somewhere2\\out.log\"; using (var sw = new StreamWriter(resultFile)) { foreach (var file in Directory.EnumerateFiles(searchDirectory, searchPattern)) { using (var stream = File.OpenRead(file)) { var reader = ReaderFactory.Open(stream); while (reader.MoveToNextEntry()) { if (reader.Entry.IsDirectory) continue; Console.WriteLine($\"------ Processing: {reader.Entry.Key} in {file} -------\"); using (var streamReader = new StreamReader(reader.OpenEntryStream())) { string line; while ((line = streamReader.ReadLine()) != null) { if (line.Contains(maxTime)) { Console.WriteLine(\"------ To far stop processing-----\"); break; } if (!line.Contains(searchValue)) continue; sw.WriteLine(line); Console.WriteLine(\"Found line\" + line); } } } } } } stopwatch.Stop(); Console.WriteLine(stopwatch.Elapsed.TotalSeconds); } }}First result 848 seconds for 12 1,5GB files with 15% compress ratio. It is better than the first version, but I check my resource monitor and discovered that CPU is using only one core with 100% utilization. In the other hand, disk utilization has some space to improve. In such case, the simple improvement is to introduce threading in the code.Make it parallel.Converting the for loop into Parallel.ForEach should bring a time reduction. The only change I have to make is WriteLog function with lock statement. Because writing is a rare operation in my case I won‚Äôt optimize this part. The final code is:using System;using System.Diagnostics;using System.IO;using System.Threading.Tasks;using SharpCompress.Readers;namespace FastIISExtractor{ class Program { private static readonly object FileLock = new object(); private static void WriteLog(string fileName, string line) { lock (FileLock) { File.AppendAllText(fileName, line + Environment.NewLine); } } static int Main(string[] args) { var stopwatch = new Stopwatch(); stopwatch.Start(); const string maxTime = \"2017-02-02 10:00\"; const string searchValue = \"phrase to find\"; const string searchDirectory = @\"d:\\somewhere\\\"; const string searchPattern = \"*.rar\"; const string resultFile = @\"d:\\somewhere2\\out.log\"; if (File.Exists(resultFile)) { Console.WriteLine($\"File {resultFile} already exists. Quit\"); return -1; } Parallel.ForEach(Directory.EnumerateFiles(searchDirectory, searchPattern), file =&gt; { using (var stream = File.OpenRead(file)) { var reader = ReaderFactory.Open(stream); while (reader.MoveToNextEntry()) { if (reader.Entry.IsDirectory) continue; using (var streamReader = new StreamReader(reader.OpenEntryStream())) { string line; while ((line = streamReader.ReadLine()) != null) { if (line.Contains(maxTime)) { Console.WriteLine($\"------ To far stop processing {reader.Entry.Key} in {file}-----\"); break; } if (!line.Contains(searchValue)) continue; //sw.WriteLine(line); WriteLog(resultFile, line); Console.WriteLine($\"Found line {reader.Entry.Key} in {file}: {line}\"); } } } } }); stopwatch.Stop(); Console.WriteLine(stopwatch.Elapsed.TotalSeconds); return 0; } }}The result is 421 seconds. Better, much better. The best thing is that I spent 15 minutes on coding and now have a powerful parallel grep tool. I already run it a few times and have some fun. Do you think I should make a tool from it?" }, { "title": "Ubuntu .NET developer - possibile or not? - part 1 - Hyper-V", "url": "/ubuntu-net-developer-possibile-or-not-part-1-hyper-v/", "categories": "", "tags": "dotnet, ubuntu-net", "date": "2017-02-03 08:15:04 +0000", "snippet": "A long time ago when I was a student I was using Linux side by side with Windows. Dual boot was standard, and I cursed myself many times, just because I launched a wrong operating system. My professional career caused that I forgot about Linux, except SSH console from putty or Git. To be fair, I was sure that Linux won‚Äôt come back on my PC. But one day some magic happened in Microsoft world. NET Core, bash on Windows, Visual Studio Code, SQL Server on Linux. Is it time to come back?Before we startFirst of all, you don‚Äôt have to install Ubuntu or any other Linux distribution on a physical machine. Virtual one should be enough to check if everything is working fine. I will use Ubuntu because it is the most popular distribution of Linux. I will use Hyper-V because virtual machine for an experiment is better - one click, and it is gone. If you prefer to use VirtualBox or VMWare feel free. For this experiment you can use the technology, which you like most.PreparationFirst of all, we need Ubuntu. You can download it from the official website. When an ISO is downloading, we can enable Hyper-V. There is a good guide from Microsoft: Install Hyper-V on Windows 10. A restart will be needed, so be careful if you are downloading the file in the background.Linux installationIt is a piece of cake. Open Hyper-V, go through the wizard. On the third screen, you can select Generation 2. But this is optional. Generation 1 will be good enough. I suggest you assign more than 1024MB RAM but don‚Äôt worry you can change this value later.On the last step, please select downloaded ISO.Now important step before you start the new shiny Virtual Machine. On Hyper-V settings, you have to uncheck Enable Secure Boot in the Security tab:Without above your machine won‚Äôt boot from downloaded ISO.Ubuntu installation is a classic wizard process. Next, next, next and you are done. After a while, we will see beautiful shiny Ubuntu desktop. Hurray!Installing VS CodeGo to https://code.visualstudio.com/ and download deb package and run it. I‚Äôm still surprised that I didn‚Äôt install Visual Studio Code from the terminal.Hyper-v and Ubuntu resolutionI don‚Äôt know why Ubuntu on Hyper-V ends with 1152x864 and I cannot change it inside the virtual machine. Previous steps were ok, but I want to code in a full screen mode. To change the resolution, we need some magic tricks. We need to edit /etc/default/grub. If you know how to use vi or nano text editor just runsudo vi /etc/default/grubThe alternative option is to use a graphic text editor like Visual Studio Code. I didn‚Äôt manage to launch the latest version of Code as sudo because it needs additional parameter which just doesn‚Äôt work. We can temporary change permissions to above file with chmod command.Anyway I need to replace one line from GRUB_CMDLINE_LINUX_DEFAULT=\"quiet splash\"to GRUB_CMDLINE_LINUX_DEFAULT=\"quiet splash video=hyperv_fb:1920x1080\". After saving the file there are two commands to launch in the terminal:sudo update-grubrebootThe machine should launch in the new resolution, so working with it will be a pleasure..NET CoreInstalling .NET Core is the last thing for today. Go to https://www.microsoft.com/net/core#linuxubuntu and choose the appropriate version. In my case, it is Ubuntu 16.04:sudo sh -c 'echo \"deb [arch=amd64] https://apt-mo.trafficmanager.net/repos/dotnet-release/ xenial main\" &gt; /etc/apt/sources.list.d/dotnetdev.list'sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 417A0893sudo apt-get updateAbove only configure package source and now we can install .NET Core, but please check current version on the official website.sudo apt-get install dotnet-dev-1.0.0-preview2.1-003177Next timeIn the next post, I will create, compile and debug a simple app on host (Windows) and guest (Ubuntu) environment. Of course with SQL Server installed.If you don‚Äôt want to miss next post ==follow me using bellow buttons==." }, { "title": "VS 2017 + new csproj = useful or useless?", "url": "/vs2017-plus-new-csproj/", "categories": "", "tags": "visual-studio, dotnet, vs2017", "date": "2017-01-23 19:50:58 +0000", "snippet": "Here we go again. New Visual Studio, new NET Core tools. This post is the second one about new Visual Studio 2017. Last week I wrote about Common code style in Visual Studio 2017.The question for today is simple: is VS2017 with new csproj useful or useless? Let‚Äôs find it out.Simple structureDo you remember project.json? For me, it was a new hope (read more), but Microsoft removed it. Instead, we have a new csproj format, much simpler than ever. It is human readable and can be created without IDE. For example, a csproj file for NET core console app is following:&lt;Project Sdk=\"Microsoft.NET.Sdk\" ToolsVersion=\"15.0\"&gt; &lt;PropertyGroup&gt; &lt;OutputType&gt;Exe&lt;/OutputType&gt; &lt;TargetFramework&gt;netcoreapp1.0&lt;/TargetFramework&gt; &lt;/PropertyGroup&gt; &lt;ItemGroup&gt; &lt;Compile Include=\"**\\*.cs\" /&gt; &lt;EmbeddedResource Include=\"**\\*.resx\" /&gt; &lt;/ItemGroup&gt; &lt;ItemGroup&gt; &lt;PackageReference Include=\"Microsoft.NETCore.App\" Version=\"1.0.1\" /&gt; &lt;/ItemGroup&gt;&lt;/Project&gt;Three groups with a clear responsibility. The first one defines project type and target framework. The second one includes all source code files. The last one is for NuGet dependencies. Still too complicated? We can reduce root element to &lt;Project Sdk=\"Microsoft.NET.Sdk\"&gt;. There are a few advantages of such csproj file structure for me, including: no more changes when we add or remove a file in a folder; no more git conflicts in merge; all files included in subdirectories using ** symbol; no more xmlns attribute.I was really worried when Microsoft decided to replace project.json with a csproj file, but above looks simple, clean and easy to understand.Auto reloadDo you remember below popup? There is a chance that you seen it today just because some git operations cause it.In VS 2017 with NET core projects it is gone forever. The newest Visual Studio auto reloads changes in a file structure and the csproj itself. When we add or remove a file, it is automatically visible inside a Solution View. The same behavior is with the package references. For example, when we change the version of NuGet package, Visual Studio auto-setup it in a second.What about XYZ property from project.json?Old csproj and project.json have a lot more properties to set. Above example is a minimal working file. If you want to add version, multiple frameworks, before and after targets, snk file, test framework, etc - it is possible. In IDE it is a click, in ‚Äúplain text file mode‚Äù you will have to check the documentation. But this is same as before and properties have clean names. Even complicated files are readable for a human.Sum upVisual Studio 2017 with NET core toolset is nice. The csproj format looks better for me even than project.json. I hope that this is a final version of NET core project file. Or to be precise a final major version.Next time about NuGet dependencies in above structure. Few things have changed, especially that NET Core project uses NuGet 4.0. If you don‚Äôt want to miss the next post, follow me using buttons bellow." }, { "title": "Common code style in Visual Studio 2017", "url": "/common-code-style-in-visual-studio-2017/", "categories": "", "tags": "tools, visual-studio, vs2017", "date": "2017-01-16 19:03:57 +0000", "snippet": "You open a file, change one line, press save and the file has changed its structure. But you add only a simple comma, variable or if. How could this happen to me again?IDE vs style vs developersToday we use different IDE and notepads in teams. Moreover, I‚Äôm sure that you have more than one text editor installed. And most of them have syntax helpers and code formatters.Tool to the rescueThere is a nice tool that helps developers configure and enforce formatting and code style. It is EditorConfig. Using a plain file .editorconfig we can enforce for example: spaces or tab indentation; charset; end of line char; and more.Can I use it?Editor Config has a really wide support. It will work without plugin in JetBrains tools (InteliJ, WebStorm, PHPStorm, ‚Ä¶), Github and TortoiseGit. Moreover, there is a plugin for almost every IDE like NetBrains, Eclipse, Emacs, Atom, Sublime, Notepad++, Visual Studio Code and older versions of Visual Studio.Visual Studio 2017In Visual Studio 2017 Microsoft implemented native support for Editor Config. They extended specification and added more features for .NET developers. For example: force or disallow this. before methods and properties in C#; prefer expression-bodied members for properties in C# (public int Age =&gt; _age; instead of public int Age { get { return _age; }}); null propagation in null checking; and more.All options are described on .NET Code Style Settings For EditorConfig. What‚Äôs more? We can specify a severity for every option: none, suggestion, warning, or error.The result in VS 2017 looks like:Really nice, isn‚Äôt it?##Common code format is better, it makes it more readable. But the most important reason is much simpler. Using common formatting and code style has one big advantage. There aren‚Äôt commits with formatting only. And all changes can be small.The last thing. Do you want to code with the same code format as Microsoft developers? Yes we can! The Roslyn Editor Config file is available on Github, so we can use it in our projects." }, { "title": "Do I need a lawyer to develop software?", "url": "/do-i-need-a-lawyer-to-develop-software/", "categories": "", "tags": "unproductive", "date": "2017-01-12 07:05:00 +0000", "snippet": "Do I need a lawyer to develop software? Sounds like a stupid question. Can I guess your first through? Mine would be: Are you serious? I‚Äôm a software developer, I don‚Äôt need a lawyer.Today, when a lot of code is available on GitHub, our programming is easier. We find a code; we use it and we are happy. But who checks the license?Top open source license typesThe Black Duck compare over 9000 global forges and repositories. The top 5 is following: MIT License - 29% GNU General Public License (GPL) 2.0 - 19% Apache License 2.0 - 15% GNU General Public License (GPL) 3.0 - 8% BSD License 2.0 (3-clause, New or Revised) License - 5%If it isn‚Äôt enough, more can be found in their post: Top Open Source LicensesAre them same?If you don‚Äôt have a lawyer, I suggest you visiting for example choosealicense.com. The most important points are listed and they are easy to compare. But the main page is for open source creators. Not for¬†users of¬†libraries.Fortunately, the [choosealicense.com] web page has two subpages. The short one list with GNU AGPLv3, GPLv3 and LGPLv3, Mozilla Public License 2.0, Apache License 2.0, MIT License and The Unlicense. The long one list has much more. ‚ÄúSo what?‚Äù - you will ask. I found it, I check it everything is ok. But the real life is more complicated.Real lifeA few months ago there was a discussion on the Internet about React library. Everything because Facebook uses BSD license with an additional patent grant. According to Rob Eisenberg: If you are using or considering using React in a project, you might want to consult a lawyer. Because of the patent clause, you are not allowed to do anything that constitutes as competing with Facebook. If you do take legal actions or in other ways challenge Facebook, your license to use React is immediately revoked. Your license is also revoked if you have any legal disputes if you have legal disputes with any other company using React. This is the reason why both Google and Microsoft employees are not allowed to use React.js in their workJust watch his talk below (interesting part from 46:34) or open it in on YouTubeTime has passedThe above internet discussion forced Facebook to publish the Open Source License FAQ and change in their open source licensing. More details in their blog post Updating Our Open Source Patent Grant. And effect looks better. The new patents clause for me is clean enough. But I‚Äôm not a lawyer or English native. So don‚Äôt trust me and read it by yourself.How about adoption?For example, Microsoft offers React UI component library for extending SharePoint. Apple also created something using React. The API Reference is using it. Just check the source Luke to find it out. But above examples aren‚Äôt the core products.The conclusionWe are developers, we want clear statements. But choosing technology stack these days isn‚Äôt obvious. Maybe we don‚Äôt need a lawyer, but for sure we should check the license twice.Half a year laterThat‚Äôs funny but almost nothing has changed.Facebook just published blog post with explaining React‚Äôs license" }, { "title": "Do you want to fire 34 people?", "url": "/do-you-want-to-fire-34-people/", "categories": "", "tags": "unproductive", "date": "2017-01-09 07:05:00 +0000", "snippet": "The day has come. With your team, you finished a project. Few months have passed. Not so big, not so small. Just typical, like few before and few next. The business partners even create a party with free beer and pizza. Maybe even you will receive a bonus. It looks great, isn‚Äôt it?Changes &amp; consequencesA few months have passed. Your team changed. Jack decide to search luck somewhere else. Your boss decides to stop working with Mike because he doesn‚Äôt fit or something. But you know now he is working for XYZ company. Normal life in IT teams.One day you hear strange news. Do you still remember the project, you did a few months ago? This with the great party. The company/department who ordered it announced releases. They are firing people. What happened? A few months ago everything was perfect. They even organize a great party. So why they are firing people?Can you imagine that this can be your fault? You created an application. It is working great. And this is why the company/department can optimize costs. People are¬† redundant, so they can fire them. Your code did it.Is it real?On 5 January the 33rd Square publish the following article: Japanese Insurance Company to Replace Workers with Artificial Intelligence Japanese insurance firm Fukoku Mutual Life Insurance is making 34 employees redundant and replacing them with IBM‚Äôs Watson Explorer artificial intelligence.The artificial intelligence is better and cheaper than 34 workers. The company saves ¬£1.4 million a year. A developers team just fired 34 people.My experienceI had one such project in my life. A long time ago. I was proud when I finished it. I get worst a few months later. I know such optimization is required in companies. I know such apps are created every day. I hope that everybody, who was fired then, has a new better job. But still, when I think about it I feel terrible.Technical progress100 years ago Ford Model T was built almost using hands. You can watch this on YouTube. Today in the car factory, people almost doesn‚Äôt exist. But they write code for modern cars. A lot of code. For example, the new Ford F-150 has 150 million lines of code in a truck (read more).We are developers. We write code. We are changing word every day. We make it better. It is worth even when someone loses his job. The only problem is that it isn‚Äôt nice." }, { "title": "Time tracking - seriously?", "url": "/time-tracking-seriously/", "categories": "", "tags": "unproductive", "date": "2017-01-06 09:52:25 +0000", "snippet": "A few days ago I read a beautiful comment: The man without time tracking doesn‚Äôt know what he did. What is the first thing to do after holidays? Fill time tracking.Did you have same feelings? Me too. Many times, probably too many.Issues and timeThe agile development should be simple. Someone, for example, a product owner or the boss decides what has the top priority. In Kanban, it is very simple. The team put an issue on the top and do it as the next one. In Scrum situation is a bit more complicated. We need to wait for the next sprint or to the next-next sprint. Sometimes even the next-next-‚Ä¶-next sprint. Because some issues aren‚Äôt as much important after few weeks/days/hours. In theory, we should delete such issues, but real life isn‚Äôt so simple.Anyway, we start doing the issue. Of course, we also start logging time. Good guys log time as soon as possible (ASAP). For example in the end of the day. Bad guys log it using different ASAP- ‚ÄúAround September August Probably‚Äù.The sum must equalThe time has passed. And suddenly once a year/quarter/month the team leader receive a notification. Probably you cannot believe this, but a total number of hours doesn‚Äôt equal to proper sum. A poor team leader with his own team search where is an error or who forget to log some time.Two hours later everything is correct. A total sum equal to the proper total. Hallelujah!What for?Now hold down for a while. What happens next? Is anybody looking at this data and analyze them?If the answer is yes, you are a happy man. Time spend on time tracking is useful and important. Because creating conclusions, based on it can be really useful. Especially in future, for making estimations about next tasks/projects/issues.The problem is when the answer is no. Then the whole time tracking is just a waste of time.Measure or not?Ideal programmers, life is just 4 steps: Eat, Sleep, Code, Repeat.Everything else is distractive. But we are optimizers. We are lazy. We want to automate stuff. How much you spend only on the full code compilation. Full time from CTRL+S in IDE to view effects in a browser. Is it 2 hours a day? Or maybe only 10 minutes? What about deployment process. How much it takes in a month in total? Few hours or maybe few days?In most time tracking tools, we cannot answer such questions. We just measure for measurement. Of course, you don‚Äôt have to measure everything every day. Such measurement will cost too much. But from time to time it can be really helpful.Beacuse you should measure smart, to reduce boring tasks. To have more fun. Just use following 4 steps: Do, Measure, Improve, Repeat." }, { "title": "Piotr Stapp in 2016", "url": "/piotr-stapp-in-2016/", "categories": "", "tags": "", "date": "2017-01-03 20:57:42 +0000", "snippet": "I have been busy in 2016. I did a lot for a community, my job and myself also. Moreover, I didn‚Äôt realize what I achieved until today.SpeakingLast year I was speaking on: 1 grand finale of Daj siƒô poznaƒá 5 user groups including SysOps/DevOps Polska, Dev@ZG, Dev@LDZ and Warsaw Web Performance Group 6 conferences including Get.NET, IT w bankowo≈õci, IT career summit, BBQ4IT, High Load Strategy LT and DotNetConfPLThat is in average one talk per month. Awesome!Writing 32 posts in 52 weeks on my blog. More stats: TOP in 2016 - some stats. I should write more often. MVB award and 4 articles on DZone. For more check my DZone profile ~850 tweets on Twitter with ~200K impressions and new Facebook account :)StackOverflowAmazing stats. I‚Äôm shocked. My post reached ~1.2m people. Can you imagine 1.2 million people?The rest is also impressive. I‚Äôm in: top 3% overall top 5% in asp.net, asp.net-mvc, C#, jquery and twitter-bootstrap tags top 10% in .net, asp.net-mvc4, javascript and powershell tagsFor more check out my StackOverflow profileDevWarsztatyWith Jakub Gutkowski and Pawe≈Ç ≈Åukasik we organized 10 meetings last year. 10 amazing, hands-on, all-day workshops in Warsaw and Wroc≈Çaw.Want to be a member? Sign up now to our meetup.FrenchI‚Äôm learning French on DuoLingo. I decide to learn French to be more romantic. For example I can say: La chaussette rougeIf you don‚Äôt know what it is, just google thisI had 450+ days streak, but in the 29th of December, I forgot to do my lesson. Moreover, I forgot to buy a ‚Äúcode freeze‚Äù. Gosh.So here I go again. Wish me luck.My jobLast year I learned that my job is top secret banking stuff 8 hours a day, 5 days a week. Next year will bring something new and more exciting.ManagementBeing the manager of 9 people team is exhausting. Moreover when you want to code from time to time. But I did it with a few successes. I‚Äôm really proud of myself. But to be fair, I still think that I‚Äôm better in coding than management. Or to be more precise I much more like development.CodeLast year I created some code in C#, JavaScript, TypeScript (new stuff), CSS+LESS (fight versus code), PowerShell, Java (stream in Java 8 are pure fun), bash and even in PHP on devwarsztaty. That is an awesome list. And still, I‚Äôm web guy with DevOps skills. Funny mix :)FunIn the last day of 2016, I learned how to ice skating backward. Can you believe it?AwesomeLast year was busy for me. I have worked a lot for a community, my job and myself. Moreover, I didn‚Äôt realize what I did until today.I‚Äôm really proud of myself. I still cannot believe that I did so many things. It is awesome!" }, { "title": "TOP in 2016 - some stats", "url": "/top-in-2016-some-stats/", "categories": "", "tags": "", "date": "2017-01-02 09:56:33 +0000", "snippet": "Today some stats about my blog in 2016.Audience overviewJust three numbers: 34380 users 43856 sessions 58357 pageviewsI‚Äôm really amazed.Posts - Top 5 Clean code - regular expressions - 3.6% - Top post in January and February. First time I had more than 200 users in one day. How to enhance debugging in Visual Studio with just one simple step - 4.82% - I‚Äôm suprized that this one is so high. Project.json made my life easier and it is not a joke - 5.59% - My long journey. I‚Äôm happy that so many users like it. Docker: Windows containers on Windows host - step by step - 11.28% - New stuff :) Fighting with corporate proxy and modern tools like git, npm, bower (SSL problems) - 13.52% - Gosh so many users have same problems. That is really sad :(More dataSummaryI‚Äôm proud of myself. Moreover my English gets better." }, { "title": "First never follows", "url": "/first-never-follows/", "categories": "", "tags": "software-engineering", "date": "2016-11-21 18:44:15 +0000", "snippet": "In last few years, there is a fashion, especially in IT companies. A lot of managers create a catchphrase like: We are the Twitter of telecommunicationor We want to be the Uber of bankingThey are really easy to create. Just take a company from Silicon Valley on the left and put company core business on the right.In 90% cases company from Silicon Valley will be Google or Facebook.Copy and pasteAs a catchphrase above sentence is really good. Moreover in a lot of conferences you can meet stuff from Github, Facebook, Uber, Google and more. They talk about how they are working. Describing their methodologies, languages, and tools.As you know success brings success, so we decide to copy their way of doing software. But only the important part, because Go/Ruby/Java/C#/Linux/[anything] won‚Äôt work in our environment. For example one repository will help for sure.Monolith repositoryThe monolith repository is a great example of a tool that we can easy adapt to our company. Just watch a short part of ‚ÄúWhy Google Stores Billions of Lines of Code in a Single Repository‚Äù from 10:44 below or open direct link: https://youtu.be/W71BTkUbdqE?t=10m43sGreat idea, isn‚Äôt it? With a simple trick, we can be like Google. There is always a ‚Äúbut‚Äù. Moreover, let‚Äôs check out the numbers:&quot;Google scale&quot; never ceases to wow me. 30k devs, 800k builds a day, 150m test cases run each day. More from @mmeckf at #FS16sf pic.twitter.com/YFXEixpXdK&mdash; Richard Seroter (@rseroter) 17 November 2016Impressive!Healthy checkBefore we start migrating, just a small sanity check with others. First of all Facebook. The latest article about source control in Facebook, which I found, is quite old. From January 2014. Great stuff about using Mercurial at scale - Scaling Mercurial at Facebook. Yes Mercurial, not Git. You are probably asking yourself, why are they using Mercurial, not Git? Everybody says that Git is great. The second company from the top isn‚Äôt using Git. Facebook uses Mercurial, Google uses a closed-source Piper. Let‚Äôs check out Uber. At least Uber is using Git. So how many repositories do they have? In the May of 2016, they had more than 8000. Eight thousand is not a mistake, exactly it was 8263. Moreover, they had 7005 one month earlier. For more details, watch the following video from 16:02 or open a direct link https://youtu.be/kb-m2fasdDY?t=16m2s:Amazing!ConclusionIs one monolith repository good or bad? I don‚Äôt know. Probably you too. The best answer is as always: it depends. But the one repository pattern is just an example. The biggest problem is the ‚Äúcopy-paste from Silicon Valley‚Äù pattern. This method isn‚Äôt a simple way to fix anything. They made some decisions to solve their problems. We should do exactly the same. Solve problems instead of applying tools. Different companies have different problems and need different solutions.In the end, to be the best in your core business, remember one rule: first never follows." }, { "title": "NPM versus Yarn - the epic fight for speed in Continuous Integration", "url": "/npm-versus-yarn-the-epic-fight-for-speed-in-continuous-integration/", "categories": "", "tags": "javascript, teamcity", "date": "2016-11-03 18:25:43 +0000", "snippet": "A few days ago, a new tool came from the Facebook team: Yarn package manager. The results published on the official Yarn web page are fantastic. I‚Äôm using local NPM registry, so some of the NPM problems don‚Äôt exist for me. Still, the npm install command is quite slow. How the yarn install command performs? Can it be much better?Let‚Äôs check it out.Why is speed so important in CI?In the Continuous Integration systems, there are two types of build: ‚Äúgit pull‚Äù clean checkoutTo achieve better performance, some of us decide to use the first option. Just because it is faster. But we all know that second is much safer because there won‚Äôt be anything left after the previous build.Anyway, all CI systems have one responsibility. Compile as fast as possible to check if nothing is broken after the last commit.You can use TeamCity, Travis, Jenkins, Bamboo or anything else. But the main goal doesn‚Äôt change.On developers machine, such speed is not so important. Unless you change your projects frequently. But this is rare. The developers need fast compilation time.When I look at my build task a lot of time is consumed by dependency management. The compilation is as fast as your build machine is, but download (and push) of artifacts can take ages. Especially for JavaScript dependencies.Goal defined. Now I‚Äôm ready to describe the environment setup.Environment setupI have the following components in my environment: Artifactory Windows machine no access to the internet (excluding stuff from Artifactory)Artifactory is really important for this tests because using it I have the stable connection to all packages. They are cached there.CertificateUp to yarn@16.0, it was impossible to use a custom certificate. To use it now just type:yarn config set cert.pemAfter that, you will find it in the .yarnrc file in your home directory. On Windows, its location is C:\\Users\\[username]\\.yarnrcWhat will I test?I decide to use below packages.json file. It has only some dev dependencies.{ \"name\": \"yarn_test\", \"version\": \"1.0.0\", \"devDependencies\": { \"aurelia-bundler\": \"^0.4.0\", \"aurelia-tools\": \"^0.2.4\", \"browser-sync\": \"^2.17.0\", \"conventional-changelog\": \"1.1.0\", \"del\": \"^2.2.1\", \"es6-module-loader\": \"^0.17.11\", \"gulp\": \"^3.9.1\", \"gulp-aurelia-template-lint\": \"^0.9.1\", \"gulp-bump\": \"^2.2.0\", \"gulp-changed\": \"^1.3.1\", \"gulp-concat\": \"^2.6.0\", \"gulp-less\": \"^3.1.0\", \"gulp-lesshint\": \"^2.0.0\", \"gulp-notify\": \"^2.2.0\", \"gulp-plumber\": \"^1.1.0\", \"gulp-protractor\": \"3.0.0\", \"gulp-rename\": \"^1.2.2\", \"gulp-shell\": \"^0.5.2\", \"gulp-sourcemaps\": \"^1.6.0\", \"gulp-tslint\": \"^6.0.2\", \"gulp-typescript\": \"^2.13.6\", \"isparta\": \"^4.0.0\", \"jasmine-core\": \"^2.4.1\", \"jspm\": \"^0.16.41\", \"karma\": \"^1.1.2\", \"karma-jasmine\": \"^1.0.2\", \"karma-mocha-reporter\": \"^2.1.0\", \"karma-phantomjs-launcher\": \"^1.0.2\", \"karma-systemjs\": \"^0.14.0\", \"less\": \"^2.7.1\", \"less-plugin-autoprefix\": \"^1.5.1\", \"less-plugin-clean-css\": \"^1.5.1\", \"object.assign\": \"^4.0.4\", \"require-dir\": \"^0.3.0\", \"run-sequence\": \"^1.2.2\", \"systemjs\": \"0.19.35\", \"tslint\": \"^3.13.0\", \"typescript\": \"^2.0.0\", \"typings\": \"^1.3.2\", \"vinyl-paths\": \"^2.1.0\", \"yargs\": \"^4.8.1\" }}The methodologyI decided to test npm install including following options: delete npm cache before restore or not delete node_modules directory or not add -cache-min=9999 in command or notThe last option forces NPM command to use the local cache as much as possible. Some time ago there was --no-registry option, but it isn‚Äôt working now.For yarn install I decided to use analogous options: delete yarn cache before restore or not delete node_modules directory or not delete yarn.lock file or notThe yarn.lock file stores all dependencies links. The best definition is on the official website: In order to get consistent installs across machines, Yarn needs more information than the dependencies you configure in your package.json. The Yarn needs to store exactly which versions of each dependency were installed.Above allows the Yarn to download modules without checking dependencies. It knows it up front.The resultsThe results for NPM are: \ttool \tdelete cache \tremove node_modules \tmaximizeCache(cachemin=9999) \ttime (ms) \tNPM \tYes \tYes \tNo \t178846 \tNPM \tNo \tYes \tYes \t127843 \tNPM \tNo \tYes \tNo \t113699 \tNPM \tNo \tNo \tNo \t13576 \tNPM \tNo \tNo \tYes \t13472 And for the Yarn: tool delete cache remove node_modules maximizeCache (lock file exists) time Yarn Yes Yes No 68145 Yarn Yes No Yes 67609 Yarn Yes Yes Yes 60631 Yarn No Yes Yes 59247 Yarn Yes No No 29712 Yarn No No Yes 560 What about bower/jspm/‚Ä¶?Bower is more front-end focused than NPM. First versions of Yarn support it, but with some problems. Unfortunately, they decided to remove Bower support: Bower support isn‚Äôt something we document or support very well. We should just get rid of it since it has a lot of issues. Bower is on it‚Äôs way out and we shouldn‚Äôt be supporting it. If you want to continue to use Bower then just use it‚Äôs CLI. We initially had support for it to support Polymer but since they‚Äôre migrating entirely to npm there‚Äôs not much use.The rest of the tools, like jspm, aren‚Äôt supported as well. They decided to concentrate on NPM only. In my opinion, it is a good decision especially that latest Tech Radar set the trial status for ‚ÄúNPM for all the things‚Äù (read more). Probably one package manager for JavaScript is enough. Especially if it can be fast.To sum upSpeed in the Continuous Integration is important. We want to know as fast as possible that everything is OK. If the tool slows us, we have a problem. NPM is exactly such problem. It slows our builds. But now we have an alternative - Yarn.As we can see Yarn is much faster than NPM. Moreover, it can produce repeatable build with yarn.lock file. The only downside is that we will have to migrate all our dependencies to NPM. No more Bower, JSPM, volo, ringojs, etc.One more advantage. On the developer‚Äôs machine, Yarn will work faster too.In the end, there can be only one conclusion: The king is dead, long live the king!" }, { "title": "Ghost blog with SSL on azure - the correct way", "url": "/ghost-blog-with-ssl-on-azure-the-correct-way/", "categories": "", "tags": "tips, azure", "date": "2016-10-30 16:38:59 +0000", "snippet": "My blog is hosted on Azure Web Pages. A few months ago I decided to use SSL on it because I can get a free SSL certificate from ‚ÄúLet‚Äôs Encrypt‚Äù website.Change URL in configSimple change URL in config.js generates the following error:Too many redirects Why?As you probably know, Ghost is a node.js app. Azure IIS, in this case, is a reverse proxy server. It is working on localhost with http. Moreover, Ghost has the build-in check for SSL. But it doesn‚Äôt know anything about the reverse proxy. And it checks http://127.0.0.1:[process.env.PORT] instead of https://stapp.space in my case.Bad fixWhen first time I had above error. I found a simple fix: rewrite all URL in the web.config from http to https. And it was working like a charm. The rule was more or less like below: &lt;rule name=\"ForceSSL\" stopProcessing=\"true\"&gt; &lt;match url=\"(.*)\" /&gt; &lt;conditions&gt; &lt;add input=\"{HTTPS}\" pattern=\"^OFF$\" /&gt; &lt;/conditions&gt; &lt;action type=\"Redirect\" url=\"https://{HTTP_HOST}/{R:1}\" redirectType=\"Permanent\" /&gt; &lt;/rule&gt;But this ends with bellow problems: my web page generates all URLs starting with http. So almost every user click was redirected without a reason. on RRS feed I had the same problem. Firefox shows warnings on my page only because of aboveSearchingOn Google search, I found Tom SSL blog with an article about this problem. But he suggests changing a function inside Ghost. This function is function isSSLrequired(isAdmin).After applying this solution, on every Ghost upgrade, I will have to remember about above fix. No way.Then, I found the second solution. But it applies only to NGIX and Apache: https://github.com/TryGhost/Ghost/issues/2796And this gave me a clue.Good fixTo add the X-Forwarded-Proto header in IIS configuration, I need one line in iisnode.yml:enableXFF: trueThe iisnode.yml file is side by side with web.config.Now put correct URL in your config.js. Restart web page and it is working like a charm." }, { "title": "Share post on Facebook gives 404", "url": "/share-post-on-facebook-gives-404/", "categories": "", "tags": "tips", "date": "2016-10-29 16:07:34 +0000", "snippet": "A few days ago, I tried to share my own post on Facebook. But instead of post miniature, I ended up with 404 - Page not found. Exactly like bellow:Quick fixA quick fix was simple: I added a query string into URL. And magically Facebook read my post without problems.But a few days later my friend wants to share my post on his timeline. And he gets the exactly same result as me before.Facebook developers toolsThe real solution is simple, but not well known. Moreover is as easy as the quick fix. Go to https://developers.facebook.com/tools/debug/ Put your not working URL and press the ‚ÄúDebug‚Äù button. You will see 404 in Response Code like below: Press ‚ÄúScrape again‚Äù (marked on above image) and you are done. Now you should see 200 in Response Code: Done!" }, { "title": "Project.json made my life easier and it is not a joke", "url": "/project-json-made-my-life-easier-and-it-is-not-a-joke/", "categories": "", "tags": "visual-studio", "date": "2016-10-25 13:10:36 +0000", "snippet": "Project.json is a celeb in .NET world. But its status is rather ‚Äúfamous for being famous‚Äù, instead of usefulness. Probably you heard some time ago that it will replace csproj and a few months later that it won‚Äôt. What is it still doing here? Only one word: dependencies.TL;DR;The history of the package management in .NET world is long and rather turbulent. Getting always the latest version for many years was just impossible. If know NuGet history and structure, you can skip first few paragraphs up to Episode IV: A new hope. The solution description starts there. I hope you will find it useful.Where are we from?I don‚Äôt know anybody who hasn‚Äôt copied/included/committed external libraries into a project. Even today JavaScript libraries are frequently included in web projects. Well, in the beginning of this century it was a standard.One of the oldest package management system - Maven - was born on Mon, 27 Aug 2001 16:35:05 GMT. .NET didn‚Äôt even exist then. The first version was released on 2002-02-13. That time, the most advanced dependency management on Windows platforms used shared folders. The solution above helps the developers sharing their work in the big teams.What about NuGet?NuPack was born on August 2010 as an open-source package manager for the Microsoft development platform. Did you notice the open source? It was hosted on CodePlex.But what is NuPack? Now it is NuGet. The name was changed on 21 October 2010. The funny thing is that the most people pronounce it like ‚Äúnugget‚Äù. But according to the authors it should be pronounced slightly different: NuGet (pronounced ‚ÄúNew Get‚Äù and not ‚ÄúNugget‚Äù and not ‚ÄúNoojay‚Äù for you hoity-toity)As you probably know NuGet is a simple zip file with a defined structure. You can use for example 7zip to see the structure of any nupkg file. Below structure of microsoft.aspnet.mvc.6.0.0-rc1-final.nupkglib/lib/dotnet5.4/lib/dotnet5.4/Microsoft.AspNet.Mvc.dlllib/dotnet5.4/Microsoft.AspNet.Mvc.xmllib/net451/lib/net451/Microsoft.AspNet.Mvc.dlllib/net451/Microsoft.AspNet.Mvc.xmlMicrosoft.AspNet.Mvc.nuspec[Content_Types].xml_rels/_rels/.relsIt is simple and clean. Even nuspec file is human readable. Just take a look:&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;&lt;package xmlns=\"http://schemas.microsoft.com/packaging/2012/06/nuspec.xsd\"&gt; &lt;metadata&gt; &lt;id&gt;Microsoft.AspNet.Mvc&lt;/id&gt; &lt;version&gt;6.0.0-rc1-final&lt;/version&gt; &lt;requireLicenseAcceptance&gt;true&lt;/requireLicenseAcceptance&gt; &lt;authors&gt;Microsoft&lt;/authors&gt; &lt;owners&gt;Microsoft&lt;/owners&gt; &lt;description&gt;ASP.NET MVC is a web framework that gives you a powerful, patterns-based way to build dynamic websites and Web APIs. ASP.NET MVC enables a clean separation of concerns and gives you full control over markup.&lt;/description&gt; &lt;dependencies&gt; &lt;group targetFramework=\".NETFramework4.5.1\"&gt; &lt;dependency id=\"Microsoft.AspNet.Mvc.ApiExplorer\" version=\"6.0.0-rc1-final\" /&gt;&lt;!--a few more lines like above--&gt; &lt;/group&gt; &lt;group targetFramework=\".NETPlatform5.4\"&gt; &lt;dependency id=\"Microsoft.AspNet.Mvc.ApiExplorer\" version=\"6.0.0-rc1-final\" /&gt;&lt;!--a few more lines like above--&gt; &lt;/group&gt; &lt;/dependencies&gt; &lt;frameworkAssemblies&gt; &lt;frameworkAssembly assemblyName=\"mscorlib\" targetFramework=\".NETFramework4.5.1\" /&gt;&lt;!--a few more lines like above--&gt; &lt;/frameworkAssemblies&gt; &lt;licenseUrl&gt;http://www.microsoft.com/web/webpi/eula/net_library_eula_enu.htm&lt;/licenseUrl&gt; &lt;iconUrl&gt;http://go.microsoft.com/fwlink/?LinkID=288859&lt;/iconUrl&gt; &lt;copyright&gt;Copyright ¬© Microsoft Corporation&lt;/copyright&gt; &lt;projectUrl&gt;http://www.asp.net/&lt;/projectUrl&gt; &lt;/metadata&gt;&lt;/package&gt;Even the basic package definition inside a project is easy to understand. Simple XML file called packages.config. More or less like below:&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;&lt;packages&gt; &lt;package id=\"Microsoft.AspNet.Mvc\" version=\"5.2.3\" targetFramework=\"net452\" /&gt; &lt;!--a few more lines like above--&gt;&lt;/packages&gt;Nice, isn‚Äôt it?But my feelings are ‚Ä¶Moreover, there are a lot of developers like you. With exactly the same feeling. For example Gutek once upon a time posted on twitter:Where is the problem?Bear in mind that NuGet is simple, but it integrates with csproj file in ##@$@$ way. Just take a look inside this simple csproj: &lt;ItemGroup&gt;&lt;Reference Include=\"System.Web.Mvc, Version=5.2.3.0, Culture=neutral, PublicKeyToken=31bf3856ad364e35, processorArchitecture=MSIL\"&gt; &lt;Private&gt;True&lt;/Private&gt; &lt;HintPath&gt;..\\packages\\Microsoft.AspNet.Mvc.5.2.3\\lib\\net45\\System.Web.Mvc.dll&lt;/HintPath&gt; &lt;/Reference&gt;Can you see this version string inside a path? That‚Äôs the main reason of our frustration. Just 6 chars: .5.2.3.Every package upgrade always changes these two files: csproj and packages.config. Moreover, there wasn‚Äôt a way to have always the LATEST version. We need to upgrade the packages every time. And this made an irritating GIT/TFS/SVN conflicts.How to run NuLatestGet?There aren‚Äôt many ways. One is to run it on a Continuous Integration tool. For example, TeamCity has built-in task just for this operation. You can read about it on their blog.But still, we need to commit such changes.In my company, we have been using ripple from the fubu team for a few years. But this project died. Today the official web page even doesn‚Äôt exist. Only the GitHub repository linked above remained. Sad.Another solution is to switch to paket. It was created for the F# guys, but it works nicely with C# projects, too. Good stuff.Can you any disadvantage of above solution? I know only one. Developers have to use a console to restore project dependencies.Episode IV: A new hopeWhen I read about project.json for the first time, I was excited. I crossed my fingers and even took a part in a discussion about versions string format.Then project.json died, because of the tooling or something. But some part of it is still alive. Exactly NuGet part. Almost one year ago I even created a post about it: Floating versions in Nuget.But it wasn‚Äôt useful at that time. This has changed a few months ago. Exactly on 27 June 2016, when Visual Studio Update 3 was released. At first, I didn‚Äôt notice this fact, but it is working. After two months of using it, I‚Äôm just happy. It isn‚Äôt perfect, but it is good enough.There can be only oneThe main difference is really simple. Instead of two entries about package dependency we have only one in project.json. In csproj, there is only an include of project.json file. All dependencies are in project.json. Much simpler, isn‚Äôt it?How to start?It‚Äôs simple. Just start a new or open an existing solution in Visual Studio. Add at least one NuGet package, but probably this was done by project template. Now check if you already have the Update 3. It has a few months already, but after all, it is better to be safe than sorry. To check it just open Help-&gt;About Microsoft Visual Studio.Close Visual Studio and open csproj in your favorite text editor. Find all dependencies with HintPath set to packages folder and remove them. Like below:&lt;Reference Include=\"System.Web.Mvc, Version=5.2.3.0, Culture=neutral, PublicKeyToken=31bf3856ad364e35, processorArchitecture=MSIL\"&gt; &lt;Private&gt;True&lt;/Private&gt; &lt;HintPath&gt;..\\packages\\Microsoft.AspNet.Mvc.5.2.3\\lib\\net45\\System.Web.Mvc.dll&lt;/HintPath&gt; &lt;/Reference&gt;Now step number two. Find below line:&lt;Content Include=\"packages.config\" /&gt;And replace packages.config with project.json. Create such file with below content. We need to specify the target frameworks. In my case .NET 4.6.1:{ \"frameworks\": { \"net461\": { } }, \"runtimes\": { \"win\": {} }}Now we can add the dependencies manually by using one of the methods below: from packages.config extract strings like: \"Microsoft.AspNet.Mvc\": \"5.2.3\" - it‚Äôs a few find and replace commands install all the needed packages from the Package Manager ConsoleIf you would like to use ==an automatic script==, just subscribe to my newsletter on the right. I will release it in a few days. Also, above tutorial will be released as GitHub repository.Final effectIt‚Äôs simply working. Just press F5 in Visual Studio. In my case, after nuget restore, I can see following in the Solution Explorer:ProblemsThe latest package still has a small feature. You can use it just with * as a version number, but in Visual Studio it will disappear.The second problem is described in the following article: .NET Core Tooling in Visual Studio ‚Äú15‚Äù . I don‚Äôt know how project.json will finish. But I hope the version strings will be working inside csproj too.Sum upA very short sum up: Correct NuGet pronunciation is ‚ÄúNew Get‚Äù, not ‚Äúnugget‚Äù or ‚Ä¶ From Visual Studio Update 3, we can get latest NuGet package with project.json It integrates well with Visual Studio and MsBuild. We don‚Äôt need additional step in console. ==Subscribe my newsletter to receive an info about automatic conversion tool==. I will publish it in a week or two." }, { "title": "Don't send THIS email", "url": "/dont-send-this-email/", "categories": "", "tags": "unproductive", "date": "2016-10-05 19:38:51 +0000", "snippet": "Can you imagine a day without an email? To be exactly without XX emails. Or even XXX emails if something is wrong. Some teams moved from email to slack, hipchat or [put your stuff here].But do we need them?HistoryEmail history is really interesting. Just take a look at Wikipedia: Electronic mail is a method of exchanging digital messages between computer users; Email first entered substantial use in the 1960s and by the 1970s had taken the form now recognised as email. Email operates across computer networks, which in the 2010s is primarily the Internet. [‚Ä¶] Some early email systems required the author and the recipient to both be online at the same time, in common with instant messaging.Wow! We are using email for more than 40 years and we still don‚Äôt know how to use it properly.Day with emailsWhen I look at my day, I check email too often. Probably you too. Why? Because of the notifications. To be precise email is totally asynchronous stuff. Same with instant messaging. But almost everyone expects that we answer ASAP. With Slack is even worst:Make it even worseA lot of us is getting emails from automatic systems. For example issue tracker, CI server or intranet information. Lots of unread emails.Why unread? Because we create email filters to delete such useless emails.So why are we sending them? Why do we subscribe to notifications that we don‚Äôt even read?Here you can say: I read all email notifications. They are important. But do you always take an action after you get a notification? Do you think about them or just read and delete?ImagineImagine that email does not exist. Instead, you have to print a message, put it in the envelope and address it manually. If you want to send the same message to two people, you have to print it twice and address it twice.How many messages would you send then? How many would you receive?" }, { "title": "Did you PLAN this?", "url": "/did-you-plan-this/", "categories": "", "tags": "", "date": "2016-10-03 18:05:31 +0000", "snippet": "In computer industry planning is important. Well, everybody says that planning is important, I know of only two types of planning functionalities: ASAP - As Soon As Possible ASAP - After September, April PossiblyTo Estimate or Not to Estimate?Estimation is really difficult. Moreover, as we know, it is really hard to estimate correctly. Maybe it‚Äôs because an IT job is more of artist stuff rather than an automated factory. This year on DevDay, I heard a really interesting talk about making estimations in IT.But still, it is good to predict costs, not the time spent on the current issue.Scrum AloneScrum, the most popular methodology, gives us Sprint. In my opinion, the biggest advantage of a Sprint is shortness. Why shortness? Because if we fail we can cancel or even delete our work, and in the next Sprint do something better. The funny fact is that developers usually do not remove their code; they comment it out.Scrum Plus Long-Term PlanningOn of the idea of scrum is a possibility to plan work for a long time. When we take repeatable tasks, and the same amount of people (or Story Points), we should be able to plan our work to the end of the universe. Unfortunately, IT stuff is usually not repeatable. Moreover, it is really difficult to explain why changing the position of Button A takes three hours, but making the same change to Button B takes three days.I‚Äôm sure that every developer has such situation is his or her life. And explanation usually was something like: mmmm, You know, mmmm, [mumbling], because mmmm [mumbling]The only serious explanation I‚Äôve heard in my life was: We are using JSFIf you don‚Äôt know what JSF is, don‚Äôt even bother checking it out. This technology without proper use is dangerous.Planning or ‚Ä¶ ?To be precise, I don‚Äôt know. But I suspect that business guys need to share our man-days. There are a lot of them with plenty of ideas. And they need to know when their ideas have a chance to be implemented and installed on production.But as I wrote before, it is always ASAP stuff. So, maybe instead of planning time, it‚Äôs better to just order global backlog? Real ASAP stuff will be put on the top and the rest on the bottom. Such change requires a lot of trust between coders and white collars. And this is really difficult to achieve." }, { "title": "1 unbelievable trick to create an easy to consume API in .NET", "url": "/1-simple-trick-to-create-a-good-api-in-net/", "categories": "", "tags": "code, visual-studio", "date": "2016-09-26 20:14:24 +0000", "snippet": "Creating a service for unknown technology is usually painful. Sorry, ‚Äúservice‚Äù is an unpopular world, nowadays we call it an API. For a few years, everybody says: let‚Äôs create some REST service.Cool, flexible and uneasy to consume. Exactly uneasy - it is not a mistake, uneasy is a correct word.My storyLast week I was on hackathon in Berlin: Collabothon 2016. In my team we want to create following parts: iOS app .NET service - facade for IOS app Web page Python service external API to useFirst of all, we needed a facade to create a connection between iOS and .NET.First thought: let‚Äôs create REST service. But how to create a client in Swift 3.0?Swagger and Swashbuckle to the rescue Swagger is a simple yet powerful representation of your RESTful API. With the largest ecosystem of API tooling on the planet, thousands of developers are supporting Swagger in almost every modern programming language and deployment environment. With a Swagger-enabled API, you get interactive documentation, client SDK generation, and discoverability.Swashbuckle seamlessly adds a Swagger to WebApi projects. It is a NuGet package for .NET projects:)Just install it withInstall-Package SwashbuckleEnter F5 in Visual Studio and enter following URL (be careful with a port number):http://localhost:[YOUR_PORT_HERE]/swagger/ui/index#/You will see something like bellow:Just 2 clicks allow seeing all method specification, with example request and all strong-type fields. Just take a look:What about a client?It is super easy. For actual version download command is:wget http://repo1.maven.org/maven2/io/swagger/swagger-codegen-cli/2.2.1/swagger-codegen-cli-2.2.1.jar -O swagger-codegen-cli.jarTo generate a client, You will need Java and just type:swagger-codegen generate -i http://localhost:[YOUR_PORT_HERE]/swagger/docs/v1 -l [PUT LANGUAGE NAME HERE] -o /tmp/test/Sources and more instructions can be found on https://github.com/swagger-api/swagger-codegen.Swift 3.0But for today swagger-cli doesn‚Äôt support Swift 3.0. It has only a few weeks and has breaking changes with Swift 2.0 :/But there is a cool guy Hexelon on Github: https://github.com/hexelon/swagger-codegen He is preparing a pull request to the main repository. Because building a swagger-cli is super easy, so we used his work already. Just clone his git repository and run:mvn clean packageThe jar will be in swagger-codegen\\modules\\swagger-codegen-cli\\targetWhere can I read more?Just look on http://swagger.io/. A lot of examples and tools are ready to use.Happy coding!!" }, { "title": "How to enhance debugging in Visual Studio with just one simple step", "url": "/debuggerdisplayattribute/", "categories": "", "tags": "c, visual-studio", "date": "2016-09-12 17:33:00 +0000", "snippet": "During the debugging session, I always had problems with complex structures. Preview most interesting properties were irritating, we need to expand a variable.In most cases, I tried to override ToString method. But it isn‚Äôt always possible.Default tooltip is useless. Just look on it:Adding an attributeProbably You won‚Äôt believe me, but a few years ago, in .NET 2.0 times, Microsoft introduced DebuggerDisplayAttribute, which is very useful. Just add it like below:[DebuggerDisplay(\"Range=&lt;{RangeStart}; {RangeEnd})\")]class Range{ public int Start; public int End; //Other stuff}Now, tooltip looks great. Moreover, your class doesn‚Äôt change at all. Just check it out:More awesome stuffAnother great attribute is DebuggerBrowsableAttribute and DebuggerTypeProxyAttribute. Just take a look on MSDN to find out more." }, { "title": "How to split monolith solution - part 3 - compilation tiers", "url": "/how-to-split-monolith-solution-part-3/", "categories": "", "tags": "monolith", "date": "2016-08-01 16:38:24 +0000", "snippet": "In the previous article I used a term ‚Äúcompilation tiers‚Äù, but I didn‚Äôt define it. But before the definition, I have to describe all the possibilities of the split. To make it easier let‚Äôs assume that we split our big solution into two smaller ones: solution one and solution two. Now we have following possibilities:Solution two depends on solution oneImage is worth 1000 words, so here it is:The most interesting part of above picture is the green arrow. It represents dependencies, usually called artifacts. The artifacts can be following: ==packages== like Nuget packages/Maven packages/npm packages/etc. ==plain files== like simple code files/dll files/protobuf schema/etc.We can define above as:Solution one ==produces== some kind of artifacts, that are needed by solution two during or before ==the build==. It is not a run-time dependency. It is a compilation dependency.Solution one depends on solution twoAs in old joke: replace one with two and start from the beginning of the article.Solution one and two doesn‚Äôt have any relationshipAgain 1000 words:How above can happen? I wrote about this in the previous article. In most cases, there is a run-time dependency instead of build dependency. The second option is simpler: projects just don‚Äôt relate to each other at all. For example database migration tool and mobile app. They just don‚Äôt have any relation.DefinitionWe can define compilation tier as: Compilation tier is a subset of all projects without artifact dependencies to each other. The only artifact dependency can be to previous compilation tier.Making above simpler: ==You can compile solutions from one compilation tier in parallel==.Real lifeFind first seam is quite easy. First split probably also. But month or two pass away and You will start to worry if You did a good job. Are you compilation tiers correct?To find out you will have to create a checklist: You don‚Äôt have circular dependencies between tiers. After You introduce package manager it can be easy to create such. Because all packages already exist in artifact repository. You can compile everything from zero. Very similar to above. Most changes are in last tiers. If you do a lot of changes in first tiers, it suggests that every time You will have to compile everything. You didn‚Äôt create too many tiers. I think a number between 5 and 10 is maximum. In most projects, it should be below 5.Next timeNext time I will try to introduce contracts (interfaces) between solutions. But if you wish something else please let me know in comments." }, { "title": "How to split monolith solution - part 2 - find the seams", "url": "/how-to-split-monolith-solution-part-2/", "categories": "", "tags": "monolith, code, visual-studio", "date": "2016-07-05 17:03:53 +0000", "snippet": "In the first part How to split monolith solution - part 1: common myths I wrote about common myths. In this post I will try to find the ‚Äúseams‚Äù to cut, using the Visual Studio and the ReSharper. Let‚Äôs go!Assumptions Big solution :) Time :D Will for changeI will use .NET solution and tools because I know them the best.Tools Visual Studio ReSharper from JetBrains (https://www.jetbrains.com/resharper/) - be careful it is addictiveMy solutionOverview: 67 projects 36 projects on root 2 sub-folders with projects 19 projects with testsIn the Visual Studio, it looks like below:To start working we need to visualize dependencies. And in this part, ReSharper save my day. With ReSharper, you can explore project dependencies in your solution using a visual representation of the solution architecture. At any time, you can open the Architecture View (ReSharper Architecture Show Project Dependency Diagram) and explore project dependencies without compiling anything. If you know other tools, which can do above, please let me know in comments.Anyway. After simple click on ReSharper-&gt;Architecture-&gt;Show Project Dependency Diagram below image showed. (Yellow and green rectangle I added myself using paint):What did I notice? There are 3 projects ==without a reference to anything else==. It is a yellow group on the bottom-right. There are more groups like this, but on the overview, I don‚Äôt see them clearly. My solution folders marked with black background rectangles. The left-hand side is tests. Green rectangle marks a big separated part. There isn‚Äôt anything with reference to this part.The good thing is that I have a folder so I can use collapsed graph in smaller groups. Just click:.In my case it looks like following:After collapse I noticed more: I have only one arrow down - arrow shows dependency between projects I have more project without a reference. I can ease create compilation tiers, there are 7 layers on above picture.The most interesting point is 2. Why do I have unrelated projects? I see following possibilities: IoC; tools without direct dependency like MSBuild tasks, file converter, UI test stuff, DB helpers, etc.; more complex infrastructure - above solution is not the only one in the project; something more? If yes please let me know :)As you think about first 2 groups we can easily move out. In my case, it is 9 projects from 67. It is ~13% of all projects in the solution. If I exclude tests projects it is ~18%. So I just make my solution a bit smaller.Sum upAfter quite simple above steps I noticed that: I have parts that can be easily separated Most my references are one-way. From bottom to up. I can easily create compilation tiers. Folders in big solution are good in the overview.What‚Äôs coming next? What is a compilation tier, because I didn‚Äôt explain it at all and guys in comments ask for it :) - it is already available: How to split monolith solution - part 3 - compilation tiers Dependency management, for compilation tiers. Split or remove circular dependencies in grouped projects" }, { "title": "How to split monolith solution - part 1: common myths", "url": "/how-to-split-monolit-solution-part-1/", "categories": "", "tags": "code, visual-studio", "date": "2016-07-04 11:39:00 +0000", "snippet": "Do you have a big and heavy solution? Do you want to split it? Are you afraid? I will try to help. Be for we start we should answer a few questions.Why should I split?Before I show how to split, I have to deal with common myths: My solution is not so big. It has only 10/20/100/100/X projects/classes insideI cannot recommend how small one solution should be. I can agree that in some cases 10 can be enough small, but I am sure that 50 will be too much. I‚Äôm sure that when You open a solution You will know that it is too big. But we fair with yourself. I cannot split because we don‚Äôt have enough test coverageThat is a very interesting myth. A split shouldn‚Äôt make functional changes, so probably it won‚Äôt break anything in functional part of your system. Anyway, how do you test when you make a new functionality? Automatically, manually or using production users? In this case, do the same. I don‚Äôt see anything to separateIt is untrue, isn‚Äôt it?. For example tools/utils can be separate, DTO, database projects, web projects, ‚Ä¶ I don‚Äôt have any dependency managementMaybe it is time to introduce it. Moreover, think about how do you include external components. Can you build your stuff when the Internet is down? Aren‚Äôt they a dependency management?To introduce dependency management system, try just one library as proof of concept. My business owner/product manager/product owner won‚Äôt accept spending time on thisExplain it to him using following words: it won‚Äôt take long and after this change, we can develop easier. We will waste some time now, but in the near future, we can work faster.What if he doesn‚Äôt understand? Maybe it is time to quit your job ;)What next?In near future I will show how we can use ReSharper and Visual Studio to split our solution to smaller ones.Second part is available. Check it out on How to split monolith solution - part 2 - find cut lines" }, { "title": "HSTS error in Google Chrome", "url": "/hsts-error-in-chrome/", "categories": "", "tags": "web", "date": "2016-06-10 12:03:11 +0000", "snippet": "Sometimes, specially after using Fiddler, my Google Chrome shows me HSTS error like below:localhost normally uses encryption to protect your information. When Google Chrome tried to connect to localhost this time, the website sent back unusual and incorrect credentials. This may happen when an attacker is trying to pretend to be localhost, or a Wi-Fi sign-in screen has interrupted the connection. Your information is still secure because Google Chrome stopped the connection before any data was exchanged.You cannot visit localhost right now because the website uses HSTS. Network errors and attacks are usually temporary, so this page will probably work later.There is an easy way to fix it: Open chrome://net-internals/#hsts Go to Delete domain and delete problematic domain :) You can checkout everything in Query domainCool :)" }, { "title": "sudo !! - run last command in elevated PowerShell prompt", "url": "/run-last-command-in-elevated-powershell/", "categories": "", "tags": "powershell", "date": "2016-06-06 20:09:51 +0000", "snippet": "Last week I have a lot of fun with docker on windows, but I have one small problem. I usually run PowerShell in standard mode, without admin rights. And from time to time I want to restart service, run docker command or ‚Ä¶ - run the previous command in the elevated prompt. In Linux, there is a sudo !!. But in PowerShell, there is no even built-in sudo.A few months ago I described how to run sudo command in PowerShell (more in Run sudo in Windows). But I was still missing sudo !!.Today I found a solution.Using Get-HistoryI need a good name for my function. I decided to name it: f--k. I usually say this word, when I forgot about admin rights.It is really easy to create we just need to get last invoked command using Get-History ‚ÄéCmdlet:function f--k{ $cmd = (Get-History ((Get-History).Count))[0].CommandLine Write-Host \"Running $cmd in $PWD\" sudo powershell -NoExit -Command \"pushd '$PWD'; Write-host 'cmd to run: $cmd'; $cmd\"}Linux styleNow we can modify original sudo function to accept !!:function sudo { if($args[0] -eq '!!') { f--k; } else {\t $file, [string]$arguments = $args;\t $psi = new-object System.Diagnostics.ProcessStartInfo $file;\t $psi.Arguments = $arguments;\t $psi.Verb = \"runas\";\t $psi.WorkingDirectory = get-location;\t [System.Diagnostics.Process]::Start($psi); }}And now I am happy Windows user with Linux styled command." }, { "title": "Docker: Windows containers on Windows host - step by step", "url": "/docker-windows-containers-on-windows-host-step-by-step/", "categories": "", "tags": "windows, docker", "date": "2016-05-29 18:39:03 +0000", "snippet": "For more than one year Windows developers can hear about Docker stuff. But after few sentences how docker is great there is one important sentence: Nowadays we can run only docker containers on Linux. There is no way to run Windows containers.or Docker Engine for Windows Server requires Windows Server 2016. You can try Preview to test it.Few days ago above statements outdated. Are you ready to run it on your Windows 10? PrerequisitesTo have fun with Windows containers you need Windows 10 Insider Preview. I describe how to get it in How to setup Bash and Ubuntu on Windows - step by step guideAfter you have it check out if you already install version 14352 or greater.The second way is to install Windows Server 2016 technical preview - as I remember from version 4 containers are available.Having one of above versions we are ready to deploy a simple Hyper-V container.Small tip before we startMost commands we will run in elevated PowerShell. I usually, forget to run it. But few months ago I described how to run sudo command in Windows. Using sudo I can run sudo powershell and use up arrow to rerun command as admin.Enable Hyper-v and containers featureIf you didn‚Äôt play with Hyper-v on your machine before, this step is most complicated.To enable Hyper-v in modern laptops usually we have to access BIOS. For example in Lenovo, I have to find a special hidden button to enter BIOS. Tricky!When you are ready, start PowerShell as Administrator and run:Enable-WindowsOptionalFeature -Online -FeatureName containers ‚ÄìAllEnable-WindowsOptionalFeature -Online -FeatureName Microsoft-Hyper-V ‚ÄìAllThe first command will install containers feature and the second Hyper-v stuff.Now reboot if needed :)Install dockerContainers needs docker. To install it we can run following commands:$dockerDir = \"$env:ProgramFiles\\docker\\\"New-Item -Type Directory -Path \"$dockerDir\\docker\\\"Invoke-WebRequest https://aka.ms/tp5/b/dockerd -OutFile $dockerDir\\dockerd.exeInvoke-WebRequest https://aka.ms/tp5/b/docker -OutFile $env:ProgramFiles\\docker\\docker.exe[Environment]::SetEnvironmentVariable(\"Path\", $env:Path + \";$dockerDir\\Docker\", [EnvironmentVariableTarget]::Machine)$env:Path = $env:Path + \";$dockerDir\\Docker\"First three lines will download ‚Äúspecial‚Äù docker. Last two commands will add docker path for future and current session use.Now we can install docker as a service (maybe DaaS will be next famous shortcut?) and start it:dockerd --register-serviceStart-Service DockerBase image - NanoServerWe are ready now to install base container image: NanoServer. NanoServer is the ‚Äúsmallest‚Äù version on Windows Server 2016. It doesn‚Äôt have GUI :)Few simple commands and we are ready:Install-PackageProvider ContainerImage -ForceInstall-ContainerImage -Name NanoServerRestart-Service dockerAttention: If you have a problem with running commands run Set-ExecutionPolicy Bypass -scope Process to change execution policy in current PowerShell process.To check if everything is OK, invoke:docker imagesyou should see something like:REPOSITORY TAG IMAGE ID CREATED SIZEnanoserver 10.0.14300.1010 9db95268a387 9 weeks ago 817.1 MBDeployNow we are ready to deploy container. At first, we need to mark our image as latest using:docker tag nanoserver:10.0.14300.1010 nanoserver:latestAnd our final commands are:docker pull microsoft/sample-dotnetdocker run --isolation=hyperv --rm microsoft/sample-dotnetHurrah, we just run our first container. To be sure that we are running Windows not Linux try another one:docker run --isolation=hyperv --rm -it nanoserver powershellWhich will run powershell on NanoServer image. For example use ls and find out what directories you have on Nano ServerWhat‚Äôs next?Next time I will show how to deploy normal IIS and old .NET version (like 3.5 or 4.5) on docker images. Follow me on twitter, facebook or RRS.Stay tuned!" }, { "title": "Make you code editor a little more bloody", "url": "/make-you-code-editor-a-little-more-bloody/", "categories": "", "tags": "tools, visual-studio, code", "date": "2016-05-23 16:45:23 +0000", "snippet": "Your Integrated Development Environment(IDE) is Your main tool. Anything that can make it better is worth of trying :)Nowadays we are using usually more than one IDE. For example Visual Studio for backend and Sublime for the frontend. Or maybe You prefer IntelliJ/WebStorm/PyCharm/PhpStorm with Vim?Also, Xcode with TextMate and Visual Studio Code is an interesting combo.Black or white?A long time ago Michael Jackson sing: But, if you‚Äôre thinkin‚Äô about my baby It don‚Äôt matter if you‚Äôre black or whiteBut if you ask any developer, it really matters if his editor is black or white :)For me, the biggest problem was with colors during switching between different editors.The second problem is key scheme, but this is a different story :)I always set black/dark theme but differences irritate me.Blood is goodSometime ago at least I found solution: Dracula theme for all editors. If you favourite is missing you can add it, because it is open-source. Sweet is it? Just check it out:Now all we can have a bloody editor. Have fun!" }, { "title": "Craft Conf is for crafters (2016)", "url": "/craft-2016/", "categories": "", "tags": "conferences", "date": "2016-05-09 19:31:47 +0000", "snippet": "This year I was attempting Craft Conf for the first time. A lot of people told me that it is the awesome conference. So I was expecting a lot. Especially that ‚Äúcraf is for crafters‚Äù. How was it this year?Place, food &amp; stuffIf You don‚Äôt know craft is in Budapest. I love this city. Unfortunately, I didn‚Äôt have a lot of time to sightseeing, because my flight was canceled. Anyway, the conference was in the Hungarian Railway History Park, with a lot of historical trains. In every break, we can look at a lot of historical trains. That was so cool.The breaks, of course, relates to food. The organizers serve a lot of Hungarian food. It was delicious.The organization was great. Even queues during lunch break weren‚Äôt long. WiFi was working well. What do we need more? Especially that there were more than 1000 attendees.Above is not much important when you came to listen, but congratulations for organizers for their huge job.TopicsTo be precise most sessions were about 3 topics: Docker Microservices Other DevOps stuffI don‚Äôt know why there wasn‚Äôt almost anything on: web development techniques/frameworks/etc. .NET - there was exactly one session by Bart De Smet mobile - maybe two sessionsSessionsI‚Äôm still impressed. To be fair I marked during two days only two sessions on average 3 out of 5. Most of them were gets more than 4 in 5 categories. There were 16 slots. It is really impressive.The best sessions for me in appearance order: Seven Secrets of Maintainable Codebases by Adam Tornhill What I Wish I Knew Before Scaling Uber to 1,000 Services by Matt Ranney Under the hood of the C# programming language by Bart De SmetSummaryIt was awesome. I was exactly like Minions:I hope next year will be a least good as this year." }, { "title": "Windows WiFi card and huge spikes", "url": "/windows-wifi-card-and-huge-spikes/", "categories": "", "tags": "windows", "date": "2016-05-09 18:02:19 +0000", "snippet": "The problemIn my local network, I had huge network spikes on my laptop. Ping to my router were like:Reply from 192.168.1.1: bytes=32 time=7ms TTL=64Reply from 192.168.1.1: bytes=32 time=4ms TTL=64Reply from 192.168.1.1: bytes=32 time=2ms TTL=64Reply from 192.168.1.1: bytes=32 time=146ms TTL=64Reply from 192.168.1.1: bytes=32 time=2249ms TTL=64Reply from 192.168.1.1: bytes=32 time=84ms TTL=64Request timed out.Reply from 192.168.1.1: bytes=32 time=1068ms TTL=64Reply from 192.168.1.1: bytes=32 time=2ms TTL=64Reply from 192.168.1.1: bytes=32 time=3ms TTL=64Reply from 192.168.1.1: bytes=32 time=3ms TTL=64Reply from 192.168.1.1: bytes=32 time=3ms TTL=64Request timed out.Request timed out.Request timed out.Reply from 192.168.1.1: bytes=32 time=3ms TTL=64WTF?I‚Äôm Windows Insider and have fast ring updates, so I was afraid that it is this.My laptop has Intel(R) Dual Band Wireless-AC 8260. I found out on Google that there is a problem with a driver. It can cause spikes like this.Solution Open regedit. Search for AdapterModel In the same section try to find ScanWhenAssociated. If is not there add it as DWORD Set value to 0 Repeat few times until you find all entries RebootResultsNow my ping look like:Reply from 192.168.1.1: bytes=32 time=2ms TTL=64Reply from 192.168.1.1: bytes=32 time=2ms TTL=64Reply from 192.168.1.1: bytes=32 time=2ms TTL=64Reply from 192.168.1.1: bytes=32 time=1ms TTL=64Reply from 192.168.1.1: bytes=32 time=1ms TTL=64Reply from 192.168.1.1: bytes=32 time=1ms TTL=64Reply from 192.168.1.1: bytes=32 time=1ms TTL=64Better. I hope it is not a problem with Windows Anniversary Update" }, { "title": "How to setup Bash and Ubuntu on Windows - step by step guide", "url": "/how-to-setup-bash-on-windows/", "categories": "", "tags": "windows", "date": "2016-04-20 17:32:47 +0000", "snippet": "Running Ubuntu apps on Windows is now a piece of cake. We just need to setup Ubuntu (with Bash) on Windows using Insider Preview Build in few simple steps.Became Windows InsiderIf you aren‚Äôt a Windows Insider, this is the first step. Go to https://insider.windows.com/ and Get Started.Bash on windows is still available only on builds for InsidersSetup Windows UpdateThis step took me more than one week. So be patient :)First of all open Windows Update on your PC. Press start icon and write Advanced Windows Update options, like below:Next step is to setup Fast ring on progress bar marked with the red ellipse on below screenshot. By the way, funny bug on Windows is marked with the green rectangle. I have 100% English windows but fast ring description is in Polish.Be patientThis part took on my PC more than 5 days. I manually clicked few times Get Updates but nothing happened. It was day after day like that:But one day, yesterday, I received the update.Enable Windows Subsystem for LinuxRun _Turn Windows features on or off and select below the checkbox.Do restart after installation as setup suggests. Now You are ready to final stepsFinal stepsSearch for bash in the start menu. Then run it and say yes to below question:After a few seconds/minutes you will see:Now difficult part happen. Do You still remember some Linux commands? And be careful with rm /mnt/c -rf/ It isn‚Äôt a good command for the first time. Even if some tweet suggests it :DCan I run docker on it? Or redis? Or ‚Ä¶?Unfortunate the answer is: no. Kernel must be 3.10 at minimum to run docker on Ubuntu. When we run uname -r the output is 3.4.0+. So it is incompatible. But still you can install and run for example redis or pgsql on native Linux inside Windows.How does it work?If you are interested how Windows Subsystem for Linux does work there is a very good blog post on MSDN: https://blogs.msdn.microsoft.com/wsl/2016/04/22/windows-subsystem-for-linux-overview/" }, { "title": "Slides from Get.NET", "url": "/slides-from-get-net/", "categories": "", "tags": "speaker, devops", "date": "2016-04-16 13:59:54 +0000", "snippet": "AbstractDevOps is a buzz word for last few years. It associate with other buzz words like: cloud, agile, fast deployment. Moreover in DevOps stuff we usually talk about Linux tools. But is it possible to have DevOps in company like bank? Can we do all fancy stuff on Windows? Is bank really something different than Google or Facebook?Slideshttp://stapp.space/content/images/slides/devopsinbanking/more: http://stapp.space/speaking/" }, { "title": "Top news from Build 2016 - my private ranking", "url": "/top-news-from-build-2016/", "categories": "", "tags": "windows, visual-studio", "date": "2016-04-03 17:21:00 +0000", "snippet": "Build 2016 just ended. I wasn‚Äôt lucky to watch it live. Fortunately live streaming, twitter, and news were available.There were a lot of sessions and news. I decided to present my favorites.Bash on WindowsThis was the first day top news. Running apt-get sounds great, especially if you did some stuff on Linux. I don‚Äôt know it yet, but I hope so that alongside with Docker we can run every Docker image native on Windows. I hope it won‚Äôt be just cool stuff, but a real useful tool. Unfortunately, we have to wait until Windows 10 Update will be public. Up to then we can only read about it on Scott Hanselman blogXamarin - free and Open SourceIn my opinion, this looks like a game changer. Until now I didn‚Äôt have a chance to use Xamarin because my pet project took longer that 30 days (demo time).Now I will have a chance to create a full mobile app using even Xamarin.FormsHolo lensHolo Lens developer edition is ready to buy. But only for devs in USA and Canada. I‚Äôm from Europe, so I will have to wait - gosh :(Xbox OneMaybe I should buy Xbox One. Paying 19$ (plus console) you can convert it to Dev Console and test your Windows Apps.It will be a great reason to convince my wife to buy Xbox One. Please keep your fingers crossed for me and my kidsChat botsMicrosoft announced tools for creating powerful chat bots. Sound funny and I hope that I will have a chance to use it on real product. For example it will be great as a helper to our product without buying an expensive external tool-set. Check it out on github and https://dev.botframework.com/Next Visual Studio ‚Äú15‚ÄùVisual Studio ‚Äú15‚Äù is in preview version. It is avaliable on https://www.visualstudio.com/en-us/news/vs15-preview-vs.aspx. The coolest thing is that it is small - less than 1GB.Watch more on youtube: &lt;iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/DvJoYlaKgEA\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;SumaryI will have a few working weekends in near future to explore new stuff and I hope Windows 10 update will be available soon.Did you notice something that I missed? Just let me know in comments." }, { "title": "Why HTML form with AJAX submit gives 404?", "url": "/why-html-form-with-ajax-submit-gives-404/", "categories": "", "tags": "javascript", "date": "2016-03-16 07:32:00 +0000", "snippet": "Imagine you have a simple login HTML form on your /Login page. It looks like below:&lt;form id=\"loginForm\" method=\"POST\"&gt; &lt;input id=\"textinput\" type=\"text\" placeholder=\"login\"/&gt; &lt;br/&gt; &lt;input id=\"passwordinput\" type=\"password\" placeholder=\"password\"/&gt; &lt;br/&gt; &lt;button type=\"submit\"&gt;Submit&lt;/button&gt;&lt;/form&gt;Because we all love SPA, I/you/they decide to add AJAX login instead of full reload. Below AJAX call script uses jQuery, but You can replace it with anything you want. It is only a snippet, not a full code:// attach to form submit$(\"#loginForm\").submit(function(e) { var url = \"/ajaxLogin\"; // the url where you handle the form input. $.ajax({ type: \"POST\", url: url, data: $(\"#loginForm\").serialize(), // serializes the form's elements. success: function(data) { alert(\"You did it hurray :D\"); // do stuff } }); e.preventDefault(); // avoid to execute the actual submit of the form.});If you want to run above code just use bellow snippet: AJAX form causes 404 - JSFiddleThe problemFrom time to time, a small and random group of users sends a bug: After I log in, I have 404 errorYou check your landing page, error handling, etc. - but there isn‚Äôt anything causing 404. Desperately you decide to search your www servers like Apache or IIS. You find out that some users have 404 error on POST request to /Login page.What? Why? When?Let‚Äôs check the script. The only POST should go to /ajaxLogin -in fiddler example it is /echo/json. This URL is responsible for handling the form. The /Login page can be a static HTML resource or method with only GET implemented.So what just happened? There are few similar possibilities: scripts loading was slower than user there is an error in the script, which happens occasionally if You have any other idea just put it in the commentsThe solutionsBelow my ideas for above problems:Option 1: accept POST on /Login and handle form correctly.It will work perfectly even if there is a problem with JavaScript. But there is one problem. If it isn‚Äôt a fault in your script on the login page, the user probably will have problems on the ‚Äúafter-login‚Äù page.Option 2: add URL into form and handle full reload in /ajaxLogin. This option is same as above.Option 3: disable form until JavaScript is ready. For example, we can remove submit button and add it from JavaScript only. Remember: hiding form will not always work correctly especially for passwords managers.Option 4: remove the form and use only inputs. This is a bad idea because form element is handled well by accessibility toolsIf you have more options please put it in the comments or write me a message.Updated status - what caused my problemI found out that some chrome extensions broke my JavaScript. It is not compatible with Require.JS and causes bug like below:If you know that extension, please send me a name." }, { "title": "Should I refactor my code?", "url": "/should-i-refactor-my-code/", "categories": "", "tags": "", "date": "2016-02-24 08:00:00 +0000", "snippet": "Probably everybody knows below meme:But why are we so afraid?New requirementImagine that your boss just gave you a new requirement. He wants to include twitter alias in login method.You open the file and see below code:public bool Login(string us, string p){ if(us.Length==7){ return TempLogin(us, p); } if(us.IsDigits()==false &amp;&amp; us.Length==8){ if(us.IsEmail()){ us = RunQuery(\"SELECT id FROM UserInfo WHERE Email=@email\",us); } else{ us = RunQuery(\"SELECT id FROM UserInfo WHERE Alias=@alias\",us); } } bool r; ValidateUser(us, p, out r); return r;}Fortunately code isn‚Äôt bad. We can read it and even understand quickly: us is for user p is for password. But ask yourself what should you do: add one more if with another SELECT propose some refactoring to clean up above code like separate business logic from data access, change names, ‚Ä¶ quit job because they don‚Äôt use ORM/NoSQL/‚Ä¶ other options?I‚Äôm sure most of developers will decide to use copy-paste method: add one more if. Why?Because this looks safe. Because we are afraid of mistakes.The funniest thing is that without refactoring we only delay errors. I will fix it fast and make it work. Next time it won‚Äôt be me, it will be someone else.Yes, sure, it won‚Äôt be me, in 95% it will be future me.Names and ifsCool names like us, p or rare completely unreadable. The us is ‚Äúperfect name‚Äù for email, id, alias and of course United States of America username. Fortunately p is for password. It could be temp or x1. Maybe be agent007 or secret_agent will be better? ;)I‚Äôm sure first use of ‚Äúif‚Äù was 100% rationale. Of course except adding a SELECT :) But maybe it was a junior? Or someone was in hurry? Or maybe someone was lazy? Or ‚Ä¶?When we see code which could be better, we shouldn‚Äôt wait, but fix it ASAP. Real ASAP, not ‚ÄúAround September, August - Probably‚Äù.If you are afraid or you aren‚Äôt sure, talk to your boss or team leader. There is a big probability that nobody before said it to him, because adding a new else if was always easier.What more?There are other well know cases like: hell in CSS, hell in dependencies, unreadable class/method names, etc. And we have a lot of ways to fix it. Sometimes even with rewriting the part or changing technology to something better. Like switch from CSS to LESS or SASS. And teach everybody around you how to use the new technology. The coolest technologies don‚Äôt bring good code. You do!Be a craftsmanThere are wise words by Sandro Mancuso in The Software Craftsman: [In a well-crafted codebase] adding or changing features does not take longer than it used to take at the beginning of the project.And this is true but difficult to achieve. The only way is to fix the code when you see a problem. It will not take long, maybe an hour or two. Do not wait, because next developer probably won‚Äôt fix it also. He will be even more afraid than you.I am sure that everyone will add the first ‚Äúif‚Äù. The problem is always with third-fifth-tenth when we start to see that the code is strange and ugly. Moreover you have the biggest chance to modify this method next time, because you know it well.Remember the refactoring is for You." }, { "title": "4developers is coming (CFP is open till 22.02.2016)", "url": "/4developers-2016/", "categories": "", "tags": "speaker", "date": "2016-02-17 08:28:59 +0000", "snippet": "One of the biggest IT festival in Poland 4developers will be this year 11th April.Last year 4developers was superb. There were a lot of different paths like: .NET, Java, PHP, UX, Front-end, Application architecture and more.Even I have a presentation with Marcin Przekop about DevOps in mBank. It is available on YouTube (polish only):This year is really near, but still you can send your CFP till 22th February.2016 dotNet pathMoreover dotNET path for sure there will have 2 great sessions: Azure Service Fabric by Sebastian Gƒôbski Lock Anatomy and Scalable Multicore Programming by Bartosz AdamczewskiI know both speakers well and I am sure that both sessions will be essential.15% discountIf you want to see above sessions and much much more, I can offer you 15% discount.Just use promo-code: 4dev_stappspace on registration pageLet‚Äôs Make IT Grow. See you there." }, { "title": "How to remove JavaScript console.log - pros&cons", "url": "/how-to-remove-javascript-console-log-pros-2/", "categories": "", "tags": "javascript, code", "date": "2016-02-02 06:10:25 +0000", "snippet": "How to remove JavaScript console.log - pros&amp;consA few days ago I wrote a post: Disable JavaScript console on production. Shortly after that, I had a discussion with a friend about other ways to remove console.* statements. I felt that the conclusions of that conversion are useful, so I decided to share them with you.Method 1: remove it manuallyThe header is self explanatory. If we decide to go down this route, we need to search and remove all variations of console.* statements. For example: console.trace, console.time, console.warn, ‚Ä¶In this case, a regular expression like console[.]\\w[(].+[)] will be usefulPros console.* statements are removed from our codebaseCons it‚Äôs manual it needs to be run after every single commit there‚Äôs no way to turn logging on, on demand (i.e to troubleshoot a crash) did I mention that it is manual?Of course we can run some static code analyst to find out all the console.* statements. Event JSLint will be enoughMethod 2: build stepWe can automate the previous steps using our build tools. A couple of examples are included below: grunt task to remove console.* statements. UglifyJS 2 drop_console option to discard console.* functionsand some more.Pros console.* statements are removed from our codebase it is done automaticallyCons only way turn logging is to rebuild&amp;deploy our code we have to check if library supports fancy statements like console &amp;&amp; console.log(\"foo\")Method 3: Using a logging libraryThere are a couple libraries available on market. Just few names: JsNLog, log4javascript, loglevel.The main idea using a 3rd party library is that instead of console, we wrote: log, debug, JL, etc.Another benefit is that logging can be turned on and off globally through a config file. It is handy for troubleshooting in production environmentsPros libraries are configurable, so we can disable only some logsCons used plugins probably does not use same logging library we need to run periodic check to see if nobody is using console.*Method 4: OverrideThis approach allows us to override the console.* statements. And, we can easily do it globally. The full code is in my post: Disable JavaScript console on productionPros it‚Äôs easy to implement it includes all external libraries logs statements it is configurable. In my code I disable all log types, but we can choose to disable only a subset (only error or info)Cons does anybody have an idea?If you have another idea on how to implement this or want to discuss this further feel free to leave a comment or send me a message." }, { "title": "New features in jQuery 3.0", "url": "/jquery-3-0/", "categories": "", "tags": "javascript", "date": "2016-01-25 08:08:00 +0000", "snippet": "We received a big present for the 10th anniversary of jQuery, jQuery 3.0 has reached beta status. Let‚Äôs take a look at what‚Äôs new in jQuery and how to use it.jQuery.Deferred Updated to be Promises/A+ CompatibleQuoting https://promisesaplus.com/ A promise represents the eventual result of an asynchronous operation. The primary way of interacting with a promise is through its then method, which registers callbacks to receive either a promise‚Äôs eventual value or the reason why the promise cannot be fulfilled.The promises in jQuery was a problem before (more you can read on stackoveflow). At least they are fixed.New .data() implementationThe new implementation is closer to HTML5 dataset specification. All keys are now converted from kebab-case to camelCase, regardless of access method. One important notice: digits no longer take part in this conversion.For example: foo-bar and fooBar will be same, but foo-23 and foo23 not. More on below image from jsbin.com:Deprecated methods .load(), .unload() and .error() are removedThey were deprecated since jQuery 1.8. To handle above events correctly we should use on method. For example:$( \"#element\" ).on( \"load\", function() { // implementation});I‚Äôm pretty sure that a library with above extensions will be released soon :)##.unwrap now with selectorThe selector allows to be specific about which wrappers should we remove. Before it was impossible.Speed-up for some jQuery custom selectorsPaul Irish at Google make a huge detective work and identified some cases where a bunch of extra work can be skipped for custom selectors. For example selector :visible is 17 times faster now.This change actually was made in 1.12/2.2, but was reiterated for jQuery 3.0.Better handling of error casesIncorrect requests which have been ignored till now. Simple example: $(\"#\") now throws an error rather than returning empty collection.AnimationsBecause all modern browsers support requestAnimationFrame (more on caniuse.com), jQuery will use this API when performing animations. Main advantage is smoother animations and less CPU-intensive animations." }, { "title": "ReAttach - how to easily (re)attach the debugger in Visual Studio", "url": "/reattach-visual-studio/", "categories": "", "tags": "tools, visual-studio", "date": "2016-01-21 14:07:00 +0000", "snippet": "In more complicated projects, simple F5 doesn‚Äôt compile&amp;debug. In cases where you‚Äôre working with IIS (w3wp process), we need to attach the debugger manually.Attaching debuggerThe simple gif below shows you how to attach it normally:For the first time it‚Äôs ok, but repeating this step 10 times in hour is painful. Could this be done in one button or keyboard shortcut?Tool to the rescueReAttach is a free tool which solves this exact issue. Just take a look at the picture below:It has many useful and simple features: Hotkey sequence CTRL+R, CTRL+[number] allows you to attach to one of the latest targets It waits for the process if it doesn‚Äôt exist yet (for example w3wp IIS\\APPPOOL myapp) It supports all Visual Studio versions after and including 2010You can install it from the Visual Studio Gallery and browse source code on github" }, { "title": "New Year's resolution: try new front-end JavaScript frameworks", "url": "/new-year-task-try-at-least-two-new-javascript-frameworks/", "categories": "", "tags": "code, javascript", "date": "2016-01-15 08:32:31 +0000", "snippet": "Quoting Wikipedia: Blue Monday is a name given to a day in January (typically the third Monday of the month) reported to be the most depressing day of the year. The concept was first publicised as part of a 2005 press release from holiday company Sky Travel which claimed to have calculated the date using an equation. [‚Ä¶]In 2016 is predicted quite late, on 25th of January. So still there is a time to fulfill my New Year‚Äôs resolution: try more new JavaScript front-end frameworks. Especially that there is a lot of new and re-new stuff on the market. Just look on below list:ReactJSReact is almost passe now as 2015 pass away. But still if you does not try it you should as soon as possible. Especially that guys from Arkency created a set with practical examples on github. Just reserve an evening and do it even today from reactjs koans repo. In my opinion it is the best resource to start with ReactJSAngular2I am not an angular fan. But I know some of them. In my opinion Angular starts to be religion like Windows or Linux. You believe in it or not :)Moreover articles like Angular 2 versus React: There Will Be Blood doesn‚Äôt make me feel better.There is a 5 minutes quick start tutorial on official Angular page. Also http://learnangular2.com/ page looks quite interesting.Ember 2.0They have the funniest 2.0 version release reason: Ember 2.0 only removes features that were deprecated as of Ember 1.13, so apps that run on Ember 1.13 without any deprecation warnings should run without issues on Ember 2.0.Anyway Ember is not so popular but do not cross it out. The tutorial on official page shows basic. Just visit https://guides.emberjs.com/. For more guides check comments bellow.Aurelia.ioMy personal favorite. I really love this framework. I only regret that I did not make any ‚Äúproduction‚Äù site using it. But after doing tutorial form http://aurelia.io/docs.html I just fell in love with it. I have some objections which are described in Most important question on your next JavaScript framework, but I love it anyway.Why should I try?Last week on twitter I saw bellow tweet:&quot;I think I&#39;ll give this new front-end framework a try.&quot; pic.twitter.com/DBfiaFy77R&mdash; Heydon (@heydonworks) January 10, 2016It make me think that I had such situation a few times in my life. But JavaScript is changing fast and me/you/them/everybody should have know how ‚Äúthe gun‚Äù is working before trying to use it.@Cover from https://www.pexels.com" }, { "title": "Private npm registry with fallback to global registry", "url": "/private-npm-registry-with-fallback/", "categories": "", "tags": "nodejs, devops", "date": "2016-01-13 16:01:15 +0000", "snippet": "The requirementFrom time to time we need to use npm packages on following machines: Continuous Integration build agent with no access to internet (usually CI agent) with slow internet proxyMoreover as Windows developers we have Windows servers.PrerequisitesThere are few mostly obvious prerequisites. I will list them to make sure You won‚Äôt forget about anything: Windows server (or Linux one) - can be a simple virtual machine Installed nodejs from https://nodejs.org/ Installed and configured CTLM (if your proxy need this)SinopiaSinopia is a private/caching npm repository server.To install it just run:npm install sinopia -gTo ‚Äúrun‚Äù it just create a new directory, enter it and start sinopiamkdir sinopiacd sinopiasinopiaTwo important things just happened: In your profile a config.yaml file was created (something like C:\\Users\\XXXXXX\\AppData\\Roaming\\sinopia\\config.yaml) The server started on default port (enter URL in browser to see it)Config in global dir is cool but useless. So let‚Äôs copy it to current dir and run sinopia with:sinopia.cmd --config .\\config.yamlUseful configurationThere are some points we should add or change in configuration: Changing the port to ‚Äúnormal‚Äù one like 80 or 8080. In the end of config.yaml we need to add:```listen: 0.0.0.0:80 # listen on all addresses (INADDR_ANY)``` Add proxy with http_proxy: http://localhost:3128/https_proxy: http://localhost:3128/no_proxy: localhost,127.0.0.1 Depends on your proxy change https://registry.npmjs.org/ to http://registry.npmjs.org/To verify if everything above is working just run again:sinopia.cmd --config .\\config.yamlCreate a Windows serviceBecause I mainly work on Windows environment I need to run above a Windows service.I decided to use winser package. The easiest usage is to create package.json and add commands in it.Sum upAll above stuff and changes I put in simple github repo. If you find any error contact me and I will fix it. Just clone, fix port if you need and run npm install as windows administrator." }, { "title": "Disable JavaScript console on production", "url": "/disable-javascript-console-on-production/", "categories": "", "tags": "javascript, code", "date": "2016-01-12 12:21:46 +0000", "snippet": "I love to put a log of debugging information in my code. Especially in JavaScript part. It shows me a lot of stuff during my work and allows to diagnose what had happened.Moreover I can include tracing/measure/etc using time and profile functions.But all above logs shouldn‚Äôt be visible on production to every user.The solution for above is simple. we need to override window.console functions.(function () { var method; var noop = function noop() { }; var methods = [ 'assert', 'clear', 'count', 'debug', 'dir', 'dirxml', 'error', 'exception', 'group', 'groupCollapsed', 'groupEnd', 'info', 'log', 'markTimeline', 'profile', 'profileEnd', 'table', 'time', 'timeEnd', 'timeStamp', 'trace', 'warn' ]; var length = methods.length; var console = (window.console = window.console || {}); while (length--) { method = methods[length];\t\tconsole[method] = noop; }}());If I forgot about something in window.console object, just let me know.@Cover from WAToday" }, { "title": "Contact", "url": "/contact/", "categories": "", "tags": "", "date": "2016-01-10 20:26:14 +0000", "snippet": "If you‚Äôre interested in contacting me for any kind of particular reason, feel free to use the information below: Mail: piotr.stapp(at)gmail.com Twitter: https://twitter.com/ptrstpp950 StackOverflow: http://stackoverflow.com/users/1749895/piotr-stapp LinkedIn: https://www.linkedin.com/in/piotrstapp GitHub: https://github.com/ptrstpp950" }, { "title": "Clean code - regular expressions", "url": "/clean-code-regular-expression/", "categories": "", "tags": "code", "date": "2016-01-04 14:27:11 +0000", "snippet": "The most important thing I learn in 2015 is about clean code. It is everything. You can have a bad performance or logical errors or even security issues. But without clean code, you cannot fix it. Unless you don‚Äôt understand the code, you won‚Äôt fix it.Regexp versus clean codeThe regular expressions were always dramatic for fast reading. I always need a lot of time to understand what exactly is going on. The only help was in the variable name.Take a look on example from StackOverflow question Regular expression for matching account numbers:^Acc(?:oun)?t(?:\\s+Number)?.+[\\d-]+$Reading it through gives me an idea that string should contain: some parts of Account Number digits at the end of the string.But the full requirement was: I need a regex which checks each line starting with ‚ÄúAcc‚Äù, ignore white space and any special character and then ending with numeric. However, in addition to account numbers it is also matching other patterns such as ‚ÄúBank ID : 12345‚Äù; ‚ÄúAccount Name : ABC‚Äù . ; ‚ÄúAccount Address‚Äù etc. Any way to exclude these?Does above rather simple regexp fulfill requirements? To be fair I don‚Äôt know.The only rescue is in tools like Jex Regulex to visualize regular expressions and make them readable:How about the code?Such solution has one problem. We need an additional tool or plugin to see what is going on in one line in the code. Which make this solution useless until we are debugging an error. A few unit tests can make above regexp cleaner. But isn‚Äôt more readable. It just makes it easier to investigate.Can you image that above code will look like following?var tester = VerEx() .startOfLine() .then( \"Acc\" ) .maybe(\"oun\") .then(\"t\") .maybe(VerEx().add(\"\\\\s+\").then(\"Number\")) .anything() .then(VerEx().then(\"-\").or().range(0,9)).add(\"+\") .endOfLine();which produces expression like:The above code is done using the [VerbalExpressions] (https://github.com/VerbalExpressions/JSVerbalExpressions) library. There are also ports to other languages like C#, Java, Ruby or Objective-C on http://verbalexpressions.github.io/. It is much more readable. I miss only two functions, so I have to cheat above. The missing functions are: oneOrMore() instead of add(\"+\"). Existing function repeatPrevious allows only repeating the item exactly. It produces {x} or {x,y} expression whitespace() - above I used add(\"\\\\s+\"). They implemented functions like word() or tab() or even lineBreak(), but this one is missing.With above two functions we can make our code even better:var tester = VerEx() .startOfLine() .then( \"Acc\" ) .maybe(\"oun\") .then(\"t\") .maybe(VerEx().whitespace().oneOrMore().then(\"Number\")) .anything() .then(VerEx().then(\"-\").or().range(0,9)).oneOrMore() .endOfLine();The above code is clean. Using it I can guess the requirements without knowing them. And I can easy make some changes.Update: pull request is pending merged on github@cover from http://wall.alphacoders.com/big.php?i=157189" }, { "title": "Floating versions in Nuget", "url": "/floating-versions-in-nuget-2/", "categories": "", "tags": "dotnet, c, code", "date": "2015-12-18 12:34:25 +0000", "snippet": "Asp.NET 5 RCAt least ASP.NET 5 is production ready. I know that current state is release candidate, but be honest it means that it is ready. Maybe some quick fixes will be need but who cares ;)This means that we can start thinking about migrating csproj to project.jsonNuget &gt;= 3.0The separate part of .NET is Nuget. It is separate but very important unless you don‚Äôt have one project solution, and you don‚Äôt have external dependencies.In Visual Studio 2015 Update 1 Nuget 3.3. is included. I hope 3.3 means all big bugs was eliminated.Dependency managementUntil now nuget allows us to create version range but this causes that lowest applicable version was used. Or cached one if you have it on disk.This causes that a lot of companies which needed always latest version (useful in CI development) have to use external tools like ripple from fubu. Tools like ripple solves some problems, but add some others.Floating Versions in NugetGoing back to plain nuget. From version 3.0 they added support for floating versions which means that: When a floating version constraint is specified then NuGet will resolve the highest version of a package that matches the version pattern, for example 6.0.* will get the highest version of a package that starts with 6.0 more on official docsWhich means no more problems in Continuous IntegrationSomething more?If you look on npm, bower or F# tool called Paket there is only one more thing they should added: git dependencies.I hope next version will bring them also" }, { "title": "Speaking", "url": "/speaking/", "categories": "", "tags": "", "date": "2015-12-13 08:21:47 +0000", "snippet": "I pretend to be an IT speaker and I really like it. In my opinion sharing knowledge is the best thing that every developer should do. The easiest way is to speak loud :)My goal is to share my knowledge and experience in building stuff. From time to time sharing crap to make sure that someone will learn from my mistakes.Invitation to talkIf I sound like an interesting person and you would like to invite me for a talk, then feel free to email me and I would love to attend.Upcoming events DevConf x 3Previous events Chmura jak alkohol - 2018-05-26 - Rzemios≈Ço IT Ciemna strona krypto-walut - 2018-05-25 - rg-dev Chmura jak alkohol - 2018-05-23 - Infoshare with ≈Åukasz Dziekan Pogromcy mit√≥w HTTP/2 i wydajno≈õƒá stron WWW - 2018-05-15 - Akademia j-labs Magic tricks of Azure Key Vault - 2018-04-26 - Microsoft Tech Summit Warsaw Od juniora do seniora czyli tam i z powrotem - 2018-04-17 -≈ölƒÖska Grupa Microsoft Od juniora do seniora czyli tam i z powrotem - 2018-02-01 -Software Craftsmanship Wroc≈Çaw Pogromcy mit√≥w HTTP/2 i wydajno≈õƒá stron WWW - 2017-12-05 -Code Europe in Krak√≥w Chmura jak alkohol, wyciƒÖga to co najlepsze I najgorsze w Twoim zespole - 2017-12-05 - Public Cloud User Group with ≈Åukasz Dziekan Pogromcy mit√≥w HTTP/2 i wydajno≈õƒá stron WWW - 2017-12-07 -Code Europe in Warszawa CCC conitinous, cloud, configuration - 2017-12-07 - Code Europe in Warszawa Cyfrowa platforma kredytowa ‚Äì PROSTA ‚Äì dla bank√≥w i klient√≥w - 2017-11-21 - IT@Bank with ≈Åukasz Dziekan CCC conitinous, cloud, configuration - 2017-09-19 - 4developers Gda≈Ñsk Od juniora do seniora czyli tam i z powrotem - 2017-05-27 -Rzemios≈Ço IT Power(i)SHell - PowerShell 4 dev - 2017-05-26 -Rzeszowska Grupa Dev rg-dev HTTP/2 na ratunek wydajno≈õci - 2017-05-18 - Infoshare - Video Pogromcy mit√≥w HTTP/2 i wydajno≈õƒá stron WWW - 2017-04-27 -Rzeszowska Grupa Dev rg-dev ‚ÄúCzwartek z .NET‚Äù (Thursday with .NET) in Channel 9 - 2017-04-06 - Channel 9 Pogromcy mit√≥w HTTP/2 i wydajno≈õƒá stron WWW - 2017-04-03 - 4developers Pogromcy mit√≥w HTTP/2 i wydajno≈õƒá stron WWW - 2017-02-08 WG.NET Pogromcy mit√≥w HTTP/2 i wydajno≈õƒá stron WWW - 2017-02-07 WarsawJS - Video Myth busters - HTTP/2 and web performance - 2016-11-30 Dev@Ldz Pogromcy mit√≥w HTTP/2 i wydajno≈õƒá stron WWW - 2016-11-05 DotNetConf - Video Wydajno≈õƒá webowa - jak to ugry≈∫ƒá? - 2016-10-18 Dev@ZG Is DevOps in banking possible? - 2016-10-14 High Load Strategy Czy DevOps w bankowo≈õci jest mo≈ºliwy? - 2016-09-09 BBQ4IT Czy DevOps w bankowo≈õci jest mo≈ºliwy? - 2016-09-08 IT Career Summit Czy DevOps w bankowo≈õci jest mo≈ºliwy? - 2016-07-21 SysOps / DevOps Polska MeetUp - Video Czy DevOps w bankowo≈õci jest mo≈ºliwy? Czyli s≈Ç√≥w kilka o wdra≈ºaniu oprogramowania - 2016-06-18 Daj siƒô poznaƒá Czy DevOps w bankowo≈õci jest mo≈ºliwy? - 2016-05-19 IT w bankowo≈õci Is DevOps in banking possible? - 2016-04-14 Get.NET in ≈Å√≥d≈∫ PowerIsHell ‚Äì czyli PowerShell dla dewelopera - 2015-10-17 Kariera IT Jak dojrzewa≈Ç DevOps w mBanku - teoria i praktyka :) - 2015-09-30 Dev@LDZ Unit tests workshop in .NET - 2015-05-30 DevWarsztaty DevOps in mBank - lesson learned - 2015-05-18 Atmosphere conference - Video DevOps in mBank - lesson learned - 2015-04-20 4developers - Video1.Vagrant with Windows - 2015-10-31 dotNetConfPL - Video Vagrant with Windows - 2015-09-23 Bia≈Çostocka Grupa .NET Power(I)Shell - Powershell for developers - 2015-09-23 Bia≈Çostocka Grupa .NET Vagrant with Windows - 2015-06-11 Warszawska grupa .NET Power(I)Shell - czyli kr√≥tki snack o powershell dla developera - 2015-04-16 Warszawska grupa .NET - Video Cook your Vagrant - 2014-04-26 Warsjava about.mbank - 2014-09-16 Wroc≈Çawska Grupa .NET - Video Cook your Vagrant - 2014-04-26 Warsjava about.mbank - 2014-02-27 Warszawska grupa .NETSlides from previous eventsIn random order: PowerIShell - Powershell for developers - some useful tips&amp;trick in PowerShell for not only developers :)http://stapp.space/content/images/slides/powerishell/ Myth busters: HTTP/2 and web performance - https://stapp.space/content/images/slides/http2andwebperf OctopusDeploy from Zero to Hero OctopusDeploy workshop with ASP.NET and EntityFrameworkhttps://stapp.space/content/images/slides/octopusdeploy-workshop/ Is DevOps in banking possible?DevOps is a buzz word for last few years. It associates with other buzz words like cloud, agile, fast deployment. Moreover, in DevOps stuff, we usually talk about Linux tools. But is it possible to have DevOps in a company like the bank? Can we do all fancy stuff on Windows? Is bank really something different than Google or Facebook?http://stapp.space/content/images/slides/devopsinbanking/#/title Vagrant with Windows, czyli w≈Ç√≥czƒôga z oknamiHow to use Vagrant in Windows environment for Windows boxeshttp://stapp.space/content/images/slides/vagrantwithwindows/ Cook your vagrant - 4-5 hours workshop about Vagrant with bash and chef-solohttp://stapp.space/content/images/slides/cookyourvagrant/ about.mbank - the unique stuff mBank team did in rewriting mBank access channelshttp://stapp.space/content/images/slides/about.mbank/ Logon Workshop - step by step guide how to refactor your code to introduce Unit testshttp://stapp.space/content/images/slides/logonWorkshopSlides DevOps in mBank - lesson learnedhttp://mbankdevops.azurewebsites.net/#/" }, { "title": "Repeatable way to setup Your PC", "url": "/repeatable-way-to-setup-your-pc/", "categories": "", "tags": "devops, windows, code", "date": "2015-12-05 19:37:00 +0000", "snippet": "ImagineImagine following situations: My Windows is soo slow. I need to reinstall itor I bought a new PC, but I don‚Äôt have time to setup it for 2 weeksor [put here your last reason of new Windows installation]It will take me sooooo long :(. Why it couldn‚Äôt be done somehow without me?First approachChocolatey on Windows environment is like apt-get on Ubuntu. Using it we can install easy a lot of stuff like: Tools like: 7zip or TotalCommander Utils like: Git + Sourcetree + poshgit or NodeJS Editors: Sublime/Brackets/Notepad++ Browsers: Firefox/Chrome/Safari/Opera or even All this browsers in one command IDE: Visual Studio/Intellij/Eclipse/Android StudioWith one simple command:choco install [name of the stuff you really need]or even simpler with:cinst [name of the stuff you really need]Using above we can create simple script to install almost all we need like:cinst gitcinst poshgitcinst nodejs.installcinst sublimetext3cinst notepadpluspluscinst 7zip.installcinst google-chrome-x64cinst firefoxIt starts to be really interesting :) But is it enough?Boxstarter - the next stepBoxstarter web launcher is a next step. Using simple file like gist on github we can run and setup our machine with Chocolatey pages and Windows stuff (like IIS or Windows update) and even pinned task-bar item or Visual studio add-ons on local or remote machine. We can event test it on example Azure machine :)Today morningUsing boxstarter gist (like https://gist.github.com/ptrstpp950/1274f0b8d16f1c25b686) I run yesterday evening my new PC setup. In today morning I have a new shiny Windows 10 with almost all features which I needed. Just Fire&amp;ForgetIs everything working?Of course not. In my case problem was installing Visual Studio after Windows Update without reboot in the middle. But next time I will just try to make my script better :)" }, { "title": "Finding bad dependencies using PowerShell", "url": "/finding-bad-dependencies-using-powershell/", "categories": "", "tags": "code, powershell, c", "date": "2015-12-01 10:05:06 +0000", "snippet": "Difficult upgradeFrom time to time we want (or have) to upgrade our old application to a new version of framework. Usually for core libraries it goes quite fast but finding, which dependency is using previous version of assembly which is used by ‚Ä¶ - it isn‚Äôt trivialPowerShell to the rescueFirst we need to find out all exe and dll files in our bin folder and load them using reflection into array. We can use below script:$references = Get-ChildItem . -Recurse -Include @(\"*.exe\",\"*.dll\")| % { $loaded = [reflection.assembly]::LoadFile($_.FullName) $name = $loaded.ManifestModule $loaded.GetReferencedAssemblies() | % { $toAdd='' | select Who,FullName,Name,Version $toAdd.Who,$toAdd.FullName,$toAdd.Name,$toAdd.Version = ` $loaded,$_.FullName,$_.Name,$_.Version $toAdd }}Now we can list them sorted by group and sort to give duplicate references:$references | Group-Object FullName,Version | Select-Object -expand Name | Sort-ObjectIn my case I got:[...]System.Web.Mvc, Version=2.0.0.0, Culture=neutral, PublicKeyToken=31bf3856ad364e35, 2.0.0.0System.Web.Mvc, Version=3.0.0.0, Culture=neutral, PublicKeyToken=31bf3856ad364e35, 3.0.0.0System.Web.Mvc, Version=3.0.0.1, Culture=neutral, PublicKeyToken=31bf3856ad364e35, 3.0.0.1System.Web.Mvc, Version=5.2.0.0, Culture=neutral, PublicKeyToken=31bf3856ad364e35, 5.2.0.0System.Web.Mvc, Version=5.2.3.0, Culture=neutral, PublicKeyToken=31bf3856ad364e35, 5.2.3.0[...]RazorGenerator.Mvc, Version=1.5.0.0, Culture=neutral, PublicKeyToken=null, 1.5.0.0RazorGenerator.Mvc, Version=2.0.0.0, Culture=neutral, PublicKeyToken=7b26dc2a43f6a0d4, 2.0.0.0With MVC I don‚Äôt have to worry about, because I have biding redirection in web.config file, but RazorGenerator.Mvc looks strange. To have information what is using this library just run:$references | Where-Object {$_.Name -eq 'RazorGenerator.Mvc'}And results will show you exactly problems.@cover from http://www.trainingforwarriors.com/are-you-ready-for-an-upgrade/" }, { "title": "cfp.help - my first real MVP project", "url": "/cfp-help-my-first-real-mvp-project/", "categories": "", "tags": "oss, speaker", "date": "2015-11-24 08:40:54 +0000", "snippet": "The reasonEverything started with a simple tweet@mihcall is there any site with the list of upcoming conferences in Poland?&mdash; Konrad Kokosa (@konradkokosa) November 4, 2015IdeasI am almost sure that when you think about such site you just included: server side technology front-end framework database hosting git ‚Ä¶We also had a long discussion on Tweeter about technologies (from golang to .NET), databases (like rethinkDB), etc.Real needsSend here a moment and think what do we need to present.My idea accepted by others (Konrad Kokosa, Micha≈Ç ≈öliwo≈Ñ and Marcin Biega≈Ça) was to create a simple web page with one table containing following information: conference name localization (city/country) conference date CFP final date CFP link optional tags for easier search optional search box (because we can replace it by CTRL+F in browser)TechnologiesSum up of above idea can be: static webpage. For layout we decided to use bootstrap and bootstrap table component.Konrad did it a simple page with a JSON file (for date) in one evening. For source control we used github, and for hosting github pages. Because the only database we have is plain JSON file. After 2 or 3 days we decide to have one JSON per CFP and use Jekyll to merge it into final one JSON file - it is much easier to maintain.MVPThe funny thing is that instead of ‚Äúhuge‚Äù project we created 100% useful page. Exactly what we all understand as Minimum Viable Product. Now we can go we new ideas like: RSS feed, icon/image, more user friendly process of adding CFP, etc.If you have an idea just visit http://cfp.help/ or https://github.com/callForPapers/callForPapers.github.io/ and propose it :)@cover by http://blog.fastmonkeys.com/" }, { "title": "Vagrant with Windows @dotNetConfPL", "url": "/vagrant-with-windows-dotnetconfpl/", "categories": "", "tags": "vagrant, slides, speaker", "date": "2015-10-31 16:56:00 +0000", "snippet": " Slides Survey Source code" }, { "title": "Setup vagrant with Azure - step by step guide", "url": "/setup-vagrant-with-azure/", "categories": "", "tags": "vagrant, tutorial, azure", "date": "2015-10-28 12:12:31 +0000", "snippet": "Setup Vagrant with Azure is not complicated, but require a lot of steps, especially if you never configure azure using command linePrerequisitesFirst of all we need some tools: Vagrant (https://www.vagrantup.com/downloads.html) (at least 1.7.2) NodeJS + NPM Git (for openssl.exe) or puttygen Azure account :) Powershell or other command line tool :)Setup Azure CLIAzure CLI will help us to create virtual machine in Azure. We will setup credentials and certificate, which we use in vagrant.Installation is quite simple: npm install azure-cli -gThen we have to runazure account downloadAbove command is completely insane, because it opens browser to download file from azure. So instead of it you can just download file from URL. The only reason to use above is that URL changes from time to time:)Anyway save publishsettings to disk as azure.publishsettings and import it using:azure account import [PATH]\\azure.publishsettingsAfter import I suggest you to delete above file, because it contains sensitive information.In the end I suggest to remove account info, becaue all the date can be ease retrieve from [HOME_DIR]\\.azure\\azureProfile.json. The command is:azure account clearSubscription id (guid) is also needed. To get it run:azure account list and copy `Id` column calueThe next step is to find out, which Virtual Machine will we use. To find for example Windows 2012 R2 DataCenter we can run:azure vm image list | Select-String \"Datacenter\" | Select-String \"2012\"We need to remember data id like: a699494373c04fc0bc8f2bb1389d6106__Windows-Server-2012-Datacenter-20150916-en.us-127GB.vhdWe will also need location, so let‚Äôs run:azure vm location listAnd choose one. In my case it will be North EuropeGenerate keys for AzureNow we need to generate key pair for Azure. We will need pem file with public-private key pair and cer file to upload with public key. Generation can be done with openssl.exe (located in C:\\Program Files (x86)\\Git\\bin\\openssl.exe) or puttygen.exe. Command for openssl are straightforward, so I will use them. First set openssl config file as environment variable, than run generation (assuming that openssl.exe is in PATH):$env:OPENSSL_CONF=\"C:\\Program Files (x86)\\Git\\ssl\\openssl.cnf\"openssl.exe req -x509 -nodes -days 365 -newkey rsa:2048 -keyout $HOME/.ssh/azurevagrant.key -out $HOME/.ssh/azurevagrant.keyopenssl x509 -inform pem -in $HOME/.ssh/azurevagrant.key -outform der -out $HOME/.ssh/azurevagrant.cerNow we need to upload azurevagrant.cer to azure. Open in browser https://manage.windowsazure.com/#Workspaces/AdminTasks/ListManagementCertificates and click Upload certificate button. Select file from $HOME/.ssh/azurevagrant.cerVagrant install azure pluginIt is simple command (unless you are not in the proxy environment):vagrant plugin install vagrant-azureVagrant fileWe are almost in the end. Now we need to create vagrant file like following:Vagrant.configure('2') do |config|\tconfig.vm.box = 'azure'\tconfig.vm.boot_timeout = 1200\tconfig.vm.provider :azure do |azure|\t\t#full path to pem file\t\tazure.mgmt_certificate = File.expand_path('~/.ssh/azurevagrant.key')\t\tazure.mgmt_endpoint = 'https://management.core.windows.net'\t\t\t\t##to get this run: azure account list\t\tazure.subscription_id = 'YOUR SUBSCRIPTION ID'\t\t\t\tazure.storage_acct_name = 'azurevagrant' # optional. A new one will be generated if not provided.\t\t##to get this run: azure vm image list | Select-String \"Datacenter\" | Select-String \"2012\"\t\tazure.vm_image = 'a699494373c04fc0bc8f2bb1389d6106__Windows-Server-2012-Datacenter-20150916-en.us-127GB.vhd'\t\tazure.vm_user = 'vagrant' # defaults to 'vagrant' if not provided\t\tazure.vm_password = 'vagrant123#@!' # min 8 characters. should contain a lower case letter, an uppercase letter, a number and a special character\t\tazure.vm_name = 'azurevagrant' # max 15 characters. contains letters, number and hyphens. can start with letters and can end with letters and numbers\t\tazure.cloud_service_name = 'azurevagrant' # same as vm_name. leave blank to auto-generate\t\t##to get this run: azure vm location list\t\tazure.vm_location = 'North Europe'\t\t\t\tazure.tcp_endpoints = '3389:53389' # opens the Remote Desktop internal port that listens on public port 53389. Without this, you cannot RDP to a Windows VM.\t\tazure.winrm_https_port = 5986\t\tazure.winrm_transport = %w(https) end endBefore final run we have to add azure box - don‚Äôt be afraid it is almost empty file:vagrant box add azure https://github.com/msopentech/vagrant-azure/raw/master/dummy.boxAnd install azure-vagrant plugin with vagrant plugin install vagrant-azureFinal commandEnter the directory with Vagrantfile. Modify subscription id and run:vagrant up --provider=azure" }, { "title": "Null pointer exceptions and warnings", "url": "/null-pointer-exceptions-and-warnings/", "categories": "", "tags": "c, visual-studio", "date": "2015-10-21 06:59:00 +0000", "snippet": "Simple codeI am developer so let‚Äôs start with following C# code: public static void Main() { var sampleList = GetSamples(); if (sampleList.FirstOrDefault().StartsWith(\"I\")) { Console.WriteLine(\"First element starts with I\"); } else { Console.WriteLine(\"First element doesn't start with I\"); } } It compile without any warnings, which is quite cool, But will it work without problems? Of course it depends on `GetSamples` function. But it should, isn't it?Full implementationSo let‚Äôs extend above code to full implementation: static readonly Random Random = new Random(); static List&lt;string&gt; GetSamples() { if (Random.Next(100) &gt; 2) { return new List&lt;string&gt;{\"I\"}; } return new List&lt;string&gt;(); } public static void Main() { var sampleList = GetSamples(); if (sampleList.FirstOrDefault().StartsWith(\"I\")) { Console.WriteLine(\"First element starts with I\"); } else { Console.WriteLine(\"First element doesn't start with I\"); } }Now it starts to look tricky and dangerous. But again you will don‚Äôt have any warning in Visual Studio.Resharper to the rescueIf you use R# you will see following warning:Using Alt+Entershortcut generate simple fix:var firstOrDefault = sampleList.FirstOrDefault();if (firstOrDefault != null &amp;&amp; firstOrDefault.StartsWith(\"I\")){ Console.WriteLine(\"First element starts with I\");}else{ Console.WriteLine(\"First element doesn't start with I\");}And we are safe now :)##‚Ä¶ but it doesn‚Äôt show everythingBut imagine that instead of FirstOrDefault() the developer used First(), so the above if will be following:if (sampleList.First().StartsWith(\"I\"))In above case it won‚Äôt be any warning even in Resharper :(Sum up" }, { "title": "Should not happen comment", "url": "/should-not-happen-comment/", "categories": "", "tags": "code", "date": "2015-10-19 11:35:38 +0000", "snippet": "The code &amp; the commentToday I found the simple code with the even more simple comment:[...]if (entry== null) return; // should not happen[...]What if the ‚Äúif‚Äù just happened?The Finagle‚Äôs law is: Anything that can go wrong, will‚Äîat the worst possible moment.Especially in IT it is true. Unfortunately I saw variation of above code many times, which is completely sad.Fail fastIn my private opinion the best way is to fail fast. I know that user/system/boss will be sad/broke/angry, but if abnormal situation happen even once you will be know about it.So the easiest way is toif (variable == null) throw new Exception(\"Oh my god the universe just doomed: entry is null\");but I cannotIf you really cannot throw an Exception, log this error. Log it in standard way with all needed information like: stack trace, context, etc. It should be easy to access as all your logs.The perfect solutionThe perfect solution is simple: combine logging and exception. This should allow to track fast what exactly happened, which should not happened" }, { "title": "PowerIShell - slides and video", "url": "/powerishell-slides/", "categories": "", "tags": "", "date": "2015-10-17 08:27:00 +0000", "snippet": " Slides Survey Source code" }, { "title": "I'll be watching you - Monitoring is everything", "url": "/ill-be-watching-you/", "categories": "", "tags": "quote, tools", "date": "2015-10-13 07:03:31 +0000", "snippet": "ImagineImagine a perfect world, with your perfect system, when everything goes fine. And fine.More fine[‚Ä¶]And then suddenly you have a call from your boss/client/colleague: Why our button XYZ is so slow? What‚Äôs going on?Then you check: IIS/apache is fine Database is fine JavaScript looks like normal External functionality is goodSo where is the problem? You check XYZ button and it is fine.So what just happened?The sad answer: now it is nothing. The problem was 2-3 hours ago and it is gone now. You just spent some of your time completely useless. The question which suddenly comes to your mind is: what had happened and what changed?Moreover one more should appear: when it will happen again?MonitoringThere are a lot of tools, you should use to measure your application: Performance counters or JMX - this is the base Logs - unfortunate most developers (including myself) create poor quality logs. But quick access to logs is very useful. Moreover try ELK or Splunk enable fast search over history Live dashboards - I know that Zabbix or Nagios are old and ugly, but they are damn useful. Add more measures to you code with tools like MiniProfiler/Glimpse on .NET or Metric in Java. This can show use easy bottlenecks in our applications. Moreover we can see it live with tools from point 2 and 3.We are IT, we are lazy, we need tools which will do work for us. Just find what is useful for you and check what your admins use :)To sum up - remember Sting song : Every move you make Every vow you break Every smile you fake every claim you stake I‚Äôll be watching you" }, { "title": "Fighting with corporate proxy and modern tools like git, npm, bower (SSL problems)", "url": "/fight-with-2/", "categories": "", "tags": "git, nodejs, ssl, tips", "date": "2015-10-06 10:19:53 +0000", "snippet": "IT Security and safety worldSometimes in big companies the goal of the IT Security department is to make environment more safe. Due to this security lockdown, a lot of modern development tools stop working. The most common error is caused by self signed certificate in certificate chain problems. So how can we make them work properly? I‚Äôve attached a few suggestions bellowRemark: if you find something not working just let me know in the comments. Or send me an email or tweet. If you have an alternative solution, send it to me please. I will be more than happy to add it.GitGit SSL ProblemFor error: fatal: unable to access 'https://github.com/garris/BackstopJS.git/': SSL certificate problem: self signed certificate in certificate chainOr SSL certificate problem: unable to get local issuer certificateJust run: git config http.sslVerify falseGit SSH ProblemWhen you have: ssh: connect to host ssh.github.com port 443: Bad file numberOpen .gitconfig file and in the end put:[url \"https://github.com/\"]\tinsteadOf = git://github.com/npmThe error shows only when you add --verbose flag and it is something like: will retry, error on last attempt: Error: self signed certificate in certificate chainThe solution is to change the registry to use HTTP with following command: npm config set registry http://registry.npmjs.org/BowerIf you have problems with git repo use above Git solution.If the error is:Download of https://github.com/something/archive/0.7.0.tar.gz failed with SELF_SIGNED_CERT_IN_CHAIN, trying with git create .bowerrc file with following content:{ \"strict-ssl\": false}VSCode - Visual Studio CodeIf you get the following error in VS Code ETIMEDOUT 191.238.172.191:443, there can be 2 possible solutions: If you have NTLM proxy, you need to install for example cntlm (http://cntlm.sourceforge.net/), then setup a global HTTP_PROXY environment variable or change user settings (File-&gt;Preferences-&gt; User Settings). See below filefor example. If your company replaces the SSL certificate in User Settings, place following: // Place your settings in this file to overwrite the default settings{ \"http.proxyStrictSSL\": false, \"http.proxy\": \"http://127.0.0.1:3128\" //your CNTLM} Vagrant - new boxThere are following possibilities to add box behind the proxy: Download it by hand using browser and add it manually Putting an option in local or global vagrantfile(credit goes to Kevin P. Kucharczyk - more in comments): config.vm.box_download_insecure = true Using feature switch in command (credit goes to Kirtis Mcglynn - more in comments): vagrant box add {your/box} --insecureVagrant - pluginsIf you try to install a plugin with a command like this: vagrant plugin install vagrant-openstack-plugin you may get this error: Could not verify the SSL certificate forThe solution to this is to use this alternative command: vagrant plugin install vagrant-openstack-plugin --plugin-source¬†http://rubygems.org/Ruby gemsJust add the new source with the following command: gem sources -a http://rubygems.org/" }, { "title": "Presentation tip", "url": "/presentation-tip/", "categories": "", "tags": "quote", "date": "2015-10-06 04:19:00 +0000", "snippet": "For today quote by Winston S. Churchill A good speech should be like a woman‚Äôs skirt; long enough to cover the subject and short enough to create interest.So go ahead and do your work. There is a lot of places when you can present yourself. I know that you are shy and do nothing interesting, just like me. But for 99% of people what you are doing is much more interesting that what they do. Believe me, just look around :)" }, { "title": "Most important question on your next JavaScript framework", "url": "/most-important-question-on-your-next-javascript-framework/", "categories": "", "tags": "", "date": "2015-10-01 07:47:09 +0000", "snippet": "JavaScipt and frameworksNot so long time ago we have JavaScript and new nice libraries (they are usually called frameworks) like: jQuery, MooTools, YUI.Moreover not so long time ago everybody was writing their own AJAX library.But this changed and now we have a lot of new, shiny frameworks.ComicStip has a beautiful strip about it:FrameworksLast few years gives a so many frameworks, which lives fast and died young. Even big ‚Äúplayers‚Äù like Angular had problems like: Angular 2 won‚Äôt be compatible with Angular 1 in the begging they said: we won‚Äôt give you way to upgrade then after community @#$@#$, they change their mindIn my work I am having problems with backbone.js, because we used it in a way which disabled easy upgrade. Of course we can make a big effort and write a big part only to enable upgrade. The question is: what for?React.js now is in a crazy boarding state. And in 2015 everybody must use it.Next one - Aurelia?Aurelia looks like a shiny cool new framework. It is not even in public beta, but have a great community. I check it and it really looks simple and easy to adopt - which is great. But there is one small problem - the author. Rob Eisenberg is a really well know JS developer. With a lot of UX/UI experience. Just look on Aurelia framework commits:So where is the problem? There is ~40 contributors to this project, but most of them changed few lines. Which means that in this project a ‚Äúnew‚Äù design pattern is used: Single Person Responsibility. Similar to Single Principle Responsibility, but with noticeable difference.The questionAfter all you can ask yourself one important question about your next JavaScript framework: will it blend?p.s. The post cover of course is from http://www.willitblend.com/" }, { "title": "Slides and survey - Dev@ldz", "url": "/slides-and-survey-from-devops-in-mbank/", "categories": "", "tags": "speaker", "date": "2015-09-30 16:00:00 +0000", "snippet": "Slides: http://mbankdevops.azurewebsites.netSurvey: https://www.surveymonkey.com/s/XFJ2BFX" }, { "title": "Slides and survey from #bstoknet", "url": "/slides-and-survey-bstoknet/", "categories": "", "tags": "speaker", "date": "2015-09-23 16:12:50 +0000", "snippet": "Vagrant with windowsSlides: http://stapp.space/content/images/slides/vagrantwithwindows/Survey: http://j.mp/vagrantWithWindowsSources: https://github.com/ptrstpp950/vagrantDemoWin2012PowerIsHell - powershell 4 devsSildes: http://piotrstapp.azurewebsites.net/content/images/slides/powerishell/Survey: https://www.surveymonkey.com/s/MXZVKMWSource code: https://github.com/ptrstpp950/powerISHell" }, { "title": "Is User-Agent mobile, desktop or maybe TV?", "url": "/is-useragent-mobile-desktop-or-maybe-tv/", "categories": "", "tags": "c, web, code", "date": "2015-09-22 20:01:09 +0000", "snippet": "When we design new layout, we need to find out what kind of users are using our website.The easies way is to parse User Agent string, but it isn‚Äôt easy. Just look on following links: http://googlewebmastercentral.blogspot.com/2011/03/mo-better-to-also-detect-mobile-user.html http://www.useragentstring.com/pages/Mobile%20Browserlist/ http://detectmobilebrowsers.com/ http://techbrij.com/display-mode-mobile-tablet-tv-aspnet-mvcAfter few tries I ended with last link, which gives me enough result approximation :) public static string GetDeviceType(string ua) { string ret = \"\"; // Check if user agent is a smart TV - http://goo.gl/FocDk if (Regex.IsMatch(ua, @\"GoogleTV|SmartTV|Internet.TV|NetCast|NETTV|AppleTV|boxee|Kylo|Roku|DLNADOC|CE\\-HTML\", RegexOptions.IgnoreCase)) { ret = \"tv\"; } // Check if user agent is a TV Based Gaming Console else if (Regex.IsMatch(ua, \"Xbox|PLAYSTATION.3|Wii\", RegexOptions.IgnoreCase)) { ret = \"tv\"; } // Check if user agent is a Tablet else if ((Regex.IsMatch(ua, \"iP(a|ro)d\", RegexOptions.IgnoreCase) || (Regex.IsMatch(ua, \"tablet\", RegexOptions.IgnoreCase)) &amp;&amp; (!Regex.IsMatch(ua, \"RX-34\", RegexOptions.IgnoreCase)) || (Regex.IsMatch(ua, \"FOLIO\", RegexOptions.IgnoreCase)))) { ret = \"tablet\"; } // Check if user agent is an Android Tablet else if ((Regex.IsMatch(ua, \"Linux\", RegexOptions.IgnoreCase)) &amp;&amp; (Regex.IsMatch(ua, \"Android\", RegexOptions.IgnoreCase)) &amp;&amp; (!Regex.IsMatch(ua, \"Fennec|mobi|HTC.Magic|HTCX06HT|Nexus.One|SC-02B|fone.945\", RegexOptions.IgnoreCase))) { ret = \"tablet\"; } // Check if user agent is a Kindle or Kindle Fire else if ((Regex.IsMatch(ua, \"Kindle\", RegexOptions.IgnoreCase)) || (Regex.IsMatch(ua, \"Mac.OS\", RegexOptions.IgnoreCase)) &amp;&amp; (Regex.IsMatch(ua, \"Silk\", RegexOptions.IgnoreCase))) { ret = \"tablet\"; } // Check if user agent is a pre Android 3.0 Tablet else if ((Regex.IsMatch(ua, @\"GT-P10|SC-01C|SHW-M180S|SGH-T849|SCH-I800|SHW-M180L|SPH-P100|SGH-I987|zt180|HTC(.Flyer|\\\\_Flyer)|Sprint.ATP51|ViewPad7|pandigital(sprnova|nova)|Ideos.S7|Dell.Streak.7|Advent.Vega|A101IT|A70BHT|MID7015|Next2|nook\", RegexOptions.IgnoreCase)) || (Regex.IsMatch(ua, \"MB511\", RegexOptions.IgnoreCase)) &amp;&amp; (Regex.IsMatch(ua, \"RUTEM\", RegexOptions.IgnoreCase))) { ret = \"tablet\"; } // Check if user agent is unique Mobile User Agent else if ((Regex.IsMatch(ua, \"BOLT|Fennec|Iris|Maemo|Minimo|Mobi|mowser|NetFront|Novarra|Prism|RX-34|Skyfire|Tear|XV6875|XV6975|Google.Wireless.Transcoder\", RegexOptions.IgnoreCase))) { ret = \"mobile\"; } // Check if user agent is an odd Opera User Agent - http://goo.gl/nK90K else if ((Regex.IsMatch(ua, \"Opera\", RegexOptions.IgnoreCase)) &amp;&amp; (Regex.IsMatch(ua, \"Windows.NT.5\", RegexOptions.IgnoreCase)) &amp;&amp; (Regex.IsMatch(ua, @\"HTC|Xda|Mini|Vario|SAMSUNG\\-GT\\-i8000|SAMSUNG\\-SGH\\-i9\", RegexOptions.IgnoreCase))) { ret = \"mobile\"; } // Check if user agent is Windows Desktop else if ((Regex.IsMatch(ua, \"Windows.(NT|XP|ME|9)\")) &amp;&amp; (!Regex.IsMatch(ua, \"Phone\", RegexOptions.IgnoreCase)) || (Regex.IsMatch(ua, \"Win(9|.9|NT)\", RegexOptions.IgnoreCase))) { ret = \"desktop\"; } // Check if agent is Mac Desktop else if ((Regex.IsMatch(ua, \"Macintosh|PowerPC\", RegexOptions.IgnoreCase)) &amp;&amp; (!Regex.IsMatch(ua, \"Silk\", RegexOptions.IgnoreCase))) { ret = \"desktop\"; } // Check if user agent is a Linux Desktop else if ((Regex.IsMatch(ua, \"Linux\", RegexOptions.IgnoreCase)) &amp;&amp; (Regex.IsMatch(ua, \"X11\", RegexOptions.IgnoreCase))) { ret = \"desktop\"; } // Check if user agent is a Solaris, SunOS, BSD Desktop else if ((Regex.IsMatch(ua, \"Solaris|SunOS|BSD\", RegexOptions.IgnoreCase))) { ret = \"desktop\"; } // Check if user agent is a Desktop BOT/Crawler/Spider else if ((Regex.IsMatch(ua, \"Bot|Crawler|Spider|Yahoo|ia_archiver|Covario-IDS|findlinks|DataparkSearch|larbin|Mediapartners-Google|NG-Search|Snappy|Teoma|Jeeves|TinEye\", RegexOptions.IgnoreCase)) &amp;&amp; (!Regex.IsMatch(ua, \"Mobile\", RegexOptions.IgnoreCase))) { ret = \"desktop\"; } // Otherwise assume it is a Mobile Device else { ret = \"mobile\"; } return ret; }" }, { "title": "Get Firebug logs from Selenium tests", "url": "/get-firebug-logs-from-selenium-tests/", "categories": "", "tags": "selenium, c, testing, firebug", "date": "2015-08-18 13:10:05 +0000", "snippet": "The problemSelenium test are difficult to debug, especially when they are run on CI server. To diagnose why a test failed, sometimes we need something more than a simple screenshot.In my opinion we need 3 things: a screenshot network log console logThe easiest way to get last two is to use Firebug in Firefox.Run Firebug in SeleniumTo get results from firebug we need two more extensions: netExport https://github.com/ptrstpp950/netexport (my changes are not merged yet) consoleExport https://github.com/firebug/consoleexport (my changes already merged)You can compile both using ant :)The C# codeThe final code:var fireBugLogDir = \"somewhere\";FirefoxProfile ffProfile = new FirefoxProfile();ffProfile.AddExtension(@\".\\FireFoxAddons\\firebug-2.0.11.xpi\");ffProfile.SetPreference(\"extensions.firebug.currentVersion\", \"2.0.11\"); //your version numberffProfile.SetPreference(\"extensions.firebug.showStackTrace\", true);ffProfile.SetPreference(\"extensions.firebug.delayLoad\", false);ffProfile.SetPreference(\"extensions.firebug.showFirstRunPage\", false);ffProfile.SetPreference(\"extensions.firebug.allPagesActivation\", \"on\");ffProfile.SetPreference(\"extensions.firebug.console.enableSites\", true);ffProfile.SetPreference(\"extensions.firebug.net.enableSites\", true);ffProfile.SetPreference(\"extensions.firebug.scripts.enableSites\", true);ffProfile.SetPreference(\"extensions.firebug.defaultPanelName\", \"console\");// Set default NetExport preferencesffProfile.AddExtension(@\".\\FireFoxAddons\\netExport-0.9b8.xpi\");ffProfile.SetPreference(\"extensions.firebug.netexport.alwaysEnableAutoExport\", true);ffProfile.SetPreference(\"extensions.firebug.netexport.showPreview\", false);ffProfile.SetPreference(\"extensions.firebug.netexport.includeResponseBodies\", false);ffProfile.SetPreference(\"extensions.firebug.netexport.defaultLogDir\", fireBugLogDir);ffProfile.SetPreference(\"extensions.firebug.netexport.harFileName\", @\"network\"); //the file will be save as network.har in fireBugLogDir// Set default consoleExport preferencesffProfile.AddExtension(@\".\\FireFoxAddons\\consoleExport-0.5b6.xpi\");ffProfile.SetPreference(\"extensions.firebug.consoleexport.active\", true);ffProfile.SetPreference(\"extensions.firebug.consoleexport.alwaysEnableAutoExport\", true);ffProfile.SetPreference(\"extensions.firebug.consoleexport.format\", \"xml\");ffProfile.SetPreference(\"extensions.firebug.consoleexport.logFilePath\", Path.Combine(fireBugLogDir, \"consoleLog.xml\")); //the file will be save as consoleLog.xml in fireBugLogDirvar firefoxDriver = new FirefoxDriver(ffProfile);return firefoxDriver;" }, { "title": "Web design in Photoshop - completely BAD idea", "url": "/web-design-in-photoshop-completely-bad-idea/", "categories": "", "tags": "", "date": "2015-07-11 18:28:26 +0000", "snippet": "Photoshop and web designersAs long as I remember web designers created web layout in Photoshop. When layout was ready ‚Äúthe slicer‚Äù (a some between designer and coder) take this ‚Äúperfect‚Äù layout and slice it.Then coder take sliced html, usually almost completely rewrite it and created a ‚Äúperfect‚Äù website.And then the bad thing happened. The ‚Äúmain investor‚Äù looked at completed product and starts to complain.In this moment process start over, and over and over. Most of us cheats and in some moment ‚Äúthe slicer‚Äù is eliminated.But why not PSD? The are a lot of beautiful CSS frameworks, which we can reuse, without PSD file like bootstrap, google material design, metro UI, skeleton or foundation If still you want to create something by your own, maybe at least CSS grid frameworks will help you :) Creating demo or mock, every action will be there already. You don‚Äôt have to ‚Äúdesign‚Äù 3 button states Without sliced images your website will be lighter and faster - in CSS you have nowadays all fancy stuff: gradients, fonts with images, transparency, rotation, ‚Ä¶So please designers stop use Photoshop and learn HTML+CSS." }, { "title": "Vagrant with windows", "url": "/vagrant-with-windows/", "categories": "", "tags": "", "date": "2015-06-12 06:47:20 +0000", "snippet": "Slides: http://piotrstapp.azurewebsites.net/content/images/slides/vagrantwithwindows/Sources: https://github.com/ptrstpp950/vagrantDemoWin2012Survey: http://j.mp/vagrantWithWindows" }, { "title": "Log on workshop slides", "url": "/log-on-workshop-slides/", "categories": "", "tags": "", "date": "2015-05-29 20:56:31 +0000", "snippet": "Slides are avaliable here: http://piotrstapp.azurewebsites.net/content/images/slides/logonworkshopslides/The code to workshop is avaliable here:https://bitbucket.org/ptrstpp950/logonworkshop/" }, { "title": "New notepad from MS - Visual Studio Code", "url": "/new-notepad-from-ms-visual-studio-code/", "categories": "", "tags": "", "date": "2015-04-29 18:59:25 +0000", "snippet": "First time form original Notepad Microsoft created a lightweight tool for coders: Visual Studio Code (Preview)There is a lot of amazing stuff what has changed: It is available on Windows, Mac and Linux It has Git It support ASP.NET 5 and Node.js It has debugger build-in It is lightweight (~50MB to download) It opens fastNo go and download it as it is hot: https://code.visualstudio.com/p.s. Why didn‚Äôt MS have this strategy ten years ago? :(" }, { "title": "Being an author of the \"Legacy Code\"", "url": "/being-an-author-of-the-legacy-code/", "categories": "", "tags": "", "date": "2015-04-27 18:14:01 +0000", "snippet": "Legacy codeUntil you are not working in a startup, your organization has some legacy code. Moreover when this code was created it wasn‚Äôt a legacy. It was a shiny, quite impressive, used the best frameworks available in the market, ‚Ä¶But then few years passed and you look back in the code and it stopped to be so shinny.Fresh bloodMy team changed a lot in last 3 months and we have a lot of new blood. Few times I have questions like: why this is done this way? And the answer is: this part was the first one, we did this and it was working, so nobody had time to make it better.Moreover when my teammate ask if he should ‚Äúfix‚Äù this, at first I said ‚ÄúNo‚Äù.The I asked myself, what I would did in his place - fiding something stupid in legacy code. And the answer was easy: I will fix this as fast as it is possible. The only problem is this is my code :) I did it. And it was really modern, smart, etc. But years passed.Always fix itI change my mind and I told my teammates: if you find something, just fix it. We will have to spend some time to create better legacy framework, so it will stopped to be ‚Äúlegacy‚Äù" }, { "title": "mbank.devops slides and survey", "url": "/mbank-devops-slides/", "categories": "", "tags": "speaker", "date": "2015-04-20 09:14:00 +0000", "snippet": "Survey: https://www.surveymonkey.com/s/XFJ2BFXSlides: http://mbankdevops.azurewebsites.net/#/Slides source code: https://github.com/ptrstpp950/mbank.devops" }, { "title": "I need an alarm clock", "url": "/i-need-an-alarm-clock/", "categories": "", "tags": "", "date": "2015-04-13 18:56:05 +0000", "snippet": "ChangesIn last month my work little change. For the developer I was moved up to team leader - I am not calling myself manager yet :)But this has drastic change in my work hours. Basically I work maximum till 5 PM. Sometimes from 7:30 AM, but in 99% situations I was out of office at 5:00PM.This allows me to study something at home, write blog post, play with my children, etc.But this changed and I started to have situations like: ‚ÄúF##K F##K it is after 6PM. ARGH!!!!!‚Äù and run out of office. This one hour is really important for my life, so I decided to change this drastically.The Alarm ClockI just setup my mobile phone to alarm me at 17 PM. I will give myself 5 minutes to shut down my PC and go home. I hope I will return to my previous habits, because to work hard you have to rest hard. No pain no gain doesn‚Äôt work here.Cross your fingers for me please :)" }, { "title": "Generate API instead of GUI", "url": "/generate-api-instead-of-gui/", "categories": "", "tags": "", "date": "2015-03-31 19:38:44 +0000", "snippet": "Report system is a requirementSooner or later every organization/solution/group needs a reporting system. The story usually begins like this: someone from ‚Äúbusiness part‚Äù come and say ‚Äúwe need simple report system, which will show us simple stats‚ÄùMost of use believe above statement and create MVP up which fulfill above requirements.But than something completely strange happened. The same person from ‚Äúbusiness‚Äù part say something similar to: This looks very nice, could you reorder columns AND OR possibilities in conditions is a must Ohh maybe pie char will look better than bar chart. Could you add both options? ‚Ä¶My reaction was always: ARGHExternal generic report interface to the rescueToday we can use some external report GUI, instead of build our own. There are plenty solutions available on the market. Some of them are free in part, some are very expensive, some have both options. The systems you could look for are for example: Qlick Sense - Desktop is for free: http://www.qlik.com/Sense-Desktop‚Äé Tableau - Not very cheap but online version is not so expensive https://tableau.secure.force.com/webstore Pentaho - Pentaho Community Edition is free: http://community.pentaho.com/ some othersThe main plus of above solutions for developers is that we do not have to create full reporting system. Just create API - in most cases to download plain fail with data. And that is all, rest is done by external, well-supported product" }, { "title": "Optimize your code - faster html parsing in PowerShell", "url": "/optimize-your-code-faster-html-parsing/", "categories": "", "tags": "", "date": "2015-03-25 10:10:11 +0000", "snippet": "First ideaIn PowerShell I need to parse an HTML page to get some stuff from HTML table.The web page is really simple - it contains one HTML table. I search goole and find out that I can use a lot of build in stuff.So my first try was following:$result = Invoke-WebRequest $url$table = $result.ParsedHtml.getElementsByTagName(\"table\")[0];$startDate = (Get-Date)$downloadedFiles = 0;$result.ParsedHtml.getElementsByTagName('tr') | %{ $tr = $_; $filename = $tr.childNodes.item(0).outerText; $time = $tr.childNodes.item(5).outerText.Replace(\"-\",\"\").Replace(\" \",\"T\").Replace(\":\",\"\"); if($time.StartsWith($filterDate)) { Write-host $filename \" \" $time $downloadedFiles++; }}Write-Host \"Found $downloadedFiles files in \" ((Get-Date)-$startDate)The result of above code was: Found 192 files in 00:20:34.3135602 The total rows in table is: 3490 rows. FAIL.Why above fact happened? Because PowerShell in standard mode in parsing HTML uses IE and COM object t communicate. This probably slows down the process too much.Speed upOptimization was simple: instead of parsing HTML using IE I need to parse XML using build in functions. But there is a problem my page is not a valid XML. So I extracted a table, fix XML and do following:$downloadedFiles = 0;$tableHTML = $result.ParsedHtml.getElementsByTagName('table').item(0);[xml]$tableXML =$tableHTML.innerHTML.ToString().Replace(\"&lt;BR&gt;\",\"\");$tableXML.ChildNodes.Item(0).ChildNodes| %{ $tr = $_; $filename = $tr.childNodes.item(0).InnerText; $time = $tr.childNodes.item(5).InnerText.Replace(\"-\",\"\").Replace(\" \",\"T\").Replace(\":\",\"\"); if($time.StartsWith($filterDate)) { Write-host $filename \" \" $time $downloadedFiles++; }}Write-Host \"Found $downloadedFiles files in \" ((Get-Date)-$startDate)The result: Found 192 files in 00:00:03.0420975.Much better :D" }, { "title": "Bad JSON escape sequence: \\v - the war stories", "url": "/bad-json-escape-sequence-v/", "categories": "", "tags": "", "date": "2015-02-21 11:09:21 +0000", "snippet": "IntroductionYesterday I did a small C# program for hash data in JSON. The are many files, which contains a lot of lines with JSON :)I was using Json.NET the most popular JSON library for .NET.One in 200000000 lines (the number is correct) was bad. It was like this:{\"data\": {\"str_1\": \"&amp;#39; \\u001e\\v \\u0003\\u001f&amp;quot;\\u0003\\u001d %\\b)\\v#\"}}And I receive an exception: Bad JSON escape sequence: \\v. Path 'data.str_1', line 1, position [number]\\v is equal to vertical tab which is |Google resultsThe first thing I did was to search Google for ‚Äò\\v‚Äô in JSONI was looking for phrase like this: bad JSON escape sequence: \\v.. The first result was How to escape special characters in building a JSON string on StackOverflow. There I founded that: See this list of special character used in JSON : \\b Backspace (ascii code 08)\\f Form feed (ascii code 0C)\\n New line\\r Carriage return\\t Tab\\v Vertical tab\\' Apostrophe or single quote (only valid in single quoted json strings)\\\" Double quote (only valid in double quoted json strings)\\\\ Backslash caracter More over I found a source of above quote: https://www.json.com/json-objectNext step: question on SO about Newtosoft.JsonI created a new question on SO: Newtosoft.Json Bad JSON escape sequence: \\v.Because I didn‚Äôt receive any useful answer I decided to fix Newtosoft.Json. My decision appears as a pull requestUnexpectedly James Newton-King reject it and point me that \\v is not valid according to spec on http://json.org/.WHAT???Moreover he is right. According to both: RFC 4627 and http://json.org/ which is same as ECMA-404 The JSON Data Interchange Standard the \\v is not validSum upThe unlucky escaped char took one day of my life. But I learn something new, so it is not a lost dayI also created a comment on https://www.json.com/json-object#object-with-strings and I hope they fix the definition.The funny part is that some JSON validators accept \\v some not :)Below is a table with some JSON validators from first page on Google search\t \tJSON validator Error on \\v Remarks http://jsonlint.com/ Show error http://www.freeformatter.com/json-validator.html Show valid Information sent http://jsonformatter.curiousconcept.com/ Show error https://www.jsoneditoronline.org/ Show error http://codebeautify.org/jsonvalidate Show error http://jsonviewer.stack.hu/ Show valid https://extendsclass.com/json-validator.html Show error " }, { "title": "Time and culture", "url": "/testing-your-time/", "categories": "", "tags": "", "date": "2015-02-13 09:43:43 +0000", "snippet": "The problemYesterday we find a cool bug in code. The code is really simple and was working on 3 of 4 machines. The problematic code is following:var path = @\" E:\\tmp\\test_\" + DateTime.Today.ToShortDateString()+\".tmp\";using (var file = new FileStream(path, FileMode.Create, FileAccess.Write)){ //something}The exception was System.IO.DirectoryNotFoundException in FileStream and we of course checked that E:\\tmp\\ exists. Moreover the error was only one one machineThe solutionYou can run above code on your machine and I am almost sure that if it works correctly. But If you add following lineCultureInfo.DefaultThreadCurrentCulture = new CultureInfo(\"en-us\");it will throw an error. Why? Because directory: E:\\tmp\\test_2\\13\\ does not exist. In polish culture the file E:\\tmp\\2015-02-13.tmp is created without problems, because it does not contains any \\Thing to rememberBe careful we culture specific date formating, especially when we use them in paths.The funny thing is that any unit test did not find that issue." }, { "title": "Run sudo in Windows", "url": "/sudo-under-windows/", "categories": "", "tags": "", "date": "2015-02-09 08:47:53 +0000", "snippet": "It is really frustrating when you need UAC to run one command. You are doing you stuff and you have to launch a new console (cmd or PowerShell) run one command, close it (or not) and start working again.In Linux you have one magic word: sudo and you do not have to think about it. Of course we can disable UAC, but it is not recommended :)But on Friday at least I found a magic sudo command for windows. Simple, plain PowerShell script, just to put in $PROFILE in PowerShellfunction sudo{\t$file, [string]$arguments = $args;\t$psi = new-object System.Diagnostics.ProcessStartInfo $file;\t$psi.Arguments = $arguments;\t$psi.Verb = \"runas\";\t$psi.WorkingDirectory = get-location;\t[System.Diagnostics.Process]::Start($psi);}Now we can run everything in PowerShell with sudo command arg1 arg2 argX. Hurray :)UpdateThe above works only with programs and have a problem with PowerShell commands. The simple workaround is to use sudo !! (run last command as sudo). I describe this in the sudo !! - run last command in elevated PowerShell prompt" }, { "title": "You stuck, have a problem, need help? Let's try to find an answer.", "url": "/you-stuck-have-a-problem-need-help/", "categories": "", "tags": "", "date": "2015-02-08 16:48:09 +0000", "snippet": "Almost every day we have an unsolved problem. So of them: Where is my ball? Daddy help!!! Dude were is my ride? Who have seen my keys? Why my project does not work?For first three, usually we have to deal with it alone or ask our wives.I don‚Äôt know why, but for last problem the ‚Äúwife option‚Äù does not work.Simple problem, simple solutionOur first choice is Google (or Bing) and we are redirect to StackOverflow/forum/blog. 30 seconds and we are happy.If problem is a little more complicated (or we didn‚Äôt search enough) we can ask question. My case: Last week I need to calculate stats for all threads, which I was executing. SO was the perfect solution and I received a full generic codeComplex stuffAbove stack works perfectly when we have a problem in code. But what about problem with external solution - it can be ‚Äúblack box‚Äù, open-source, something mixed - especially when it is a performance solution.I suggest to do following: find official forum (sometimes it is SO) - in my opinion authors don‚Äôt like unanswered question, so someone will take care find official mailing group - I hate this kind of forum, but it works same as above use social media if someone answer on above, especially when he is evangelist or author ‚Äúattack‚Äù him personally :) ask your friends - maybe someone knows a specialistUsing all above options remember about most important option:" }, { "title": "I am XYZ developer. How about You?", "url": "/i-am-xyz-developer-how-about-you/", "categories": "", "tags": "", "date": "2014-12-05 14:37:01 +0000", "snippet": "I am a developerI always introduce myself as a .NET developer, but most time I don‚Äôt write dotNET code at all. I can generalize it to ‚ÄúXYZ developer‚Äù, where XYZ is one technology name.But during last year I build some stuff in: JavaScript+Css+HTML+Backone+Angular+‚Ä¶ -&gt; but this part still qualify into ‚ÄúXYZ developer‚Äù SQL database in various version -&gt; this part is still OK: databases includes into ‚ÄúXYZ developer‚Äù work Cassandra+Hadoop+‚Ä¶ -&gt; Big Data is just bigger database, so same as above Apache Storm -&gt; I have to use Java+Maven+‚Ä¶, doesn‚Äôt qualifies to ‚ÄúXYZ developer‚Äù until XYZ is not Java Chef+Vagrant -&gt; some DevOps stuff in Ruby, but DevOps is cool know co it include in ‚ÄúXYZ developer‚Äù PowerShell and bash -&gt; same as above but more primitive, but still DevOps Outlook+Bug tacker+‚Ä¶ -&gt; favorite tools in every organization, nobody likes them but everybody use them every day and spend in in too much time [‚Ä¶]So who we areAm I still a XYZ developer? Or maybe just developer without XYZ? Craft men or artist? I decide to create simple test. Are you ready? What will you use to calculate following equation: (820293928213392+332249999923293)*3332332I want to write here that: until your answer wasn‚Äôt Excel you know what you should put into XYZ, but I know a developer who build stuff in Excel using VBA, formulas, pivotes, etc. I am sure that this developer is not a businesses guy.To sum up: I hope you know who you are. What about me? I should start introduce myself as craftsmen or DevOps guy - is sounds much more cool" }, { "title": "Waiting for \"the change\"", "url": "/waiting-for-the-change/", "categories": "", "tags": "", "date": "2014-11-27 20:09:58 +0000", "snippet": "It‚Äôs coming, it‚Äôs coming ‚Ä¶Probably you already know this, but the change is coming - this statement I hear a lot during last weeks. And everybody does talk only about the change. Nobody exactly knows what exactly this change is. Moreover there are a lot of gossips already, but nobody knows what exactly will change.The funniest thing is that this gossip actually paralyze all changes, because everybody are waiting for the change.What‚Äôs nextI don‚Äôt believe in changes in organizations. Most of the look like: 360 degrees turn and go straight :)The only way to do the change is to get everything in your own hands and loudly propose something new.1000 wordsInstead of thing about change just:" }, { "title": "Did you clone .NET Core already?", "url": "/did-you-clone-net-core-already/", "categories": "", "tags": "", "date": "2014-11-13 07:53:12 +0000", "snippet": ".NET Core is Open SourceI am really amazed. It may be strange but I was sure that MS some day will open .NET source code, especially after showing ‚Äúroslyn compiler‚Äù some time ago. Now projects on .NET foudation list is quite impressive. Take a look and find what you would like to explore.What is more Microsoft puts codes on Github instead of old coldplex -&gt; https://github.com/dotnet .NET Core will run OFFICIAL on OSX &amp; Linux - this point I still cannot believe Free Visual Studio to everyone - new community edition which equals to professional instead of multiple ‚Äúexpress‚Äù edition for every part of platform MSDN users will get Pluarsight as a bonus Visual studio support for Apache Cordova support &amp; Android emulator &amp; Visual C++ for Cross Platform Mobile Development -&gt; full mobile development inside VSWhat does this change? I am almost sure that .NET apps will still run best under Windows. But cool things is that you can use any other system to develop them Some companies (especially small ones) will stop buying MSDN subscription -&gt; why to pay if VS is for free Microsoft proved that it see power in community ‚Äú-aaS‚Äù is future for making money - this can be MS new statementSum upProbably this weekend I will clone .NET Core and start to analyze what is inside. Maybe some pull request on Monday ;)We are living in interesting times and this is really cool" }, { "title": "[about.mbank] devs for devs @ bstoknet", "url": "/about-mbank-devs-for-devs-bstoknet/", "categories": "", "tags": "", "date": "2014-11-13 07:21:51 +0000", "snippet": "To be very short:Slides are here: http://piotrstapp.azurewebsites.net/content/images/slides/about.mbank/#/Survey is here:https://www.surveymonkey.com/s/S3DGFKC" }, { "title": "[about.mbank] devs for devs: meet me on 12.11.2014 at Bia≈Çystok .NET UG", "url": "/about-mbank-devs-for-devs-meet-me-on-12-11-2014-at-bialystok-net-ug/", "categories": "", "tags": "", "date": "2014-11-05 09:24:21 +0000", "snippet": "Sometime ago a group of very special people did a unique application. Above sentence can describe almost any IT project, but I am sure You will agree that the New mBank in 100% meets above definition, including that: It was completely renew of access channel to one of biggest internet bank in Poland It was build by multiple teams (10+) in different cites working in parallel It has unified architecture It integrates with living IT infrastructure Much much moreAnd it was done in .NET which is quite unique project on that scale done on the top of Microsoft technologies stack.So if you will be on 12.11.2014 in Bia≈Çystok or somewhere near you are welcome to listen about how we did it.The details: When: 2014-11-12 18:00:00 Where: Bia≈Çystok ul. Piƒôkna 2 Who: Me, Myself and I Link: http://codeguru.geekclub.pl/kalendarium/podglad-wydarzenia/45-spotkanie-bialostockiej-grupy-net,10311If you decide to come, You will have unique chance to hear about: How to structure and manage 1.000.000 lines of JavaScript Side-by-side (almost XP) work of UIUX artists with .NET developers Efficiently sharing artifacts between 100+ developers Making ASP.NET MVC application really modular How to trace and find bottlenecks in multi-layered, multi-technological applicationAfter presentation I am planning to have Q&amp;A session.To sum up: See you on 12th" }, { "title": "How to shine your old windows form application", "url": "/how-to-shine-your-old-windows-form-application/", "categories": "", "tags": "", "date": "2014-10-29 15:11:52 +0000", "snippet": "Long long time ago on your PC‚Ä¶you created an ugly application using Windows Forms. Probably even not one:)I did exactly same. I have a lot of such applications, most of them nobody will never use. Exactly most, but what about some?Few days a go my friend ask me, if we can ‚Äúrenew‚Äù our old application, because he would like to show it. When he opened it: it was working - good it is still useful - very good it is 100% old/ugly/etc. - not good at allWhat can we do? Nothing - easiest way :), but we are both web developers. In modern webpage we just apply bootstrap and we have nice look&amp;feel We can buy Telerik/DevExpress/ComponentOne/etc. - but it is expensive. Our application was for free. The cheapest components box, I founded was for 200$ and it was still ugly. Nice one was from 800$. For showing a free application it is too much Convert it to WPF or other. Almost same problem as with above point. Instead money we need to ‚Äúpay‚Äù with timeFrom above only ‚Äú1‚Äù is acceptable :(Something else?YES there is a fourth option. You old ugly windows form application can look like following:To see all features watch:So we can bring new Modern UI alias Metro UI of Windows 8 to .NET Windows Forms - hurray. Just visit http://viperneo.github.io/winforms-modernui/" }, { "title": "Using poshgit @VisualStudio", "url": "/poshgit-with-vs/", "categories": "", "tags": "git, powershell, visual-studio", "date": "2014-10-09 06:05:00 +0000", "snippet": "Git in Visual StudioAs You probably know Visual Studio has integrated git client. It is really cool thing. Unfortunately there is a big BUT: it does not support git over ssh.So in 99% cases I use separate console window to push my stuff to remote repository. But not anymore :)Package manager console to the rescueThe easiest way (in my opinion of course) is to use Package Manager Console is a PowerShell console :)The only difference is that it has separate profile file form standard PowerShell.PoshGit - git extension for PowerShellIf didn‚Äôt try this before, you should now. It was described before at: Better Git with PowerShell @haacked blog Prompts and Directories - Even Better Git (and Mercurial) with PowerShell @shanselman blogThe effect in PowerShell:hich gives you all needed information with colors and numbers about your local git repository: how many new files, how many changes, is you repository behind/after origin, etc.Poshgit inside VisualStudioIf you already install posh-git in ‚Äúnormal‚Äù PowerShell the easiest step is add ‚Äúinclude‚Äù in package manager console profile file. Just run inside package manager consolenotepad $PROFILE and add line like below. Iit depends on how you installed poshgit - you can find it in normal powershell profile with typing in in notepad $PROFILE. 'C:\\Users\\XXXXXX\\Documents\\WindowsPowerShell\\Modules\\posh-git\\profile.example.ps1'or. 'C:\\[somewhere]\\posh-git\\profile.example.ps1'Reload profile with . $PROFILE or just restar Visual Studio. Now I am ready to work:" }, { "title": "\"I have a VERY interesting job offer for You\"", "url": "/i-have-a-very-interesting-job-offer-for-you/", "categories": "", "tags": "", "date": "2014-10-01 18:27:17 +0000", "snippet": "The best job offer for IT developerDuring last week I heard this statement few times. My favorite one was:I have a interesting job offer for You as SharePoint developerI still cannot understand how words: interesting and SharePoint can coexist in one sentence without destroying itself :)Example talkTo be precise I don‚Äôt want to change my job. Of course if someone calls me and offer a dream job I will change it without asking: how much you pay?Today I stuck in traffic jam an typical HR researcher call me to offer a job as ASP.NET MVC developer. I was boring, so I didn‚Äôt end call asap, but I tried to discover what is this job about.First of all I was asked if I know ASP.NET MVC (on MVC was a big accent), then jQuery, JavaScript, AJAX, HTML, XML, C#, Kanban or Scrum, T-SQL, ‚Ä¶.My answer was as you expected: yes, yes, yes, ‚Ä¶ After another ‚Äúyes‚Äù I asked her where did she found my CV, because all contains above information, but she said that she must ask such questions.Now interesting fact: she found me as ASP.NET MVC developer, she call me because in 99% she found me searching for above skills. Of course I don‚Äôt have to know C# if I am ASP.NET MVC developer: I can use VB.NET or even F# - but such cases are really rare. But be serious I have to know basic about jQuery, JavaScript, HTML, ‚Ä¶ if I do anything in web development. Facepalm.What are we talking aboutAfter above questions, she was very happy. I meet all criteria, so she was ready to send my CV to her boss/IT manager/someone else and disconnect the call. But I tried to ask some basic questions like: what project is about? where would I work if I agree? what benefits I would receive?The answer you probably know: cannot say it is secret in this part of recruiting process. The only clean answer was about last one: private medical care and MultiSport card (for non-polish: card for accessing sport objects). I asked what about: MSDN access, Pluarsight, Resharper and other stuff useful for developers. Of course answer was: don‚Äôt knowSum upI understand that woman, who call me really did not know about most my questions, I understand that someone print typical skills which are useful and she asked it, I understand that she really does not have IT knowledge. Moreover I know she is not responsible for that. But please IT guys spent more that 2 minutes preparing questionnaire for candidate. Prepare few statements about project. Ideally also few about things important to developers. We all does not have infinite time, so again please do some preparation. You, me, them, everybody - we won‚Äôt change our work until: project is really interesting we are not $%$#%@#$@# with our current job the proposition is different then others" }, { "title": "How to analyze log files (part 1)", "url": "/better-logging/", "categories": "", "tags": "", "date": "2014-10-01 07:34:07 +0000", "snippet": "Log filesIn the beginning I start I would like to present some very simple error log[2014-09-30 20:00:00] ERROR Exception: Client doesn't have Facebook id[...][2014-09-30 20:01:00] ERROR NetworkException: connection is down. Reconnecting[...][2014-09-30 20:02:00] ERROR SQLException: no such column: p.p_id[...][2014-09-30 20:03:00] ERROR SQLException: key already existAll above errors are different cases, all should we treat differently. I will try to go through them case by case. Why? To help operators and automatic monitoring help discover what is wrong with system.Case 1: Business exception[2014-09-30 20:00:00] ERROR Exception: Client doesn't have Facebook idAbove error is a typical business exception. It doesn‚Äôt say anything about your system state. In 99% percent cases it should be ignored by operators. But there is a problem. Also in your system logs you could have error like bellow:[2014-09-30 20:00:00] ERROR Exception: Something is wrong with configurationThe second error is still Exception - the signature is identical to previous one. But if operator read above message, he should start an alarm, because usually problems with configuration are FATAL problem to your system.To sum up first error type you should: precise what type of exception it is. In first case it could be for example BuisnessException and for the second one use ConfigurationException. This will be much more readable to operators try to separate business exceptions from other ones. Most logging libraries allow to log different errors to different targets quite easy.Case 2: Complex errors[2014-09-30 20:01:00] ERROR NetworkException: connection is down. ReconnectingAbove exception is very difficult to analyze. Why? Because it depends on context. If it happen rarely it is probably not a problem at all. But if your log is full of such error you probably have a BIG problem. How to monitor such situation? Unfortunate it is complex event processing, but you can simple ask your administrators about what automatic system they use. They probably will know how to monitor such situation. If you don‚Äôt have an idea write to me, I willAnyway, as a developer/designer, it is Your responsibility to sit with them and analyze what frequency is dangerous.Case 3: External system errors (like SQLExceptions)[2014-09-30 20:02:00] ERROR SQLException: no such column: p.p_id[...][2014-09-30 20:03:00] ERROR SQLException: key already existThis part in my opinion is most tricky part. Both are SQLException, but first is kind of FATAL one, and second can be last data validationBut in above errors one very important information is missing: error code. With the error code you can build rules in system monitoring base on error code. Of course it will take some time to build such rules, but you can start with simple one: everything is FATAL. After you analyze first batch of errors, you can start with building rulesSum upConclusions: Create meaningful exceptions, which are easy separable: use different exceptions and error codes Analyze your log all the time Ask operators what they needIn next part I will describe other problems with application logs." }, { "title": "[about.mbank] devs for devs @ wrocnet - slides + video", "url": "/about-mbank-devs-for-devs-wrocnet-slides-video/", "categories": "", "tags": "slides", "date": "2014-09-30 12:55:54 +0000", "snippet": "To be very short: video is here: http://www.youtube.com/watch?v=jVrihtoGknA slides are here: http://piotrstapp.azurewebsites.net/content/images/slides/about.mbank/#/" }, { "title": "My children asked for own Windows accounts", "url": "/my-children-asked-for-own-windows-accounts/", "categories": "", "tags": "", "date": "2014-09-29 17:35:11 +0000", "snippet": "Today is end of the universeI cannot believe, but I have following following conversation with my son (age almost 6): I will never hack in to Your account Daddy. - said D. Why You want to do this? - me Because I don‚Äôt have my own one - D. So You want one? - me YEEEEESSSSS!!!!!!!! - D. Me to, me to. - added M.- twin sister of D.[‚Ä¶] Why I don‚Äôt have an account? I have to use tablet and phone - my wife Ok You will have one - meSo now my PC have 4 accounts instead of one: Mine D. M. My wifeIt was mine old laptop with one small account, now it is a family laptop :)Anyway I setup Microsoft Family Safety - I am really interested how it will work." }, { "title": "Get through corporate proxy - any host any protocol", "url": "/through-proxy/", "categories": "", "tags": "tutorial, putty, azure", "date": "2014-09-24 08:30:00 +0000", "snippet": "Prerequisites Install putty Install favorite Linux machine on Azure portal Install and configure CntlmEnable endpointIn azure portal (old or new one) you need to modify SSH endpoint to port 443SSH using putty Open putty and in host name write: YOURMACHINE.cloudapp.net In port set 443 instead of 22 Open Connection-&gt;proxy in side bar and fill it with cntlm settings like bellow Save your session and log in to YOURMACHINE.cloudapp.net to check connection If above is working, load saved session Open Connection-&gt;SSH-&gt;Tunnels in proxy and fill it for example like below Save your session and in my example do RDP using localhost:2222 as Computer If you need ‚Äúfull‚Äù Internet access without proxy limits just use dynamic port forwarding like describe in DYNAMIC SSH TUNNELING WITH PUTTY TO SECURE WEB TRAFFIC" }, { "title": "Track every single change in folder with git", "url": "/track-every-single-change-in-folder-with-git/", "categories": "", "tags": "git, powershell", "date": "2014-09-12 12:44:55 +0000", "snippet": "During my last work with Vagrant I need to track every single file change. The easiest way which I found was to create simple git repository in selected folder (with git init of course) and two new functions in PowerShell profile (to edit profile just run notepad $PROFILE):function buildGitMsg(){\t$date = Get-Date\treturn $date.ToString(\"yyyy-MM-dd HH:mm:ss\");\t}function vagrant(){\t$msg = buildGitMsg;\t$gitRoot=(git rev-parse --show-toplevel).Replace(\"/\",\"\\\")\tpushd $gitRoot\tgit add .\tgit commit -m \"$msg\"\tpopd\tvagrant.exe $args}This small change allows me to automatically commit on every run vagrant with arguments.Small, easy and cool :)" }, { "title": "[about.mbank] devs for devs: meet me on 16.08.2014 at Wroc≈Çaw .NET UG", "url": "/about-mbank-devs-for-devs-meet-me-on-16-08-2014/", "categories": "", "tags": "", "date": "2014-09-08 19:52:38 +0000", "snippet": "Sometime ago a group of very special people did a unique application. Above sentence can describe almost any IT project, but I am sure You will agree that the New mBank in 100% meets above definition, including that: It was completely renew of access channel to one of biggest internet bank in Poland It was build by multiple teams (10+) in different cites working in parallel It has unified architecture It integrates with living IT infrastructure Much much moreAnd it was done in .NET which is quite unique project on that scale done on the top of Microsoft technologies stack.So if you will be on 16.08.2014 in Wroc≈Çaw or somewhere near you are welcome to listen about how we did it.The details: When: 2014-08-16 18:30:00 Where: Wroc≈Çaw ul.Rze≈∫nicza 28 Who: Me, Myself and I Link: http://www.meetup.com/wrocnet/If you decide to come, You will have unique chance to hear about: How to structure and manage 1.000.000 lines of JavaScript Side-by-side (almost XP) work of UIUX artists with .NET developers Efficiently sharing artifacts between 100+ developers Making ASP.NET MVC application really modular How to trace and find bottlenecks in multi-layered, multi-technological applicationAfter presentation I am planning to have Q&amp;A session.To sum up: See you on 16th" }, { "title": "Vagrant provisioning using shell - how to setup storm project", "url": "/vagrant-provisioning-shell/", "categories": "", "tags": "vagrant, tutorial, devops, storm", "date": "2014-09-06 08:16:00 +0000", "snippet": "Provisioning in VagrantThere is a lot of choices when you want to provision your virtual machine with vagrant: Run scripts manually - completely insane idea Run shell scripts inline or not Use chef or puppetThe first one is good only for testing, trying, ‚Ä¶, but it is insane if you want to provision all machines like this.The second one is looks quite optimistic, especially if you don‚Äôt know what are tools from third point.Today I will try to present how to setup storm project on vagrant machine using only shell provisioning. To simplify the operation I will use tutorial from Hortonworks: http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.1-latest/bk_installing_manually_book/content/ch_rpm_storm.html1000 words which describe my work looks like:Step .0. Choosing and preparing machineFirst we need to create directory and init vagrant so let‚Äôs do this:mkdir storm-vagrantcd storm-vagrantvagrant initI will use centos box which I downloaded before (you can use URL instead of file). Why Centos you asked? Because in many companies the only allowed Linux is Red Hat. Centos is closed enough. So let‚Äôs add the box: vagrant box add \"centos-6.5\" file:///c:/vagrantfiles/CentOS-6.4-i386-v20131103.boxIn vagrant file we need to change our box name so config.vm.box = \"base\" becomes config.vm.box = \"centos-6.5\"Step .1. Configure networkWe will need network access to guest machine. It can be done in two ways: Using port forwarding Setting IP adress for guest machine.I will add code to enable private network in machine so vagrantfile will look like below.:# -*- mode: ruby -*-# vi: set ft=ruby :# Vagrantfile API/syntax version. Don't touch unless you know what you're doing!VAGRANTFILE_API_VERSION = \"2\"Vagrant.configure(VAGRANTFILE_API_VERSION) do |config| # All Vagrant configuration is done here. The most common configuration # options are documented and commented below. For a complete reference, # please see the online documentation at vagrantup.com. # Every Vagrant virtual environment requires a box to build off of. config.vm.box = \"centos-6.5\" config.vm.network \"private_network\", ip: \"192.168.33.10\"Caution: In next code listings I will only append above lines.Now we are ready to run:vagrant upAfter the above command is complete (don‚Äôt worry about read color on console), let‚Äôs explore our box. Run vagrant ssh and you are inside. Check what you want and logout from the machine.Step .2. Installing storm rpms.According to ‚Äù Chapter 1. Getting Ready to Install‚Äù we need to configure remote repositories. For Centos 6 the line is:wget -nv http://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.1.5.0/hdp.repo -O /etc/yum.repos.d/hdp.repo So let‚Äôs change our vagrant file to (first two lines already exist in our file): config.vm.box = \"centos-6.5\" config.vm.network \"private_network\", ip: \"192.168.33.10\" config.vm.provision :shell, :inline =&gt; \"wget -nv http://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.1.5.0/hdp.repo -O /etc/yum.repos.d/hdp.repo\"According to Chapter 17.1 Install the Storm RPMs. we can install storm rpm withyum install stormbut above command will prompt as if we are sure to download storm + zookeeper (yes storm needs zookeeper to run). To avoid prompt just add -y to the command. So vagrant file will evolve to:config.vm.box = \"centos-6.5\" config.vm.provision :shell, :inline =&gt; \"wget -nv http://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.1.5.0/hdp.repo -O /etc/yum.repos.d/hdp.repo\" config.vm.provision :shell, :inline =&gt; \"yum -y install storm\"To call above provisioning we have to choices: Run vagrant destroy and again vagrant up - if we do too much manual changes in previous steps Or just vagrant provision to run only provisioning on running boxStep .3. Running zookeeperZookeeper? Why? -you think now. Storm need zookeeper to comunicate between its nodes. The second line of last chapter: 5. Validate the Installation is: You must start ZooKeeper before starting Storm.First we need Java (for storm and zookeeper). I prefer Java 7 form Sun, so I just downloaded rpm form official page to vagrant folder. In recipe please add: config.vm.provision :shell, :inline =&gt; '\tjavarpm=\"$(ls /vagrant/ | grep \"^jdk.*.rpm\" | tail -1)\"\techo $javarpm\trpm -Uvh /vagrant/$javarpm\tjava -version'Please notice that I have to change \" into '. The second option will be escape \" with \\\"Then we need to create dirs as it is explain in Chapter 5.2: Set Directories and Permissions. I created a file directories.sh with below content. It should be placed side-by-side to Vagrantfile:#!/bin/sh# Directory where ZooKeeper will store data. For example, /grid1/hadoop/zookeeper/dataexport ZOOKEEPER_DATA_DIR=\"/grid1/hadoop/zookeeper/data\";# Directory to store the ZooKeeper configuration files.export ZOOKEEPER_CONF_DIR=\"/etc/zookeeper/conf\";# Directory to store the ZooKeeper logs.export ZOOKEEPER_LOG_DIR=\"/var/log/zookeeper\";# Directory to store the ZooKeeper process ID.export ZOOKEEPER_PID_DIR=\"/var/run/zookeeper\";Caution: If you are on Windows host, remember to change line ending to Unix style to avoid : command not found printing on console during login to guest machineAnd do provisioning below with all steps mention in zookeeper chapter (creating dirs, set zookeeper node id and start it).config.vm.provision :shell, :inline =&gt; \"\tyes | cp /vagrant/directories.sh /etc/profile.d/directories.sh\tchmod 755 /etc/profile.d/directories.sh\"config.vm.provision :shell, :inline =&gt; \"\tmkdir -p $ZOOKEEPER_LOG_DIR;\tchown -R $ZOOKEEPER_USER:$HADOOP_GROUP $ZOOKEEPER_LOG_DIR;\tchmod -R 755 $ZOOKEEPER_LOG_DIR;\t\tmkdir -p $ZOOKEEPER_PID_DIR;\tchown -R $ZOOKEEPER_USER:$HADOOP_GROUP $ZOOKEEPER_PID_DIR;\tchmod -R 755 $ZOOKEEPER_PID_DIR;\t\tmkdir -p $ZOOKEEPER_DATA_DIR;\tchmod -R 755 $ZOOKEEPER_DATA_DIR;\tchown -R $ZOOKEEPER_USER:$HADOOP_GROUP $ZOOKEEPER_DATA_DIR\t\techo '1' &gt;&gt; $ZOOKEEPER_DATA_DIR/myid\t\tsu - zookeeper -c 'source /etc/zookeeper/conf/zookeeper-env.sh ; export ZOOCFGDIR=/etc/zookeeper/conf;/usr/lib/zookeeper/bin/zkServer.sh start &gt;&gt; /var/log/zookeeper/zoo.out 2&gt;&amp;1'\"To check if ZooKeeper is running we need to login to our box and run:ps aux | grep zookeeperIs process does not exist just check:cat /var/log/zookeeper/zoo.outStep .4. Configure StormAfter reading 17.2. Configure Storm a new file is needed: storm.yaml. Because everything will be on one machine I will set ZOOKEEPER_SERVERS and NIMBUS_HOSTNAME to localhost. Create a storm.yaml again side-by-side to vagrantfile with following content:storm.zookeeper.servers: - localhostnimbus.host: localhostdrpc.servers: - \"localhost\"storm.local.dir: /tmp/storm/locallogviewer.port: 8081and copy it into /etc/storm/conf/storm.yaml. The above file is located in shared /vagrant directory.Both above modification in vagrantfile:[...] config.vm.provision :shell, :inline =&gt; \"yum -y install storm\" config.vm.provision :shell, :inline =&gt; \" mkdir -p $STORM_LOCAL_DIR chown -R storm:storm $STORM_LOCAL_DIR chmod -R 755 $STORM_LOCAL_DIR cp -f /vagrant/storm.yaml /etc/storm/conf/storm.yaml\"Attention: I added mkdir -p $STORM_LOCAL_DIR because this folder wasn‚Äôt create on destination machine.The last step is to add proper exports in direstories.sh file:# storm local direxport STORM_LOCAL_DIR=\"/tmp/storm/local\";To validate install we should login to guest and run:sudo su - stormstorm nimbusThe expected output is something similar to:/usr/bin/storm: line 2: /etc/default/hadoop: No such file or directoryRunning: java -server -Dstorm.options= -Dstorm.home=/usr/lib/storm [...snip...] backtype.storm.daemon.nimbusDon‚Äôt be afraid about first line. We won‚Äôt need Hadoop at all, but Hortonworks installation files assume that we install it everywhere.Step .5. Configure Process ControllerThe optional 3 chapter I omit because we don‚Äôt need it now - we didn‚Äôt secure zookeeper. What is interesting we didn‚Äôt do anything about zookeeper - let‚Äôs back to this later. Just believe me.In chapter 4. Configure Process Controller there is a mention about tool called supervisord. If we check it using yum search supervisord we won‚Äôt find it in enabled repositories. To install it we need EPEL repo withcd /tmpwget http://download.fedoraproject.org/pub/epel/6/i386/epel-release-6-8.noarch.rpmsudo rpm -Uhv epel-release-6-8.noarch.rpmand then run: yum install -y supervisor. Because supervisor.conf can contain password it is suggested to mark it with 600So in vagrant file we have to add above (remember with -y in yum command):config.vm.provision :shell, :inline =&gt; \"\tcd /tmp \twget http://download.fedoraproject.org/pub/epel/6/i386/epel-release-6-8.noarch.rpm \tsudo rpm -Uhv epel-release-6-8.noarch.rpm \tyum install -y supervisor \tsudo chmod 600 /etc/supervisord.conf\"Let‚Äôs run the machine (with vagrant provision), login to it and copy default conf file: sudo cp /etc/supervisord.conf /vagrant. In copied file we can append lines from 4. Configure Process Controller.After this step we need to add copy back this file in provisioning:config.vm.provision :shell, :inline =&gt; \"\tyes | cp /vagrant/supervisord.conf /etc/supervisord.conf\t/etc/init.d/supervisord restart\"Remember: supervisord.conf, storm.yaml and vagrantfile are integral parts of our recipeStep.6. Opening portsThe last part will be open ports for storm-ui, which is by default 8080. We should add line in /etc/sysconfig/iptables file and restart iptables. The file will look like (remember about UNIX line endings):# Firewall configuration written by system-config-firewall# Manual customization of this file is not recommended.*filter:INPUT ACCEPT [0:0]:FORWARD ACCEPT [0:0]:OUTPUT ACCEPT [0:0]-A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT-A INPUT -p icmp -j ACCEPT-A INPUT -i lo -j ACCEPT-A INPUT -m state --state NEW -m tcp -p tcp --dport 22 -j ACCEPT-A INPUT -m state --state NEW -m tcp -p tcp --dport 8080 -j ACCEPT-A INPUT -j REJECT --reject-with icmp-host-prohibited-A FORWARD -j REJECT --reject-with icmp-host-prohibitedCOMMITAnd provisioning is: config.vm.provision :shell, :inline =&gt; \"\tyes | cp /vagrant/iptables /etc/sysconfig/iptables\t/etc/init.d/iptables restart\"Then run vagrant provision for the last time.Step.7. Check installationAt least we check if everything is working. Just open your favorite browser on http://192.168.33.10:8080/ and you should see Storm UI web page. If there is a connection error you can: Wait a while - it need some time to start :) Check logs in /var/log/storm and /var/log/zookeeper Recreate your machine with running vagrant destroy and then vagrant upThat all. Next time I will show how to use better provisiong than shell, because as you probably notice shell provisionig works, but is quite ugly and has a lot of harcoded stuff." }, { "title": "Run vagrant on your Windows", "url": "/run-vagrant-on-your-windows/", "categories": "", "tags": "vagrant, tutorial, devops", "date": "2014-09-05 21:00:00 +0000", "snippet": "Prerequisites Windows machine :) Install vagrant form https://www.vagrantup.com/downloads.html Install git with tools (or other ssh.exe client) + add folder to PATH. In powershell just invoke $env:PATH +=\";C:\\Program Files (x86)\\Git\\bin\" Configure folder for boxes. It is useful when you want to work in offline mode. The easy way is to create mklink to store files in same place. Start cmd as admin and run (of course change second path to proper one):mklink /D c:\\vagrantfiles d:\\downloads\\vagrantfiles Download Oracle VM VirtualBox from https://www.virtualbox.org/ (or other prefered virtualization product). Vagrant already supports Virtualbox, VMWare and Hyper-V :)Validate instalation Create new folder. Let‚Äôs call it test-vagrant Enter it and type: vagrant init Open Vagrantfile in your favorite notepad and: Change the box name Add url to the box like: config.vm.box_url = \"file:///c:/vagrantfiles/CentOS-6.4-i386-v20131103.box\" Run vagrant up to create and start the VM Run vagrant ssh to validate if it is working Run vagrant destroy to delete the machine" }, { "title": "StackOverflow question - my personal anti-record", "url": "/stackoverflow-question-my-personal-anti-record/", "categories": "", "tags": "", "date": "2014-08-28 19:27:43 +0000", "snippet": "Some minutes ago I asked a simple question on StackOverlow: How to compile C# to .NET 4.0 on machine with 4.5.1But in the text of question I made a stupid mistake because I wrote: In my computer I have .NET framework 4.0 How to be sure that compilation of c# project will run on machine with only .NET 4.5?Of course You probably see the problem, a guy called @tnw write me a comment: Your question and title are totally different. What are you asking?Until I updated my post it had already -8 points and 2 votes to close as not a real question. -8 point is ok ;), but why it is not a real question I don‚Äôt know.I updated my question to correct one, still forgetting that I didn‚Äôt mention that I don‚Äôt have a Visual Studio on compilation machine.I updated my question to full details but I ended up with -14 points and marked as duplicate and it look like this (I make a screenshot because it has already 2 delete votes):{&lt;2&gt;}The funnest thing for me is that my best answer is for question: Bootstrap 3.0.0 - what is replacement of bootstrap-responsive.css?:My answer is just quote of the Bootstrap documentation. This question has already 16 votes, my answer has 38 :)I don‚Äôt see big different with this question, of course without counting my mistake :)Next time I will read my question twice before publishing on:" }, { "title": "Why your perfect website design is lost?", "url": "/why-your-perfect-design-is-lost/", "categories": "", "tags": "ui", "date": "2014-08-28 17:52:44 +0000", "snippet": "Let‚Äôs look on simple website creation flow.1. Your idea -&gt; 2. Find design/hire designer -&gt; 3. Implement/customize designThe step one is obvious. With the second step: you go to designers agency or find template online. The third step usually is the most difficult, because you want to change font, pictures, icons, text doesn‚Äôt exactly fit, etc.The above process is classic waterfall, with no step back in 99.99%. But is it right?Yesterday I clicked Samsung adv on twitter, without any special reason, probably by accident. They are counting down to new Samsung Alpha (whatever it is). The new page loaded and I saw below: Then it transform to this: And finally this: At least on las, t presented image I saw the counter clearly. But what happened to this counter? Let‚Äôs try to reproduce the probable process of creating this site, with creating my own :) The idea: we need counter to present when MyProduct will be available I need to find template (or hire designer). I opened WrapBootstrap and find a proper one. -&gt; Done! It will be: SeventyTwo - Premium Coming Soon. And it look quite nice The most difficult step. Let‚Äôs customize it. ‚ÄúMyProduct is not about mountains, it is about bikes and it needs more lighter background‚Äù - so let‚Äôs apply changes: So PLEASE add one more step in your workflow:1. Your idea -&gt; 2. Find design/hire designer -&gt; 3. Implement/customize design -&gt; 4. Verify your customization with the designer" }, { "title": "Why am I not an European Athletic Champion?", "url": "/why-am-i-not-an-european-athletics-champion/", "categories": "", "tags": "", "date": "2014-08-17 16:06:36 +0000", "snippet": "During last few days I watched a lot relation from European Athletics Championships in Zurich. Poland got a lots of medals - 12 (when I writing) and it is a really big success when I look in other countries: Germany - 5 medals; Italy - 4; Holland - 6.It gives us 6th position in general classification and 5th in points classification after Russia, France and Great Britain (more on Polish Wikipedia)But I ask myself why am I not there? - not as a fun club member, but as an Athletics Champion. But why am I not an Athletics Champion?The simple answer is because I never tied most athletics disciplines. Of course when I was a child a do a lot of sports: I do swimming, skiing, running, playing football/volleyball/basketball and I am even train tennis for more than 10 years. But during this years I never tried discipline like javelin throw, discus throw, high jump or pole vault. Long and triple jump two I tried once when I was 10 or 11.To sum up this post: I hope my children will have a chance to try at least once some classic sports and maybe some day even be a athletic champion at school, city or world ;)" }, { "title": "Atom on Windows", "url": "/atom-on-windows/", "categories": "", "tags": "", "date": "2014-05-09 08:23:57 +0000", "snippet": "I decied to try to build Atom and I succeed.The instruction is very simple: Install git Install posh-git Install Python 2.7After above run powershell with following commands:$env:Path+=\";C:\\Python27;C:\\Program Files\\nodejs;C:\\Users\\\"+$env:USERNAME+\"\\github\\atom\\node_modules\\\"mkdir (\"c:\\users\\\"+$env:USERNAME+\"\\github\")cd (\"c:\\users\\\"+$env:USERNAME+\"\\github\")git clone https://github.com/atom/atomcd atomscript\\buildBut if you don‚Äôt want to do above download my build from here" }, { "title": "What is an Enviroment?", "url": "/what-is-an-enviroment/", "categories": "", "tags": "", "date": "2014-05-06 07:08:54 +0000", "snippet": "IntroductionYesterday our team have about domain model in our company. It wasn‚Äôt the first one and probably not the last one :) Anyway we spent more than an hour fighting discussing what is an Enviroment. Everybody knows what it is, but everybody knows something elseSimple case - one small projectLet‚Äôs assume that we develop simple project: website (asp.net,php,etc.) with database (SQL, NoSQL, files,‚Ä¶). We separate website machine with database machine for security, performance, scaling, (put here whatever you want). So we have at least 2 machinesWe can have following environments (whatever it is): Test env for developers Preproduction (test environment) ProductionSo what is an environment? First try: ‚Äúset of machines‚Äù. This option is very popular in tools like Octopus Deploy. This definition is quite good, but we can install all instances on same machines. Usually it is not installed on same machines but it can. This happen what we have problem with licences for SQL Server, lack of machines in NoSQL solutions, etc.So ‚Äúset of machines‚Äù doesn‚Äôt work for environment.More complicated caseTo complicate our solution, let‚Äôs add one more dimension: TIME. So we have now following environments: Production with version 1.0.0.333 Preproduction (test environment) with version 1.1.0.22 - hotfix is coming :) Test env for developers with version 1.5.0.555 - night build of new versionMoreover ‚Äúenvironment‚Äù sometimes needs different prerequisites, because we can for example upgrade our framework form version 3.5 to 4.5 our we upgrade our database version (e.g.: SQL Server 2008 to SQL Server 2012) and we need some migration.ConclusionTo define an environment at least we have to include: machine (virtual or physical) versions of components prerequisites and maybe some other things" }, { "title": "CDN bundles are avaliable", "url": "/cdn-bundles-are-avaliable/", "categories": "", "tags": "dotnet, ui", "date": "2014-04-28 20:01:47 +0000", "snippet": "Few days ago I discovered that ASP.NET team at least add possibility to fallback CDN usage in script bundles. It can be done like following:public static void RegisterBundles(BundleCollection bundles){ bundles.UseCdn = true; BundleTable.EnableOptimizations = true; //for debbuging var jquery = new ScriptBundle(\"~/bundles/jquery\", \"//ajax.aspnetcdn.com/ajax/jquery/jquery-2.0.0.min.js\").Include( \"~/Scripts/jquery-{version}.js\"); jquery.CdnFallbackExpression = \"window.jQuery\"; bundles.Add(jquery);}Which generates more or less bellow JavaScript: &lt;script type=\"text/javascript\"&gt; if (typeof jQuery == 'undefined') { document.write('&lt;scr'+'ipt type=\"text/javascript\"'+ 'src=\"/bundles/jquery\"&gt;&lt;/sc'+'ript&gt;'); } &lt;/script&gt; Everything is really cool, BUT bundles usually has more than ONE file in it. Of course when can create a bundle for each script, but in my opinion it is quite irritating. Especially that wen need fallback for every script :)I was looking for CDN with popular bundles, but I cannot find anything like this. Thanks to Patrick Nommensen (@pnommensen) I found that in jsDelivr I can specify URL query like://cdn.jsdelivr.net/g/jquery@2.0.1,angularjs(angular.min.js+angular-resource.min.js+angular-animate.min.js+angular-cookies.min.js+angular-route.min.js+angular-sanitize.min.js)So download multiple files from CDN, using one request, with every version specfied. Moreover I can prepare exactly same bundle localy and use only one fallback. Which is really cool :)" }, { "title": "Better SourceTree with Mono.Cecil", "url": "/better-sourcetree-with-mono-cecil/", "categories": "", "tags": "", "date": "2014-03-27 14:01:37 +0000", "snippet": "IntroductionI am using Git for a while, but I hate git console for Windows. But some time ago Scott Hanselman published how to integrate Git with PowerShell which is exactly what I need. Instead of old cmd I have all features from PowerShell.Sometime ago my friends show me SourceTree, which is quite nice tool. But it has a one irritating button: Terminal, which opens Git console or cmd. But I hate git console and I hate cmd, I would like to open PowerShell instead. I look all the options but I cannot find anyway to change this button action, or at least hide it.What does this button do?Because I didn‚Äôt find any useful option, I decided to check the source. But SourceTree is not open-source app, which is quite strange in this days ;)SourceTree is .NET application, so we can restore ‚Äúour‚Äù code using some dedicated tools like dotPeek or ILSpy. I find out that terminal button handler looks like following:public void LaunchGitBashPrompt(string path){ string str = GeneralHelper.CombinePath(RepoHandlerGit.GitBasePath(), \"bin\\\\sh.exe\"); ProcessStartInfo startInfo = new ProcessStartInfo(\"cmd.exe\"); if (path != null) startInfo.WorkingDirectory = path; startInfo.Arguments = string.Format(\"/c \\\"{0}\\\" --login -i\", (object) str); Process.Start(startInfo);}All the stings are hardcoded, so there isn‚Äôt an easy way to replace them.Mono.Cecil to the rescue.Quoting offical site (the most importat part is bolded): Cecil is a library written by Jb Evain to generate and inspect programs and libraries in the ECMA CIL format. It has full support for generics, and support some debugging symbol format. In simple English, with Cecil, you can load existing managed assemblies, browse all the contained types, modify them on the fly and save back to the disk the modified assembly. Today it is used by the Mono Debugger, the bug-finding and compliance checking tool Gendarme, MoMA, DB4O, as well as many other tools.So I produce simple Console application: public static void Main(string[] args) { const string powershellLink = @\"C:\\ProgramData\\Microsoft\\Windows\\Start Menu\\Programs\\Accessories\\Windows PowerShell\\Windows PowerShell.lnk\"; const string powershellArgs = \" -NoExit \\\"cd '{0}';\\\"\"; var resolver = new DefaultAssemblyResolver(); resolver.AddSearchDirectory(@\"C:\\tempSourceTree\\sourceMy\"); var parameters = new ReaderParameters { AssemblyResolver = resolver, }; var assembly = AssemblyDefinition.ReadAssembly(@\"C:\\tempSourceTree\\sourceMy\\SourceTree2.exe\", parameters); var repoHandlerGit = assembly.MainModule.Types.First(x =&gt; x.Name.EndsWith(\"RepoHandlerGit\")); Console.WriteLine(repoHandlerGit); var launchGitBashPrompt = repoHandlerGit.Methods.First(x =&gt; x.Name == \"LaunchGitBashPrompt\"); Console.WriteLine(launchGitBashPrompt); var cmdString = launchGitBashPrompt.Body.Instructions.First(x =&gt; \"cmd.exe\".Equals(x.Operand)); cmdString.Operand = powershellLink; var argsString = launchGitBashPrompt.Body.Instructions.First(x =&gt; x.Operand != null &amp;&amp; x.Operand is string &amp;&amp; ((string)x.Operand).Contains(\"login\")); argsString.Operand = powershellArgs; launchGitBashPrompt.Body.Instructions[14] = launchGitBashPrompt.Body.Instructions[10]; assembly.Write(@\"C:\\tempSourceTree\\sourceMy\\SourceTree.exe\"); Console.WriteLine(\"Done\"); Console.ReadLine(); }To use it copy SourceTree instalation into temp folder (in my case C:\\tempSourceTree\\sourceMy) and rename SourceTree.exe to SourceTree2.exe. After execution just replace SourceTree.exe with new fileThat all folks.p.s. I hoper bext time Atlassian will just show us code and I don‚Äôt have to use dirty tricks." }, { "title": "Setting up TeamCity agent with SSH on windows", "url": "/setting-up-teamcity-agent-with-ssh-on-windows/", "categories": "", "tags": "teamcity, ssh, devops", "date": "2014-02-28 13:39:58 +0000", "snippet": "Where is the problemI use TeamCity a lot at home and at work. Basically we work on .NET but sometimes we need to use invoke parts on Linux. One solution is to buy licence for Linux agent, but it in our case it will be just throwing out many: we need this more or less once a week for 5 minutes.One way communicationOne way communication: from agent on windows it‚Äôs quite easy. The only thing you need is a plugin for TeamCity. You can get it from here: http://confluence.jetbrains.com/display/TW/Deployer+pluginIt‚Äôs really good and allows to create simple build steps like: SSH Deployer - upload files to SSH SSH Exec - my favorite it allows to run commands using SSH SMB Deployer - upload to windows share, which is useless for me FTP Deployer - same for FTPTwo ways communicationAbove plugin are missing one much important feature: it doesn‚Äôt allow to download files from LinuxBut we have got pscp.exe from putty which we can install on agent machine ore include in our build.So the build step is quite easy: We setup Powershell build step In the Script field we enter: echo y | &amp; \"C:\\Program Files (x86)\\PuTTY\\pscp.exe\" -pw [YOUR PASS][USER]@%RedHat_host%:/somewhere/you_file.txt .Small explanation the echo y statement. When you first connect to SSH host it ask you to add host into trust one. This command allow to ‚Äúskip‚Äù this part.Unfortunately we need to include pass in statement or run in this agent a SSH agent to store password. The easiest and enought security option in my opinion is to create guest account with only read-only rightsThat‚Äôs all folks, see you next time." }, { "title": "ReStart", "url": "/welcome-to-ghost/", "categories": "", "tags": "getting-started", "date": "2014-02-28 12:40:38 +0000", "snippet": "Here are I am again. Again? I was blogging some time ago in different life: before children, house, etc. But now I am backWhat will show up here: everything about developer life, because every day gives sompletly new things to write about." } ]
